2026-01-21 14:54:51,645:INFO: device: cuda:0
2026-01-21 14:54:51,645:INFO: --------Process Done!--------
2026-01-21 14:55:59,019:INFO: device: cuda:0
2026-01-21 14:55:59,019:INFO: --------Process Done!--------
2026-01-21 14:55:59,028:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 14:55:59,028:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt. We won't load it.
2026-01-21 14:55:59,028:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 14:55:59,028:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 14:55:59,028:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 14:56:42,434:INFO: device: cuda:0
2026-01-21 14:56:42,434:INFO: --------Process Done!--------
2026-01-21 14:56:42,443:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 14:56:42,443:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 14:56:42,443:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 14:56:42,443:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 14:56:42,443:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 14:56:42,443:INFO: loading file None
2026-01-21 14:56:42,443:INFO: loading file None
2026-01-21 14:56:42,443:INFO: loading file None
2026-01-21 14:56:42,918:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 14:56:42,918:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 14:56:42,918:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 14:56:42,919:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 14:56:42,919:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 14:56:42,919:INFO: loading file None
2026-01-21 14:56:42,919:INFO: loading file None
2026-01-21 14:56:42,919:INFO: loading file None
2026-01-21 14:56:42,987:INFO: --------Dataset Build!--------
2026-01-21 14:56:42,987:INFO: --------Get Dataloader!--------
2026-01-21 14:56:42,987:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 14:56:42,988:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 14:58:31,844:INFO: device: cuda:0
2026-01-21 14:58:31,844:INFO: --------Process Done!--------
2026-01-21 14:58:31,853:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 14:58:31,854:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 14:58:31,854:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 14:58:31,854:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 14:58:31,854:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 14:58:31,854:INFO: loading file None
2026-01-21 14:58:31,854:INFO: loading file None
2026-01-21 14:58:31,854:INFO: loading file None
2026-01-21 14:58:32,334:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 14:58:32,334:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 14:58:32,334:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 14:58:32,334:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 14:58:32,335:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 14:58:32,335:INFO: loading file None
2026-01-21 14:58:32,335:INFO: loading file None
2026-01-21 14:58:32,335:INFO: loading file None
2026-01-21 14:58:32,404:INFO: --------Dataset Build!--------
2026-01-21 14:58:32,404:INFO: --------Get Dataloader!--------
2026-01-21 14:58:32,405:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 14:58:32,405:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 14:58:32,405:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 14:58:39,340:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 14:58:39,340:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 14:58:43,162:INFO: --------Start Training!--------
2026-01-21 15:01:14,162:INFO: device: cuda:0
2026-01-21 15:01:14,162:INFO: --------Process Done!--------
2026-01-21 15:01:14,172:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:01:14,172:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:01:14,172:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:01:14,172:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:01:14,172:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:01:14,172:INFO: loading file None
2026-01-21 15:01:14,172:INFO: loading file None
2026-01-21 15:01:14,172:INFO: loading file None
2026-01-21 15:01:14,670:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:01:14,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:01:14,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:01:14,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:01:14,671:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:01:14,671:INFO: loading file None
2026-01-21 15:01:14,671:INFO: loading file None
2026-01-21 15:01:14,671:INFO: loading file None
2026-01-21 15:01:14,742:INFO: --------Dataset Build!--------
2026-01-21 15:01:14,742:INFO: --------Get Dataloader!--------
2026-01-21 15:01:14,743:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 15:01:14,743:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:01:14,743:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 15:01:23,137:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 15:01:23,138:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 15:01:25,937:INFO: --------Start Training!--------
2026-01-21 15:01:30,467:INFO: Epoch: 1, train loss: 3470.7511474609373
2026-01-21 15:01:30,756:INFO: Epoch: 1, dev loss: 1412.6900482177734, f1 score: 0
2026-01-21 15:02:04,531:INFO: device: cuda:0
2026-01-21 15:02:04,531:INFO: --------Process Done!--------
2026-01-21 15:02:04,540:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:02:04,540:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:02:04,540:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:02:04,540:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:02:04,540:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:02:04,540:INFO: loading file None
2026-01-21 15:02:04,540:INFO: loading file None
2026-01-21 15:02:04,540:INFO: loading file None
2026-01-21 15:02:05,018:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:02:05,018:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:02:05,018:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:02:05,018:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:02:05,018:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:02:05,018:INFO: loading file None
2026-01-21 15:02:05,018:INFO: loading file None
2026-01-21 15:02:05,019:INFO: loading file None
2026-01-21 15:02:05,091:INFO: --------Dataset Build!--------
2026-01-21 15:02:05,092:INFO: --------Get Dataloader!--------
2026-01-21 15:02:05,092:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 15:02:05,092:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:02:05,093:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 15:02:12,643:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 15:02:12,643:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 15:02:15,314:INFO: --------Start Training!--------
2026-01-21 15:02:20,010:INFO: Epoch: 1, train loss: 1507.461264038086
2026-01-21 15:02:20,324:INFO: Epoch: 1, dev loss: 809.0915222167969, f1 score: 0
2026-01-21 15:02:24,754:INFO: Epoch: 2, train loss: 782.4843048095703
2026-01-21 15:02:25,066:INFO: Epoch: 2, dev loss: 491.8233184814453, f1 score: 0.09944751381215469
2026-01-21 15:02:25,067:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:02:26,897:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:02:26,898:INFO: --------Save best model!--------
2026-01-21 15:02:31,271:INFO: Epoch: 3, train loss: 445.5858322143555
2026-01-21 15:02:31,583:INFO: Epoch: 3, dev loss: 350.62896728515625, f1 score: 0.40404040404040403
2026-01-21 15:02:31,584:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:02:36,699:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:02:36,700:INFO: --------Save best model!--------
2026-01-21 15:02:41,127:INFO: Epoch: 4, train loss: 312.05945892333983
2026-01-21 15:02:41,429:INFO: Epoch: 4, dev loss: 314.3998209635417, f1 score: 0.5163934426229508
2026-01-21 15:02:41,430:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:02:46,404:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:02:46,405:INFO: --------Save best model!--------
2026-01-21 15:02:50,917:INFO: Epoch: 5, train loss: 237.04694671630858
2026-01-21 15:02:51,212:INFO: Epoch: 5, dev loss: 281.40623474121094, f1 score: 0.4871794871794872
2026-01-21 15:02:55,726:INFO: Epoch: 6, train loss: 180.57576637268068
2026-01-21 15:02:56,053:INFO: Epoch: 6, dev loss: 270.0382385253906, f1 score: 0.5560538116591929
2026-01-21 15:02:56,054:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:03:01,020:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:03:01,020:INFO: --------Save best model!--------
2026-01-21 15:03:05,677:INFO: Epoch: 7, train loss: 135.08359184265137
2026-01-21 15:03:05,991:INFO: Epoch: 7, dev loss: 311.47580973307294, f1 score: 0.6302521008403361
2026-01-21 15:03:05,992:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:03:10,976:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:03:10,976:INFO: --------Save best model!--------
2026-01-21 15:03:15,430:INFO: Epoch: 8, train loss: 121.89513473510742
2026-01-21 15:03:15,727:INFO: Epoch: 8, dev loss: 344.91233317057294, f1 score: 0.5963302752293578
2026-01-21 15:03:20,124:INFO: Epoch: 9, train loss: 85.14643096923828
2026-01-21 15:03:20,440:INFO: Epoch: 9, dev loss: 350.2986653645833, f1 score: 0.5727272727272726
2026-01-21 15:03:24,860:INFO: Epoch: 10, train loss: 72.29903030395508
2026-01-21 15:03:25,175:INFO: Epoch: 10, dev loss: 358.0350290934245, f1 score: 0.6550218340611355
2026-01-21 15:03:25,176:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:03:30,045:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:03:30,046:INFO: --------Save best model!--------
2026-01-21 15:03:34,462:INFO: Epoch: 11, train loss: 69.28500785827637
2026-01-21 15:03:34,756:INFO: Epoch: 11, dev loss: 417.7105000813802, f1 score: 0.5811965811965811
2026-01-21 15:03:39,147:INFO: Epoch: 12, train loss: 56.14382057189941
2026-01-21 15:03:39,462:INFO: Epoch: 12, dev loss: 324.0703837076823, f1 score: 0.6272727272727273
2026-01-21 15:03:43,849:INFO: Epoch: 13, train loss: 61.550956344604494
2026-01-21 15:03:44,146:INFO: Epoch: 13, dev loss: 369.3392283121745, f1 score: 0.6068376068376068
2026-01-21 15:03:48,685:INFO: Epoch: 14, train loss: 45.79978942871094
2026-01-21 15:03:49,002:INFO: Epoch: 14, dev loss: 341.2049255371094, f1 score: 0.6351931330472103
2026-01-21 15:03:53,389:INFO: Epoch: 15, train loss: 41.12816047668457
2026-01-21 15:03:53,691:INFO: Epoch: 15, dev loss: 433.13475545247394, f1 score: 0.6607929515418502
2026-01-21 15:03:53,692:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:03:58,552:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:03:58,553:INFO: --------Save best model!--------
2026-01-21 15:04:03,062:INFO: Epoch: 16, train loss: 39.30800247192383
2026-01-21 15:04:03,363:INFO: Epoch: 16, dev loss: 384.7939147949219, f1 score: 0.6167400881057269
2026-01-21 15:04:08,035:INFO: Epoch: 17, train loss: 32.37658538818359
2026-01-21 15:04:08,352:INFO: Epoch: 17, dev loss: 374.76630147298175, f1 score: 0.6017699115044247
2026-01-21 15:04:12,848:INFO: Epoch: 18, train loss: 35.474463653564456
2026-01-21 15:04:13,161:INFO: Epoch: 18, dev loss: 390.98630777994794, f1 score: 0.6547085201793722
2026-01-21 15:04:17,553:INFO: Epoch: 19, train loss: 26.115719985961913
2026-01-21 15:04:17,847:INFO: Epoch: 19, dev loss: 404.5549011230469, f1 score: 0.6063348416289592
2026-01-21 15:04:22,498:INFO: Epoch: 20, train loss: 17.56378517150879
2026-01-21 15:04:22,792:INFO: Epoch: 20, dev loss: 437.97833251953125, f1 score: 0.6307053941908715
2026-01-21 15:04:27,150:INFO: Epoch: 21, train loss: 19.17695198059082
2026-01-21 15:04:27,465:INFO: Epoch: 21, dev loss: 466.9869384765625, f1 score: 0.65158371040724
2026-01-21 15:04:31,976:INFO: Epoch: 22, train loss: 15.406995391845703
2026-01-21 15:04:32,279:INFO: Epoch: 22, dev loss: 507.5504964192708, f1 score: 0.6440677966101694
2026-01-21 15:04:36,714:INFO: Epoch: 23, train loss: 13.458916473388673
2026-01-21 15:04:37,021:INFO: Epoch: 23, dev loss: 521.0841674804688, f1 score: 0.6343612334801763
2026-01-21 15:04:41,453:INFO: Epoch: 24, train loss: 16.751002883911134
2026-01-21 15:04:41,762:INFO: Epoch: 24, dev loss: 556.1030883789062, f1 score: 0.6367713004484306
2026-01-21 15:04:46,270:INFO: Epoch: 25, train loss: 16.089894485473632
2026-01-21 15:04:46,571:INFO: Epoch: 25, dev loss: 523.4609781901041, f1 score: 0.6576576576576576
2026-01-21 15:04:46,571:INFO: Best val f1: 0.6607929515418502
2026-01-21 15:04:46,572:INFO: Training Finished!
2026-01-21 15:04:46,586:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:04:46,586:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:04:46,586:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:04:46,586:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:04:46,586:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:04:46,586:INFO: loading file None
2026-01-21 15:04:46,586:INFO: loading file None
2026-01-21 15:04:46,586:INFO: loading file None
2026-01-21 15:04:46,721:INFO: --------Dataset Build!--------
2026-01-21 15:04:46,721:INFO: --------Get Data-loader!--------
2026-01-21 15:04:46,721:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:04:46,722:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:04:46,722:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:04:53,600:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 15:04:53,601:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:04:53,602:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:04:53,602:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:04:53,602:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:04:53,602:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:04:53,602:INFO: loading file None
2026-01-21 15:04:53,602:INFO: loading file None
2026-01-21 15:04:53,602:INFO: loading file None
2026-01-21 15:04:54,403:INFO: --------Bad Cases reserved !--------
2026-01-21 15:04:54,410:INFO: test loss: 447.98938242594403, f1 score: 0.682842287694974
2026-01-21 15:04:54,410:INFO: f1 score of ACTION: 0.4957264957264957
2026-01-21 15:04:54,411:INFO: f1 score of LEVEL_KEY: 0.6868686868686869
2026-01-21 15:04:54,411:INFO: f1 score of OBJ: 0.7152317880794703
2026-01-21 15:04:54,411:INFO: f1 score of ORG: 0.7226890756302522
2026-01-21 15:04:54,411:INFO: f1 score of VALUE: 0.8131868131868132
2026-01-21 15:05:35,000:INFO: device: cuda:0
2026-01-21 15:05:35,000:INFO: --------Process Done!--------
2026-01-21 15:05:35,010:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:05:35,010:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:05:35,010:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:05:35,010:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:05:35,010:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:05:35,010:INFO: loading file None
2026-01-21 15:05:35,010:INFO: loading file None
2026-01-21 15:05:35,010:INFO: loading file None
2026-01-21 15:05:35,483:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:05:35,483:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:05:35,483:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:05:35,483:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:05:35,483:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:05:35,483:INFO: loading file None
2026-01-21 15:05:35,483:INFO: loading file None
2026-01-21 15:05:35,483:INFO: loading file None
2026-01-21 15:05:35,550:INFO: --------Dataset Build!--------
2026-01-21 15:05:35,551:INFO: --------Get Dataloader!--------
2026-01-21 15:05:35,551:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 15:05:35,551:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:05:35,551:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 15:05:42,735:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 15:05:42,736:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 15:05:45,346:INFO: --------Start Training!--------
2026-01-21 15:05:50,075:INFO: Epoch: 1, train loss: 1497.5359130859374
2026-01-21 15:05:50,373:INFO: Epoch: 1, dev loss: 822.6451924641927, f1 score: 0
2026-01-21 15:05:54,997:INFO: Epoch: 2, train loss: 787.9336486816406
2026-01-21 15:05:55,312:INFO: Epoch: 2, dev loss: 473.6448109944661, f1 score: 0.10784313725490195
2026-01-21 15:05:55,313:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:06:00,232:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:06:00,232:INFO: --------Save best model!--------
2026-01-21 15:06:04,654:INFO: Epoch: 3, train loss: 437.2066146850586
2026-01-21 15:06:04,952:INFO: Epoch: 3, dev loss: 369.74058787027997, f1 score: 0.38647342995169076
2026-01-21 15:06:04,953:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:06:09,921:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:06:09,921:INFO: --------Save best model!--------
2026-01-21 15:06:14,517:INFO: Epoch: 4, train loss: 276.6010208129883
2026-01-21 15:06:14,839:INFO: Epoch: 4, dev loss: 298.10979715983075, f1 score: 0.525
2026-01-21 15:06:14,840:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:06:19,843:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:06:19,843:INFO: --------Save best model!--------
2026-01-21 15:06:24,380:INFO: Epoch: 5, train loss: 226.28843460083007
2026-01-21 15:06:24,695:INFO: Epoch: 5, dev loss: 288.9297180175781, f1 score: 0.3930131004366812
2026-01-21 15:06:29,232:INFO: Epoch: 6, train loss: 191.03441047668457
2026-01-21 15:06:29,556:INFO: Epoch: 6, dev loss: 313.68846638997394, f1 score: 0.6094420600858369
2026-01-21 15:06:29,557:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:06:33,161:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:06:33,162:INFO: --------Save best model!--------
2026-01-21 15:06:37,706:INFO: Epoch: 7, train loss: 138.6830307006836
2026-01-21 15:06:38,018:INFO: Epoch: 7, dev loss: 288.752192179362, f1 score: 0.570281124497992
2026-01-21 15:06:42,525:INFO: Epoch: 8, train loss: 111.32518043518067
2026-01-21 15:06:42,830:INFO: Epoch: 8, dev loss: 294.8258870442708, f1 score: 0.5760000000000001
2026-01-21 15:06:47,331:INFO: Epoch: 9, train loss: 98.13299331665038
2026-01-21 15:06:47,644:INFO: Epoch: 9, dev loss: 354.7922007242839, f1 score: 0.6133333333333333
2026-01-21 15:06:47,645:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:06:52,634:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:06:52,634:INFO: --------Save best model!--------
2026-01-21 15:06:57,185:INFO: Epoch: 10, train loss: 95.30934638977051
2026-01-21 15:06:57,500:INFO: Epoch: 10, dev loss: 300.5426839192708, f1 score: 0.5714285714285714
2026-01-21 15:07:01,977:INFO: Epoch: 11, train loss: 64.76918869018554
2026-01-21 15:07:02,290:INFO: Epoch: 11, dev loss: 276.7551778157552, f1 score: 0.6
2026-01-21 15:07:06,807:INFO: Epoch: 12, train loss: 63.48941879272461
2026-01-21 15:07:07,104:INFO: Epoch: 12, dev loss: 323.607426961263, f1 score: 0.6580086580086579
2026-01-21 15:07:07,105:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:07:11,083:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:07:11,086:INFO: --------Save best model!--------
2026-01-21 15:07:15,612:INFO: Epoch: 13, train loss: 45.34495162963867
2026-01-21 15:07:15,916:INFO: Epoch: 13, dev loss: 382.1846059163411, f1 score: 0.6428571428571429
2026-01-21 15:07:20,545:INFO: Epoch: 14, train loss: 41.77486572265625
2026-01-21 15:07:20,844:INFO: Epoch: 14, dev loss: 367.7538096110026, f1 score: 0.6782608695652174
2026-01-21 15:07:20,844:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:07:25,894:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:07:25,894:INFO: --------Save best model!--------
2026-01-21 15:07:30,619:INFO: Epoch: 15, train loss: 45.8639331817627
2026-01-21 15:07:30,934:INFO: Epoch: 15, dev loss: 326.9518229166667, f1 score: 0.6434782608695652
2026-01-21 15:07:35,579:INFO: Epoch: 16, train loss: 33.333034133911134
2026-01-21 15:07:35,902:INFO: Epoch: 16, dev loss: 391.2712097167969, f1 score: 0.646288209606987
2026-01-21 15:07:40,521:INFO: Epoch: 17, train loss: 33.11471366882324
2026-01-21 15:07:40,843:INFO: Epoch: 17, dev loss: 387.3387908935547, f1 score: 0.6580086580086579
2026-01-21 15:07:45,415:INFO: Epoch: 18, train loss: 27.3341423034668
2026-01-21 15:07:45,721:INFO: Epoch: 18, dev loss: 415.46800740559894, f1 score: 0.610878661087866
2026-01-21 15:07:50,357:INFO: Epoch: 19, train loss: 26.67297477722168
2026-01-21 15:07:50,677:INFO: Epoch: 19, dev loss: 445.71852620442706, f1 score: 0.6329113924050632
2026-01-21 15:07:55,428:INFO: Epoch: 20, train loss: 30.47610206604004
2026-01-21 15:07:55,747:INFO: Epoch: 20, dev loss: 458.77489217122394, f1 score: 0.6637554585152838
2026-01-21 15:08:00,404:INFO: Epoch: 21, train loss: 24.45016403198242
2026-01-21 15:08:00,731:INFO: Epoch: 21, dev loss: 409.361328125, f1 score: 0.6486486486486486
2026-01-21 15:08:05,778:INFO: Epoch: 22, train loss: 19.484183883666994
2026-01-21 15:08:06,095:INFO: Epoch: 22, dev loss: 470.9401550292969, f1 score: 0.6580086580086579
2026-01-21 15:08:10,677:INFO: Epoch: 23, train loss: 15.479944229125977
2026-01-21 15:08:10,981:INFO: Epoch: 23, dev loss: 518.046142578125, f1 score: 0.6375545851528384
2026-01-21 15:08:15,618:INFO: Epoch: 24, train loss: 14.88640480041504
2026-01-21 15:08:15,943:INFO: Epoch: 24, dev loss: 556.1866658528646, f1 score: 0.6212765957446809
2026-01-21 15:08:15,943:INFO: Best val f1: 0.6782608695652174
2026-01-21 15:08:15,943:INFO: Training Finished!
2026-01-21 15:08:15,956:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:08:15,956:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:08:15,956:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:08:15,956:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:08:15,957:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:08:15,957:INFO: loading file None
2026-01-21 15:08:15,957:INFO: loading file None
2026-01-21 15:08:15,957:INFO: loading file None
2026-01-21 15:08:16,090:INFO: --------Dataset Build!--------
2026-01-21 15:08:16,092:INFO: --------Get Data-loader!--------
2026-01-21 15:08:16,092:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:08:16,092:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:08:16,092:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:08:23,210:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 15:08:23,212:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:08:23,212:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:08:23,212:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:08:23,212:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:08:23,212:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:08:23,212:INFO: loading file None
2026-01-21 15:08:23,213:INFO: loading file None
2026-01-21 15:08:23,213:INFO: loading file None
2026-01-21 15:08:24,043:INFO: --------Bad Cases reserved !--------
2026-01-21 15:08:24,050:INFO: test loss: 391.1861877441406, f1 score: 0.6550522648083623
2026-01-21 15:08:24,050:INFO: f1 score of ACTION: 0.4695652173913043
2026-01-21 15:08:24,050:INFO: f1 score of LEVEL_KEY: 0.5684210526315789
2026-01-21 15:08:24,050:INFO: f1 score of OBJ: 0.6832298136645962
2026-01-21 15:08:24,050:INFO: f1 score of ORG: 0.7321428571428573
2026-01-21 15:08:24,050:INFO: f1 score of VALUE: 0.8351648351648352
2026-01-21 15:08:43,965:INFO: device: cuda:0
2026-01-21 15:08:43,965:INFO: --------Process Done!--------
2026-01-21 15:08:43,974:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:08:43,975:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:08:43,975:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:08:43,975:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:08:43,975:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:08:43,975:INFO: loading file None
2026-01-21 15:08:43,975:INFO: loading file None
2026-01-21 15:08:43,975:INFO: loading file None
2026-01-21 15:08:44,462:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:08:44,462:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:08:44,462:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:08:44,462:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:08:44,462:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:08:44,462:INFO: loading file None
2026-01-21 15:08:44,462:INFO: loading file None
2026-01-21 15:08:44,462:INFO: loading file None
2026-01-21 15:08:44,533:INFO: --------Dataset Build!--------
2026-01-21 15:08:44,533:INFO: --------Get Dataloader!--------
2026-01-21 15:08:44,534:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 15:08:44,534:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:08:44,534:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 15:08:53,219:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 15:08:53,219:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 15:08:56,841:INFO: --------Start Training!--------
2026-01-21 15:09:01,591:INFO: Epoch: 1, train loss: 1484.886279296875
2026-01-21 15:09:01,894:INFO: Epoch: 1, dev loss: 829.7525838216146, f1 score: 0
2026-01-21 15:09:06,324:INFO: Epoch: 2, train loss: 774.9336364746093
2026-01-21 15:09:06,626:INFO: Epoch: 2, dev loss: 497.54668680826825, f1 score: 0.18269230769230768
2026-01-21 15:09:06,627:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:09:11,340:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:09:11,341:INFO: --------Save best model!--------
2026-01-21 15:09:15,879:INFO: Epoch: 3, train loss: 428.1015106201172
2026-01-21 15:09:16,190:INFO: Epoch: 3, dev loss: 336.9481964111328, f1 score: 0.4401913875598086
2026-01-21 15:09:16,191:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:09:21,122:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:09:21,122:INFO: --------Save best model!--------
2026-01-21 15:09:25,614:INFO: Epoch: 4, train loss: 316.47318572998046
2026-01-21 15:09:25,917:INFO: Epoch: 4, dev loss: 289.1085459391276, f1 score: 0.5062240663900415
2026-01-21 15:09:25,918:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:09:30,913:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:09:30,914:INFO: --------Save best model!--------
2026-01-21 15:09:35,533:INFO: Epoch: 5, train loss: 238.56150970458984
2026-01-21 15:09:35,853:INFO: Epoch: 5, dev loss: 352.3983612060547, f1 score: 0.5482233502538071
2026-01-21 15:09:35,854:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:09:40,802:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:09:40,802:INFO: --------Save best model!--------
2026-01-21 15:09:45,370:INFO: Epoch: 6, train loss: 186.40543365478516
2026-01-21 15:09:45,684:INFO: Epoch: 6, dev loss: 326.6255798339844, f1 score: 0.4156862745098039
2026-01-21 15:09:50,250:INFO: Epoch: 7, train loss: 151.15299339294432
2026-01-21 15:09:50,565:INFO: Epoch: 7, dev loss: 234.71690368652344, f1 score: 0.56
2026-01-21 15:09:50,566:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:09:55,625:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:09:55,626:INFO: --------Save best model!--------
2026-01-21 15:10:00,391:INFO: Epoch: 8, train loss: 118.70128898620605
2026-01-21 15:10:00,703:INFO: Epoch: 8, dev loss: 336.06128946940106, f1 score: 0.5217391304347826
2026-01-21 15:10:05,292:INFO: Epoch: 9, train loss: 91.31401634216309
2026-01-21 15:10:05,590:INFO: Epoch: 9, dev loss: 383.44195048014325, f1 score: 0.5945945945945946
2026-01-21 15:10:05,591:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:10:10,516:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:10:10,516:INFO: --------Save best model!--------
2026-01-21 15:10:15,001:INFO: Epoch: 10, train loss: 76.08759918212891
2026-01-21 15:10:15,307:INFO: Epoch: 10, dev loss: 336.69642130533856, f1 score: 0.579185520361991
2026-01-21 15:10:19,887:INFO: Epoch: 11, train loss: 74.52552719116211
2026-01-21 15:10:20,202:INFO: Epoch: 11, dev loss: 310.23486328125, f1 score: 0.5991561181434599
2026-01-21 15:10:20,203:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:10:25,137:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:10:25,138:INFO: --------Save best model!--------
2026-01-21 15:10:29,676:INFO: Epoch: 12, train loss: 70.10389709472656
2026-01-21 15:10:29,980:INFO: Epoch: 12, dev loss: 337.9685567220052, f1 score: 0.6695652173913044
2026-01-21 15:10:29,981:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:10:34,901:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:10:34,901:INFO: --------Save best model!--------
2026-01-21 15:10:39,460:INFO: Epoch: 13, train loss: 55.41902313232422
2026-01-21 15:10:39,771:INFO: Epoch: 13, dev loss: 332.7316436767578, f1 score: 0.6434782608695652
2026-01-21 15:10:44,329:INFO: Epoch: 14, train loss: 45.84588356018067
2026-01-21 15:10:44,627:INFO: Epoch: 14, dev loss: 398.2498474121094, f1 score: 0.6972477064220183
2026-01-21 15:10:44,628:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:10:49,474:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:10:49,474:INFO: --------Save best model!--------
2026-01-21 15:10:53,893:INFO: Epoch: 15, train loss: 49.43476333618164
2026-01-21 15:10:54,197:INFO: Epoch: 15, dev loss: 392.76597595214844, f1 score: 0.6260869565217392
2026-01-21 15:10:58,637:INFO: Epoch: 16, train loss: 46.4342456817627
2026-01-21 15:10:58,948:INFO: Epoch: 16, dev loss: 381.86729939778644, f1 score: 0.610441767068273
2026-01-21 15:11:03,555:INFO: Epoch: 17, train loss: 37.305219650268555
2026-01-21 15:11:03,867:INFO: Epoch: 17, dev loss: 361.0960998535156, f1 score: 0.6725663716814159
2026-01-21 15:11:08,657:INFO: Epoch: 18, train loss: 33.5945327758789
2026-01-21 15:11:08,960:INFO: Epoch: 18, dev loss: 411.56456502278644, f1 score: 0.6696428571428571
2026-01-21 15:11:13,402:INFO: Epoch: 19, train loss: 34.70981979370117
2026-01-21 15:11:13,701:INFO: Epoch: 19, dev loss: 321.5267028808594, f1 score: 0.6554621848739496
2026-01-21 15:11:18,285:INFO: Epoch: 20, train loss: 32.858029174804685
2026-01-21 15:11:18,581:INFO: Epoch: 20, dev loss: 437.4835917154948, f1 score: 0.6607929515418502
2026-01-21 15:11:23,017:INFO: Epoch: 21, train loss: 19.80332794189453
2026-01-21 15:11:23,315:INFO: Epoch: 21, dev loss: 422.3036193847656, f1 score: 0.6576576576576576
2026-01-21 15:11:27,678:INFO: Epoch: 22, train loss: 26.166028594970705
2026-01-21 15:11:27,999:INFO: Epoch: 22, dev loss: 487.8779805501302, f1 score: 0.6695652173913044
2026-01-21 15:11:32,476:INFO: Epoch: 23, train loss: 23.4583251953125
2026-01-21 15:11:32,776:INFO: Epoch: 23, dev loss: 489.37353515625, f1 score: 0.6502057613168725
2026-01-21 15:11:37,222:INFO: Epoch: 24, train loss: 16.56764450073242
2026-01-21 15:11:37,531:INFO: Epoch: 24, dev loss: 498.8318277994792, f1 score: 0.6581196581196581
2026-01-21 15:11:37,531:INFO: Best val f1: 0.6972477064220183
2026-01-21 15:11:37,531:INFO: Training Finished!
2026-01-21 15:11:37,546:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:11:37,546:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:11:37,546:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:11:37,546:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:11:37,546:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:11:37,546:INFO: loading file None
2026-01-21 15:11:37,546:INFO: loading file None
2026-01-21 15:11:37,547:INFO: loading file None
2026-01-21 15:11:37,691:INFO: --------Dataset Build!--------
2026-01-21 15:11:37,692:INFO: --------Get Data-loader!--------
2026-01-21 15:11:37,692:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 15:11:37,692:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:11:37,692:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 15:11:46,136:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 15:11:46,138:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:11:46,138:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:11:46,138:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:11:46,138:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:11:46,138:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:11:46,138:INFO: loading file None
2026-01-21 15:11:46,138:INFO: loading file None
2026-01-21 15:11:46,138:INFO: loading file None
2026-01-21 15:11:46,958:INFO: --------Bad Cases reserved !--------
2026-01-21 15:11:46,965:INFO: test loss: 357.85477447509766, f1 score: 0.7087198515769945
2026-01-21 15:11:46,966:INFO: f1 score of ACTION: 0.44680851063829785
2026-01-21 15:11:46,966:INFO: f1 score of LEVEL_KEY: 0.6796116504854369
2026-01-21 15:11:46,966:INFO: f1 score of OBJ: 0.786206896551724
2026-01-21 15:11:46,966:INFO: f1 score of ORG: 0.7339449541284405
2026-01-21 15:11:46,966:INFO: f1 score of VALUE: 0.8636363636363636
2026-01-21 15:24:51,377:INFO: device: cuda:0
2026-01-21 15:24:51,378:INFO: --------Process Done!--------
2026-01-21 15:24:51,387:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:24:51,388:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:24:51,388:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:24:51,388:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:24:51,388:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:24:51,388:INFO: loading file None
2026-01-21 15:24:51,388:INFO: loading file None
2026-01-21 15:24:51,388:INFO: loading file None
2026-01-21 15:24:51,866:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 15:24:51,866:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 15:24:51,866:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 15:24:51,866:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 15:24:51,866:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 15:24:51,866:INFO: loading file None
2026-01-21 15:24:51,866:INFO: loading file None
2026-01-21 15:24:51,866:INFO: loading file None
2026-01-21 15:24:51,937:INFO: --------Dataset Build!--------
2026-01-21 15:24:51,937:INFO: --------Get Dataloader!--------
2026-01-21 15:24:51,937:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 15:24:51,938:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 15:24:51,938:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 15:25:00,594:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 15:25:00,594:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:08:53,249:INFO: device: cuda:0
2026-01-21 16:08:53,249:INFO: --------Process Done!--------
2026-01-21 16:08:53,256:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:08:53,256:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:08:53,256:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:08:53,256:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:08:53,256:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:08:53,256:INFO: loading file None
2026-01-21 16:08:53,256:INFO: loading file None
2026-01-21 16:08:53,256:INFO: loading file None
2026-01-21 16:08:53,567:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:08:53,567:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:08:53,567:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:08:53,567:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:08:53,568:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:08:53,568:INFO: loading file None
2026-01-21 16:08:53,568:INFO: loading file None
2026-01-21 16:08:53,568:INFO: loading file None
2026-01-21 16:08:53,609:INFO: --------Dataset Build!--------
2026-01-21 16:08:53,609:INFO: --------Get Dataloader!--------
2026-01-21 16:08:53,609:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:08:53,610:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:08:53,610:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:14:08,140:INFO: device: cuda:0
2026-01-21 16:14:08,140:INFO: --------Process Done!--------
2026-01-21 16:14:08,147:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:14:08,147:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:14:08,147:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:14:08,147:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:14:08,147:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:14:08,147:INFO: loading file None
2026-01-21 16:14:08,147:INFO: loading file None
2026-01-21 16:14:08,147:INFO: loading file None
2026-01-21 16:14:08,457:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:14:08,457:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:14:08,457:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:14:08,457:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:14:08,457:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:14:08,457:INFO: loading file None
2026-01-21 16:14:08,457:INFO: loading file None
2026-01-21 16:14:08,457:INFO: loading file None
2026-01-21 16:14:08,497:INFO: --------Dataset Build!--------
2026-01-21 16:14:08,497:INFO: --------Get Dataloader!--------
2026-01-21 16:14:08,497:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:14:08,497:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:14:08,497:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:28:21,529:INFO: device: cuda:0
2026-01-21 16:28:21,529:INFO: --------Process Done!--------
2026-01-21 16:28:21,535:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:28:21,536:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:28:21,536:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:28:21,536:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:28:21,536:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:28:21,536:INFO: loading file None
2026-01-21 16:28:21,536:INFO: loading file None
2026-01-21 16:28:21,536:INFO: loading file None
2026-01-21 16:28:21,849:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:28:21,849:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:28:21,849:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:28:21,849:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:28:21,849:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:28:21,849:INFO: loading file None
2026-01-21 16:28:21,849:INFO: loading file None
2026-01-21 16:28:21,849:INFO: loading file None
2026-01-21 16:28:21,889:INFO: --------Dataset Build!--------
2026-01-21 16:28:21,889:INFO: --------Get Dataloader!--------
2026-01-21 16:28:21,889:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:28:21,890:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:28:21,890:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:28:28,160:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:28:28,160:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:28:29,639:INFO: --------Start Training!--------
2026-01-21 16:28:33,755:INFO: Epoch: 1, train loss: 1633.4701904296876
2026-01-21 16:28:34,011:INFO: Epoch: 1, dev loss: 903.3063557942709, f1 score: 0
2026-01-21 16:28:37,465:INFO: Epoch: 2, train loss: 865.630746459961
2026-01-21 16:28:37,736:INFO: Epoch: 2, dev loss: 513.6609598795573, f1 score: 0.07777777777777778
2026-01-21 16:28:37,737:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:28:39,211:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:28:39,212:INFO: --------Save best model!--------
2026-01-21 16:28:42,509:INFO: Epoch: 3, train loss: 490.3045181274414
2026-01-21 16:28:42,762:INFO: Epoch: 3, dev loss: 351.73150126139325, f1 score: 0.38888888888888895
2026-01-21 16:28:42,763:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:28:45,285:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:28:45,286:INFO: --------Save best model!--------
2026-01-21 16:28:48,693:INFO: Epoch: 4, train loss: 312.2005485534668
2026-01-21 16:28:48,949:INFO: Epoch: 4, dev loss: 316.98302205403644, f1 score: 0.46491228070175433
2026-01-21 16:28:48,950:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:28:51,399:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:28:51,399:INFO: --------Save best model!--------
2026-01-21 16:28:54,747:INFO: Epoch: 5, train loss: 217.7440170288086
2026-01-21 16:28:55,018:INFO: Epoch: 5, dev loss: 349.4076639811198, f1 score: 0.456140350877193
2026-01-21 16:28:58,577:INFO: Epoch: 6, train loss: 267.90553283691406
2026-01-21 16:28:58,845:INFO: Epoch: 6, dev loss: 260.46393330891925, f1 score: 0.42231075697211157
2026-01-21 16:29:02,277:INFO: Epoch: 7, train loss: 167.17597045898438
2026-01-21 16:29:02,533:INFO: Epoch: 7, dev loss: 311.6790720621745, f1 score: 0.549800796812749
2026-01-21 16:29:02,533:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:29:05,042:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:29:05,043:INFO: --------Save best model!--------
2026-01-21 16:29:08,374:INFO: Epoch: 8, train loss: 117.48712997436523
2026-01-21 16:29:08,637:INFO: Epoch: 8, dev loss: 299.8392384847005, f1 score: 0.5423728813559322
2026-01-21 16:29:12,022:INFO: Epoch: 9, train loss: 100.22969131469726
2026-01-21 16:29:12,292:INFO: Epoch: 9, dev loss: 298.13580322265625, f1 score: 0.5485232067510549
2026-01-21 16:29:15,694:INFO: Epoch: 10, train loss: 92.57419166564941
2026-01-21 16:29:15,944:INFO: Epoch: 10, dev loss: 349.40916442871094, f1 score: 0.5306122448979592
2026-01-21 16:29:19,305:INFO: Epoch: 11, train loss: 69.60522232055663
2026-01-21 16:29:19,578:INFO: Epoch: 11, dev loss: 278.1166534423828, f1 score: 0.5787234042553191
2026-01-21 16:29:19,578:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:29:21,793:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:29:21,793:INFO: --------Save best model!--------
2026-01-21 16:29:25,189:INFO: Epoch: 12, train loss: 65.41508636474609
2026-01-21 16:29:25,443:INFO: Epoch: 12, dev loss: 298.29180908203125, f1 score: 0.6244725738396625
2026-01-21 16:29:25,444:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:29:27,581:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:29:27,581:INFO: --------Save best model!--------
2026-01-21 16:29:30,935:INFO: Epoch: 13, train loss: 50.58322982788086
2026-01-21 16:29:31,205:INFO: Epoch: 13, dev loss: 329.55458577473956, f1 score: 0.6379310344827587
2026-01-21 16:29:31,205:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:29:33,620:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:29:33,621:INFO: --------Save best model!--------
2026-01-21 16:29:36,834:INFO: Epoch: 14, train loss: 39.6222957611084
2026-01-21 16:29:37,106:INFO: Epoch: 14, dev loss: 335.78660583496094, f1 score: 0.5932203389830509
2026-01-21 16:29:40,453:INFO: Epoch: 15, train loss: 34.5775650024414
2026-01-21 16:29:40,716:INFO: Epoch: 15, dev loss: 322.06284586588544, f1 score: 0.6229508196721312
2026-01-21 16:29:44,102:INFO: Epoch: 16, train loss: 35.54142265319824
2026-01-21 16:29:44,355:INFO: Epoch: 16, dev loss: 343.5875549316406, f1 score: 0.6390041493775933
2026-01-21 16:29:44,356:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:29:46,833:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:29:46,833:INFO: --------Save best model!--------
2026-01-21 16:29:50,236:INFO: Epoch: 17, train loss: 29.26065673828125
2026-01-21 16:29:50,488:INFO: Epoch: 17, dev loss: 329.4198506673177, f1 score: 0.6320346320346321
2026-01-21 16:29:53,842:INFO: Epoch: 18, train loss: 33.82666778564453
2026-01-21 16:29:54,086:INFO: Epoch: 18, dev loss: 414.1086832682292, f1 score: 0.6294820717131475
2026-01-21 16:29:57,492:INFO: Epoch: 19, train loss: 27.792900848388673
2026-01-21 16:29:57,754:INFO: Epoch: 19, dev loss: 416.9830271402995, f1 score: 0.6188340807174888
2026-01-21 16:30:01,169:INFO: Epoch: 20, train loss: 26.870903778076173
2026-01-21 16:30:01,425:INFO: Epoch: 20, dev loss: 407.09508260091144, f1 score: 0.6371681415929203
2026-01-21 16:30:04,803:INFO: Epoch: 21, train loss: 19.65934066772461
2026-01-21 16:30:05,074:INFO: Epoch: 21, dev loss: 452.14825439453125, f1 score: 0.6609442060085837
2026-01-21 16:30:05,075:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:30:07,522:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:30:07,523:INFO: --------Save best model!--------
2026-01-21 16:30:10,914:INFO: Epoch: 22, train loss: 16.566682052612304
2026-01-21 16:30:11,182:INFO: Epoch: 22, dev loss: 456.6198221842448, f1 score: 0.6396396396396397
2026-01-21 16:30:14,554:INFO: Epoch: 23, train loss: 12.257748794555663
2026-01-21 16:30:14,808:INFO: Epoch: 23, dev loss: 484.9578857421875, f1 score: 0.6375545851528384
2026-01-21 16:30:18,137:INFO: Epoch: 24, train loss: 12.733731842041015
2026-01-21 16:30:18,397:INFO: Epoch: 24, dev loss: 451.6677652994792, f1 score: 0.6200873362445415
2026-01-21 16:30:21,839:INFO: Epoch: 25, train loss: 10.114966201782227
2026-01-21 16:30:22,098:INFO: Epoch: 25, dev loss: 493.00829060872394, f1 score: 0.6493506493506492
2026-01-21 16:30:25,572:INFO: Epoch: 26, train loss: 10.926132202148438
2026-01-21 16:30:25,825:INFO: Epoch: 26, dev loss: 470.9035237630208, f1 score: 0.6666666666666667
2026-01-21 16:30:25,826:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:30:28,280:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:30:28,280:INFO: --------Save best model!--------
2026-01-21 16:30:31,585:INFO: Epoch: 27, train loss: 8.59344940185547
2026-01-21 16:30:31,837:INFO: Epoch: 27, dev loss: 522.6339569091797, f1 score: 0.6465517241379309
2026-01-21 16:30:35,112:INFO: Epoch: 28, train loss: 8.671114349365235
2026-01-21 16:30:35,360:INFO: Epoch: 28, dev loss: 564.8433939615885, f1 score: 0.6550218340611355
2026-01-21 16:30:38,701:INFO: Epoch: 29, train loss: 5.762831115722657
2026-01-21 16:30:38,972:INFO: Epoch: 29, dev loss: 578.2699584960938, f1 score: 0.6255506607929515
2026-01-21 16:30:42,420:INFO: Epoch: 30, train loss: 5.373333358764649
2026-01-21 16:30:42,693:INFO: Epoch: 30, dev loss: 618.0237731933594, f1 score: 0.6052631578947368
2026-01-21 16:30:46,077:INFO: Epoch: 31, train loss: 4.67103157043457
2026-01-21 16:30:46,330:INFO: Epoch: 31, dev loss: 655.0927327473959, f1 score: 0.6133333333333333
2026-01-21 16:30:49,689:INFO: Epoch: 32, train loss: 3.3017433166503904
2026-01-21 16:30:49,950:INFO: Epoch: 32, dev loss: 652.9381612141927, f1 score: 0.6371681415929203
2026-01-21 16:30:53,406:INFO: Epoch: 33, train loss: 2.238821792602539
2026-01-21 16:30:53,653:INFO: Epoch: 33, dev loss: 646.3360087076823, f1 score: 0.6431718061674009
2026-01-21 16:30:57,009:INFO: Epoch: 34, train loss: 3.237116241455078
2026-01-21 16:30:57,259:INFO: Epoch: 34, dev loss: 666.0604960123698, f1 score: 0.64
2026-01-21 16:31:00,434:INFO: Epoch: 35, train loss: 1.762079620361328
2026-01-21 16:31:00,709:INFO: Epoch: 35, dev loss: 675.3048502604166, f1 score: 0.64
2026-01-21 16:31:04,081:INFO: Epoch: 36, train loss: 1.08133544921875
2026-01-21 16:31:04,329:INFO: Epoch: 36, dev loss: 673.9465484619141, f1 score: 0.64
2026-01-21 16:31:04,329:INFO: Best val f1: 0.6666666666666667
2026-01-21 16:31:04,329:INFO: Training Finished!
2026-01-21 16:31:04,337:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:31:04,338:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:31:04,338:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:31:04,338:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:31:04,338:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:31:04,338:INFO: loading file None
2026-01-21 16:31:04,338:INFO: loading file None
2026-01-21 16:31:04,338:INFO: loading file None
2026-01-21 16:31:04,412:INFO: --------Dataset Build!--------
2026-01-21 16:31:04,412:INFO: --------Get Data-loader!--------
2026-01-21 16:31:04,413:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:31:04,413:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:31:04,413:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:31:09,656:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:31:09,657:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:31:09,657:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:31:09,657:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:31:09,657:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:31:09,657:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:31:09,657:INFO: loading file None
2026-01-21 16:31:09,657:INFO: loading file None
2026-01-21 16:31:09,657:INFO: loading file None
2026-01-21 16:31:10,356:INFO: --------Bad Cases reserved !--------
2026-01-21 16:31:10,361:INFO: test loss: 521.3263651529948, f1 score: 0.7262969588550985
2026-01-21 16:31:10,361:INFO: f1 score of ACTION: 0.5686274509803921
2026-01-21 16:31:10,361:INFO: f1 score of LEVEL_KEY: 0.6796116504854369
2026-01-21 16:31:10,361:INFO: f1 score of OBJ: 0.7516778523489933
2026-01-21 16:31:10,361:INFO: f1 score of ORG: 0.7586206896551724
2026-01-21 16:31:10,361:INFO: f1 score of VALUE: 0.8764044943820224
2026-01-21 16:35:21,850:INFO: device: cuda:0
2026-01-21 16:35:21,850:INFO: --------Process Done!--------
2026-01-21 16:35:21,856:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:35:21,857:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:35:21,857:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:35:21,857:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:35:21,857:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:35:21,857:INFO: loading file None
2026-01-21 16:35:21,857:INFO: loading file None
2026-01-21 16:35:21,857:INFO: loading file None
2026-01-21 16:35:22,169:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:35:22,169:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:35:22,169:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:35:22,169:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:35:22,169:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:35:22,170:INFO: loading file None
2026-01-21 16:35:22,170:INFO: loading file None
2026-01-21 16:35:22,170:INFO: loading file None
2026-01-21 16:35:22,210:INFO: --------Dataset Build!--------
2026-01-21 16:35:22,210:INFO: --------Get Dataloader!--------
2026-01-21 16:35:22,210:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:35:22,210:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:35:22,211:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:35:29,178:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:35:29,178:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:35:30,816:INFO: --------Start Training!--------
2026-01-21 16:35:34,784:INFO: Epoch: 1, train loss: 1483.1071044921875
2026-01-21 16:35:35,035:INFO: Epoch: 1, dev loss: 781.0732218424479, f1 score: 0
2026-01-21 16:35:38,385:INFO: Epoch: 2, train loss: 720.8529602050781
2026-01-21 16:35:38,657:INFO: Epoch: 2, dev loss: 458.71868896484375, f1 score: 0.187192118226601
2026-01-21 16:35:38,658:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:35:41,408:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:35:41,409:INFO: --------Save best model!--------
2026-01-21 16:35:44,815:INFO: Epoch: 3, train loss: 410.1599822998047
2026-01-21 16:35:45,086:INFO: Epoch: 3, dev loss: 349.66466267903644, f1 score: 0.3714285714285714
2026-01-21 16:35:45,087:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:35:46,300:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:35:46,300:INFO: --------Save best model!--------
2026-01-21 16:35:49,753:INFO: Epoch: 4, train loss: 310.40894546508787
2026-01-21 16:35:50,006:INFO: Epoch: 4, dev loss: 378.0263188680013, f1 score: 0.4229074889867841
2026-01-21 16:35:50,006:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:35:52,458:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:35:52,458:INFO: --------Save best model!--------
2026-01-21 16:35:55,816:INFO: Epoch: 5, train loss: 239.33647079467772
2026-01-21 16:35:56,086:INFO: Epoch: 5, dev loss: 304.2665608723958, f1 score: 0.4691358024691358
2026-01-21 16:35:56,087:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:35:58,573:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:35:58,573:INFO: --------Save best model!--------
2026-01-21 16:36:01,954:INFO: Epoch: 6, train loss: 188.37908096313475
2026-01-21 16:36:02,224:INFO: Epoch: 6, dev loss: 308.2290802001953, f1 score: 0.5163934426229508
2026-01-21 16:36:02,225:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:04,659:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:04,659:INFO: --------Save best model!--------
2026-01-21 16:36:08,185:INFO: Epoch: 7, train loss: 163.29020042419432
2026-01-21 16:36:08,455:INFO: Epoch: 7, dev loss: 263.1710713704427, f1 score: 0.5702127659574466
2026-01-21 16:36:08,455:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:11,275:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:11,275:INFO: --------Save best model!--------
2026-01-21 16:36:14,599:INFO: Epoch: 8, train loss: 107.97751426696777
2026-01-21 16:36:14,855:INFO: Epoch: 8, dev loss: 356.48081970214844, f1 score: 0.5762711864406779
2026-01-21 16:36:14,856:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:17,328:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:17,329:INFO: --------Save best model!--------
2026-01-21 16:36:20,685:INFO: Epoch: 9, train loss: 91.94590034484864
2026-01-21 16:36:20,935:INFO: Epoch: 9, dev loss: 320.1404774983724, f1 score: 0.5495495495495496
2026-01-21 16:36:24,241:INFO: Epoch: 10, train loss: 82.61326484680175
2026-01-21 16:36:24,513:INFO: Epoch: 10, dev loss: 337.1497751871745, f1 score: 0.5803921568627451
2026-01-21 16:36:24,513:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:26,679:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:26,679:INFO: --------Save best model!--------
2026-01-21 16:36:29,998:INFO: Epoch: 11, train loss: 81.9274112701416
2026-01-21 16:36:30,268:INFO: Epoch: 11, dev loss: 306.29896036783856, f1 score: 0.560344827586207
2026-01-21 16:36:33,704:INFO: Epoch: 12, train loss: 59.61132926940918
2026-01-21 16:36:33,978:INFO: Epoch: 12, dev loss: 358.5294240315755, f1 score: 0.5966386554621849
2026-01-21 16:36:33,979:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:36,132:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:36,133:INFO: --------Save best model!--------
2026-01-21 16:36:39,578:INFO: Epoch: 13, train loss: 52.48441162109375
2026-01-21 16:36:39,826:INFO: Epoch: 13, dev loss: 406.7092742919922, f1 score: 0.5941422594142258
2026-01-21 16:36:43,186:INFO: Epoch: 14, train loss: 42.93167991638184
2026-01-21 16:36:43,433:INFO: Epoch: 14, dev loss: 393.75657145182294, f1 score: 0.5873015873015873
2026-01-21 16:36:46,803:INFO: Epoch: 15, train loss: 46.777526473999025
2026-01-21 16:36:47,066:INFO: Epoch: 15, dev loss: 413.5183359781901, f1 score: 0.5726141078838175
2026-01-21 16:36:50,507:INFO: Epoch: 16, train loss: 42.67937049865723
2026-01-21 16:36:50,781:INFO: Epoch: 16, dev loss: 385.59865315755206, f1 score: 0.6172839506172839
2026-01-21 16:36:50,781:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:53,269:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:53,269:INFO: --------Save best model!--------
2026-01-21 16:36:56,609:INFO: Epoch: 17, train loss: 33.6478775024414
2026-01-21 16:36:56,878:INFO: Epoch: 17, dev loss: 383.24756876627606, f1 score: 0.641025641025641
2026-01-21 16:36:56,879:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:36:59,703:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:36:59,703:INFO: --------Save best model!--------
2026-01-21 16:37:03,168:INFO: Epoch: 18, train loss: 28.745295333862305
2026-01-21 16:37:03,427:INFO: Epoch: 18, dev loss: 450.2033996582031, f1 score: 0.5760000000000001
2026-01-21 16:37:06,858:INFO: Epoch: 19, train loss: 30.576508712768554
2026-01-21 16:37:07,125:INFO: Epoch: 19, dev loss: 454.1283365885417, f1 score: 0.6153846153846154
2026-01-21 16:37:10,429:INFO: Epoch: 20, train loss: 24.68572540283203
2026-01-21 16:37:10,694:INFO: Epoch: 20, dev loss: 424.1907653808594, f1 score: 0.6127659574468085
2026-01-21 16:37:14,110:INFO: Epoch: 21, train loss: 24.97107048034668
2026-01-21 16:37:14,377:INFO: Epoch: 21, dev loss: 425.8795572916667, f1 score: 0.6666666666666666
2026-01-21 16:37:14,377:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:37:16,833:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:37:16,833:INFO: --------Save best model!--------
2026-01-21 16:37:20,165:INFO: Epoch: 22, train loss: 14.075713729858398
2026-01-21 16:37:20,417:INFO: Epoch: 22, dev loss: 419.978515625, f1 score: 0.6233766233766234
2026-01-21 16:37:23,823:INFO: Epoch: 23, train loss: 13.275383377075196
2026-01-21 16:37:24,065:INFO: Epoch: 23, dev loss: 509.7793477376302, f1 score: 0.6605504587155964
2026-01-21 16:37:27,434:INFO: Epoch: 24, train loss: 11.807948684692382
2026-01-21 16:37:27,693:INFO: Epoch: 24, dev loss: 494.48658243815106, f1 score: 0.6083333333333333
2026-01-21 16:37:31,069:INFO: Epoch: 25, train loss: 13.93963165283203
2026-01-21 16:37:31,318:INFO: Epoch: 25, dev loss: 514.9738362630209, f1 score: 0.6141078838174274
2026-01-21 16:37:34,659:INFO: Epoch: 26, train loss: 10.029029083251952
2026-01-21 16:37:34,916:INFO: Epoch: 26, dev loss: 570.2034403483073, f1 score: 0.6460176991150443
2026-01-21 16:37:38,340:INFO: Epoch: 27, train loss: 9.630523300170898
2026-01-21 16:37:38,614:INFO: Epoch: 27, dev loss: 540.9337565104166, f1 score: 0.6266094420600858
2026-01-21 16:37:41,995:INFO: Epoch: 28, train loss: 7.299569320678711
2026-01-21 16:37:42,266:INFO: Epoch: 28, dev loss: 552.5898132324219, f1 score: 0.6283185840707964
2026-01-21 16:37:45,704:INFO: Epoch: 29, train loss: 6.238799285888672
2026-01-21 16:37:45,979:INFO: Epoch: 29, dev loss: 573.8724568684896, f1 score: 0.6320346320346321
2026-01-21 16:37:49,421:INFO: Epoch: 30, train loss: 5.598745346069336
2026-01-21 16:37:49,689:INFO: Epoch: 30, dev loss: 612.2727864583334, f1 score: 0.6206896551724138
2026-01-21 16:37:53,195:INFO: Epoch: 31, train loss: 4.722497177124024
2026-01-21 16:37:53,458:INFO: Epoch: 31, dev loss: 618.0615437825521, f1 score: 0.6607142857142857
2026-01-21 16:37:53,458:INFO: Best val f1: 0.6666666666666666
2026-01-21 16:37:53,458:INFO: Training Finished!
2026-01-21 16:37:53,464:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:37:53,464:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:37:53,464:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:37:53,464:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:37:53,464:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:37:53,464:INFO: loading file None
2026-01-21 16:37:53,464:INFO: loading file None
2026-01-21 16:37:53,464:INFO: loading file None
2026-01-21 16:37:53,540:INFO: --------Dataset Build!--------
2026-01-21 16:37:53,541:INFO: --------Get Data-loader!--------
2026-01-21 16:37:53,541:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:37:53,541:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:37:53,541:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:37:58,782:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:37:58,783:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:37:58,783:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:37:58,783:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:37:58,783:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:37:58,783:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:37:58,783:INFO: loading file None
2026-01-21 16:37:58,783:INFO: loading file None
2026-01-21 16:37:58,783:INFO: loading file None
2026-01-21 16:37:59,482:INFO: --------Bad Cases reserved !--------
2026-01-21 16:37:59,486:INFO: test loss: 374.49938201904297, f1 score: 0.6799999999999999
2026-01-21 16:37:59,486:INFO: f1 score of ACTION: 0.4954128440366973
2026-01-21 16:37:59,486:INFO: f1 score of LEVEL_KEY: 0.6862745098039215
2026-01-21 16:37:59,486:INFO: f1 score of OBJ: 0.8201438848920863
2026-01-21 16:37:59,486:INFO: f1 score of ORG: 0.6666666666666667
2026-01-21 16:37:59,486:INFO: f1 score of VALUE: 0.6966292134831461
2026-01-21 16:40:35,345:INFO: device: cuda:0
2026-01-21 16:40:35,345:INFO: --------Process Done!--------
2026-01-21 16:40:35,352:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:40:35,352:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:40:35,352:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:40:35,352:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:40:35,352:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:40:35,352:INFO: loading file None
2026-01-21 16:40:35,352:INFO: loading file None
2026-01-21 16:40:35,352:INFO: loading file None
2026-01-21 16:40:35,658:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:40:35,658:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:40:35,658:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:40:35,658:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:40:35,658:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:40:35,658:INFO: loading file None
2026-01-21 16:40:35,658:INFO: loading file None
2026-01-21 16:40:35,658:INFO: loading file None
2026-01-21 16:40:35,699:INFO: --------Dataset Build!--------
2026-01-21 16:40:35,699:INFO: --------Get Dataloader!--------
2026-01-21 16:40:35,699:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:40:35,700:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:40:35,700:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:40:42,380:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:40:42,381:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:40:43,896:INFO: --------Start Training!--------
2026-01-21 16:40:47,936:INFO: Epoch: 1, train loss: 1423.238229370117
2026-01-21 16:40:48,198:INFO: Epoch: 1, dev loss: 785.4474792480469, f1 score: 0
2026-01-21 16:40:51,702:INFO: Epoch: 2, train loss: 753.1308776855469
2026-01-21 16:40:51,954:INFO: Epoch: 2, dev loss: 507.6783752441406, f1 score: 0.13333333333333336
2026-01-21 16:40:51,955:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:40:53,155:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:40:53,156:INFO: --------Save best model!--------
2026-01-21 16:40:56,587:INFO: Epoch: 3, train loss: 443.52456817626955
2026-01-21 16:40:56,852:INFO: Epoch: 3, dev loss: 326.68585205078125, f1 score: 0.3033175355450237
2026-01-21 16:40:56,853:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:40:59,542:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:40:59,543:INFO: --------Save best model!--------
2026-01-21 16:41:03,174:INFO: Epoch: 4, train loss: 311.1633491516113
2026-01-21 16:41:03,467:INFO: Epoch: 4, dev loss: 284.7118886311849, f1 score: 0.5373134328358209
2026-01-21 16:41:03,468:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:41:05,866:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:41:05,867:INFO: --------Save best model!--------
2026-01-21 16:41:09,379:INFO: Epoch: 5, train loss: 232.3301223754883
2026-01-21 16:41:09,669:INFO: Epoch: 5, dev loss: 237.67899576822916, f1 score: 0.4672131147540983
2026-01-21 16:41:13,170:INFO: Epoch: 6, train loss: 195.2574520111084
2026-01-21 16:41:13,445:INFO: Epoch: 6, dev loss: 256.4480845133464, f1 score: 0.5132743362831859
2026-01-21 16:41:17,202:INFO: Epoch: 7, train loss: 144.14301261901855
2026-01-21 16:41:17,480:INFO: Epoch: 7, dev loss: 278.961919148763, f1 score: 0.5064377682403433
2026-01-21 16:41:20,911:INFO: Epoch: 8, train loss: 123.70186843872071
2026-01-21 16:41:21,188:INFO: Epoch: 8, dev loss: 290.8460693359375, f1 score: 0.6060606060606061
2026-01-21 16:41:21,189:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:41:23,598:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:41:23,598:INFO: --------Save best model!--------
2026-01-21 16:41:27,138:INFO: Epoch: 9, train loss: 94.09948120117187
2026-01-21 16:41:27,424:INFO: Epoch: 9, dev loss: 256.0002746582031, f1 score: 0.6260869565217392
2026-01-21 16:41:27,425:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:41:29,778:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:41:29,778:INFO: --------Save best model!--------
2026-01-21 16:41:33,350:INFO: Epoch: 10, train loss: 82.93773574829102
2026-01-21 16:41:33,640:INFO: Epoch: 10, dev loss: 265.79637145996094, f1 score: 0.5760000000000001
2026-01-21 16:41:37,216:INFO: Epoch: 11, train loss: 71.83701400756836
2026-01-21 16:41:37,502:INFO: Epoch: 11, dev loss: 283.35619099934894, f1 score: 0.6065573770491803
2026-01-21 16:41:41,021:INFO: Epoch: 12, train loss: 67.20035400390626
2026-01-21 16:41:41,290:INFO: Epoch: 12, dev loss: 270.0343017578125, f1 score: 0.6186440677966102
2026-01-21 16:41:44,755:INFO: Epoch: 13, train loss: 52.855338668823244
2026-01-21 16:41:45,025:INFO: Epoch: 13, dev loss: 297.84600321451825, f1 score: 0.6491228070175439
2026-01-21 16:41:45,025:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:41:47,385:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:41:47,385:INFO: --------Save best model!--------
2026-01-21 16:41:50,781:INFO: Epoch: 14, train loss: 47.172949600219724
2026-01-21 16:41:51,067:INFO: Epoch: 14, dev loss: 329.6819254557292, f1 score: 0.6212765957446809
2026-01-21 16:41:54,621:INFO: Epoch: 15, train loss: 43.31457176208496
2026-01-21 16:41:54,892:INFO: Epoch: 15, dev loss: 366.9684753417969, f1 score: 0.6493506493506492
2026-01-21 16:41:54,893:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:41:57,203:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:41:57,204:INFO: --------Save best model!--------
2026-01-21 16:42:00,766:INFO: Epoch: 16, train loss: 41.076622772216794
2026-01-21 16:42:01,057:INFO: Epoch: 16, dev loss: 407.6144205729167, f1 score: 0.5606694560669456
2026-01-21 16:42:04,684:INFO: Epoch: 17, train loss: 40.37171859741211
2026-01-21 16:42:04,952:INFO: Epoch: 17, dev loss: 368.4500681559245, f1 score: 0.6206896551724138
2026-01-21 16:42:08,333:INFO: Epoch: 18, train loss: 34.64358673095703
2026-01-21 16:42:08,610:INFO: Epoch: 18, dev loss: 337.6380259195964, f1 score: 0.6493506493506492
2026-01-21 16:42:12,197:INFO: Epoch: 19, train loss: 24.964497756958007
2026-01-21 16:42:12,469:INFO: Epoch: 19, dev loss: 437.3877461751302, f1 score: 0.6379310344827587
2026-01-21 16:42:16,092:INFO: Epoch: 20, train loss: 25.244713973999023
2026-01-21 16:42:16,363:INFO: Epoch: 20, dev loss: 388.6776936848958, f1 score: 0.6233766233766234
2026-01-21 16:42:19,887:INFO: Epoch: 21, train loss: 21.30930976867676
2026-01-21 16:42:20,174:INFO: Epoch: 21, dev loss: 387.4608561197917, f1 score: 0.6367713004484306
2026-01-21 16:42:23,773:INFO: Epoch: 22, train loss: 19.45207824707031
2026-01-21 16:42:24,062:INFO: Epoch: 22, dev loss: 404.9939270019531, f1 score: 0.6212765957446809
2026-01-21 16:42:27,599:INFO: Epoch: 23, train loss: 21.056293487548828
2026-01-21 16:42:27,886:INFO: Epoch: 23, dev loss: 412.30255126953125, f1 score: 0.6260869565217392
2026-01-21 16:42:31,248:INFO: Epoch: 24, train loss: 17.115719604492188
2026-01-21 16:42:31,511:INFO: Epoch: 24, dev loss: 424.9188588460286, f1 score: 0.6460176991150443
2026-01-21 16:42:34,968:INFO: Epoch: 25, train loss: 17.64741439819336
2026-01-21 16:42:35,255:INFO: Epoch: 25, dev loss: 448.9155731201172, f1 score: 0.6666666666666667
2026-01-21 16:42:35,256:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:42:37,546:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:42:37,546:INFO: --------Save best model!--------
2026-01-21 16:42:41,108:INFO: Epoch: 26, train loss: 13.330951309204101
2026-01-21 16:42:41,401:INFO: Epoch: 26, dev loss: 508.44040934244794, f1 score: 0.6343612334801763
2026-01-21 16:42:44,875:INFO: Epoch: 27, train loss: 13.458813858032226
2026-01-21 16:42:45,174:INFO: Epoch: 27, dev loss: 445.61375935872394, f1 score: 0.6434782608695652
2026-01-21 16:42:48,613:INFO: Epoch: 28, train loss: 11.970916748046875
2026-01-21 16:42:48,902:INFO: Epoch: 28, dev loss: 538.2333272298177, f1 score: 0.6334841628959276
2026-01-21 16:42:52,450:INFO: Epoch: 29, train loss: 16.87989387512207
2026-01-21 16:42:52,725:INFO: Epoch: 29, dev loss: 459.7151641845703, f1 score: 0.6266094420600858
2026-01-21 16:42:56,271:INFO: Epoch: 30, train loss: 9.592739868164063
2026-01-21 16:42:56,549:INFO: Epoch: 30, dev loss: 460.0197448730469, f1 score: 0.6153846153846154
2026-01-21 16:43:00,056:INFO: Epoch: 31, train loss: 9.023667907714843
2026-01-21 16:43:00,347:INFO: Epoch: 31, dev loss: 528.5181070963541, f1 score: 0.6200873362445415
2026-01-21 16:43:04,054:INFO: Epoch: 32, train loss: 4.6749927520751955
2026-01-21 16:43:04,337:INFO: Epoch: 32, dev loss: 476.6637420654297, f1 score: 0.6260869565217392
2026-01-21 16:43:07,963:INFO: Epoch: 33, train loss: 4.73632583618164
2026-01-21 16:43:08,235:INFO: Epoch: 33, dev loss: 489.80029296875, f1 score: 0.6008583690987125
2026-01-21 16:43:11,708:INFO: Epoch: 34, train loss: 2.2252838134765627
2026-01-21 16:43:11,995:INFO: Epoch: 34, dev loss: 504.03643798828125, f1 score: 0.5982905982905983
2026-01-21 16:43:15,583:INFO: Epoch: 35, train loss: 1.9328086853027344
2026-01-21 16:43:15,861:INFO: Epoch: 35, dev loss: 536.7797648111979, f1 score: 0.639269406392694
2026-01-21 16:43:15,861:INFO: Best val f1: 0.6666666666666667
2026-01-21 16:43:15,861:INFO: Training Finished!
2026-01-21 16:43:15,871:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:43:15,872:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:43:15,872:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:43:15,872:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:43:15,872:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:43:15,872:INFO: loading file None
2026-01-21 16:43:15,872:INFO: loading file None
2026-01-21 16:43:15,872:INFO: loading file None
2026-01-21 16:43:15,945:INFO: --------Dataset Build!--------
2026-01-21 16:43:15,945:INFO: --------Get Data-loader!--------
2026-01-21 16:43:15,945:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:43:15,946:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:43:15,946:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:43:21,123:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:43:21,124:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:43:21,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:43:21,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:43:21,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:43:21,124:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:43:21,124:INFO: loading file None
2026-01-21 16:43:21,124:INFO: loading file None
2026-01-21 16:43:21,124:INFO: loading file None
2026-01-21 16:43:21,878:INFO: --------Bad Cases reserved !--------
2026-01-21 16:43:21,883:INFO: test loss: 544.8814544677734, f1 score: 0.6764705882352942
2026-01-21 16:43:21,883:INFO: f1 score of ACTION: 0.48936170212765956
2026-01-21 16:43:21,883:INFO: f1 score of LEVEL_KEY: 0.5918367346938774
2026-01-21 16:43:21,883:INFO: f1 score of OBJ: 0.7651006711409397
2026-01-21 16:43:21,883:INFO: f1 score of ORG: 0.6486486486486486
2026-01-21 16:43:21,883:INFO: f1 score of VALUE: 0.8478260869565216
2026-01-21 16:43:55,219:INFO: device: cuda:0
2026-01-21 16:43:55,219:INFO: --------Process Done!--------
2026-01-21 16:43:55,225:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:43:55,225:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:43:55,226:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:43:55,226:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:43:55,226:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:43:55,226:INFO: loading file None
2026-01-21 16:43:55,226:INFO: loading file None
2026-01-21 16:43:55,226:INFO: loading file None
2026-01-21 16:43:55,548:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:43:55,548:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:43:55,548:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:43:55,548:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:43:55,548:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:43:55,548:INFO: loading file None
2026-01-21 16:43:55,548:INFO: loading file None
2026-01-21 16:43:55,548:INFO: loading file None
2026-01-21 16:43:55,590:INFO: --------Dataset Build!--------
2026-01-21 16:43:55,590:INFO: --------Get Dataloader!--------
2026-01-21 16:43:55,590:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:43:55,590:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:43:55,591:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:44:01,890:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:44:01,891:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:44:03,326:INFO: --------Start Training!--------
2026-01-21 16:44:07,354:INFO: Epoch: 1, train loss: 1434.8288848876953
2026-01-21 16:44:07,619:INFO: Epoch: 1, dev loss: 824.2631022135416, f1 score: 0
2026-01-21 16:44:10,970:INFO: Epoch: 2, train loss: 736.0568237304688
2026-01-21 16:44:11,233:INFO: Epoch: 2, dev loss: 453.2444661458333, f1 score: 0.30042918454935624
2026-01-21 16:44:11,234:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:44:13,368:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:44:13,368:INFO: --------Save best model!--------
2026-01-21 16:44:16,716:INFO: Epoch: 3, train loss: 449.6301834106445
2026-01-21 16:44:16,968:INFO: Epoch: 3, dev loss: 551.8707885742188, f1 score: 0.30857142857142855
2026-01-21 16:44:16,968:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:44:19,382:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:44:19,382:INFO: --------Save best model!--------
2026-01-21 16:44:22,711:INFO: Epoch: 4, train loss: 372.2636070251465
2026-01-21 16:44:22,966:INFO: Epoch: 4, dev loss: 394.6953938802083, f1 score: 0.43192488262910794
2026-01-21 16:44:22,966:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:44:25,384:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:44:25,384:INFO: --------Save best model!--------
2026-01-21 16:44:28,663:INFO: Epoch: 5, train loss: 276.9849884033203
2026-01-21 16:44:28,914:INFO: Epoch: 5, dev loss: 319.6404317220052, f1 score: 0.49760765550239233
2026-01-21 16:44:28,915:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:44:31,344:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:44:31,345:INFO: --------Save best model!--------
2026-01-21 16:44:34,715:INFO: Epoch: 6, train loss: 223.83303680419922
2026-01-21 16:44:34,959:INFO: Epoch: 6, dev loss: 442.6315002441406, f1 score: 0.4466019417475728
2026-01-21 16:44:38,290:INFO: Epoch: 7, train loss: 306.4334144592285
2026-01-21 16:44:38,542:INFO: Epoch: 7, dev loss: 281.3066762288411, f1 score: 0.48510638297872344
2026-01-21 16:44:41,912:INFO: Epoch: 8, train loss: 161.68666381835936
2026-01-21 16:44:42,185:INFO: Epoch: 8, dev loss: 267.6466369628906, f1 score: 0.44052863436123346
2026-01-21 16:44:45,524:INFO: Epoch: 9, train loss: 145.87392044067383
2026-01-21 16:44:45,776:INFO: Epoch: 9, dev loss: 445.0362854003906, f1 score: 0.23236514522821577
2026-01-21 16:44:49,082:INFO: Epoch: 10, train loss: 594.6035095214844
2026-01-21 16:44:49,355:INFO: Epoch: 10, dev loss: 1014.4594370524088, f1 score: 0.33653846153846156
2026-01-21 16:44:52,733:INFO: Epoch: 11, train loss: 440.3527542114258
2026-01-21 16:44:52,999:INFO: Epoch: 11, dev loss: 296.80223083496094, f1 score: 0.4434782608695652
2026-01-21 16:44:56,291:INFO: Epoch: 12, train loss: 141.9539031982422
2026-01-21 16:44:56,540:INFO: Epoch: 12, dev loss: 341.3497721354167, f1 score: 0.5677966101694915
2026-01-21 16:44:56,540:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:44:59,431:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:44:59,432:INFO: --------Save best model!--------
2026-01-21 16:45:02,872:INFO: Epoch: 13, train loss: 106.8543083190918
2026-01-21 16:45:03,124:INFO: Epoch: 13, dev loss: 328.03258260091144, f1 score: 0.4933920704845815
2026-01-21 16:45:06,539:INFO: Epoch: 14, train loss: 99.87030181884765
2026-01-21 16:45:06,806:INFO: Epoch: 14, dev loss: 280.2916259765625, f1 score: 0.5714285714285715
2026-01-21 16:45:06,806:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:45:09,627:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:45:09,627:INFO: --------Save best model!--------
2026-01-21 16:45:13,088:INFO: Epoch: 15, train loss: 89.47779197692871
2026-01-21 16:45:13,354:INFO: Epoch: 15, dev loss: 324.122797648112, f1 score: 0.5587044534412955
2026-01-21 16:45:16,619:INFO: Epoch: 16, train loss: 83.90161437988282
2026-01-21 16:45:16,890:INFO: Epoch: 16, dev loss: 295.4289957682292, f1 score: 0.5485232067510549
2026-01-21 16:45:20,270:INFO: Epoch: 17, train loss: 79.06833992004394
2026-01-21 16:45:20,542:INFO: Epoch: 17, dev loss: 328.66968790690106, f1 score: 0.5225225225225225
2026-01-21 16:45:23,840:INFO: Epoch: 18, train loss: 70.14683036804199
2026-01-21 16:45:24,093:INFO: Epoch: 18, dev loss: 331.2619171142578, f1 score: 0.5800865800865801
2026-01-21 16:45:24,094:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:45:26,933:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:45:26,934:INFO: --------Save best model!--------
2026-01-21 16:45:30,349:INFO: Epoch: 19, train loss: 53.28512763977051
2026-01-21 16:45:30,613:INFO: Epoch: 19, dev loss: 319.408452351888, f1 score: 0.5654008438818566
2026-01-21 16:45:34,011:INFO: Epoch: 20, train loss: 58.493978118896486
2026-01-21 16:45:34,272:INFO: Epoch: 20, dev loss: 333.1127014160156, f1 score: 0.5963302752293578
2026-01-21 16:45:34,272:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:45:37,187:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:45:37,188:INFO: --------Save best model!--------
2026-01-21 16:45:40,646:INFO: Epoch: 21, train loss: 45.92189025878906
2026-01-21 16:45:40,901:INFO: Epoch: 21, dev loss: 423.1587422688802, f1 score: 0.5762711864406779
2026-01-21 16:45:44,323:INFO: Epoch: 22, train loss: 49.651139450073245
2026-01-21 16:45:44,576:INFO: Epoch: 22, dev loss: 369.2375996907552, f1 score: 0.603305785123967
2026-01-21 16:45:44,577:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:45:47,421:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:45:47,422:INFO: --------Save best model!--------
2026-01-21 16:45:50,852:INFO: Epoch: 23, train loss: 47.037170028686525
2026-01-21 16:45:51,109:INFO: Epoch: 23, dev loss: 385.76177978515625, f1 score: 0.6272727272727273
2026-01-21 16:45:51,109:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:45:53,715:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:45:53,716:INFO: --------Save best model!--------
2026-01-21 16:45:57,166:INFO: Epoch: 24, train loss: 33.7632453918457
2026-01-21 16:45:57,423:INFO: Epoch: 24, dev loss: 427.26979573567706, f1 score: 0.6133333333333333
2026-01-21 16:46:00,740:INFO: Epoch: 25, train loss: 26.564268493652342
2026-01-21 16:46:01,002:INFO: Epoch: 25, dev loss: 478.83802286783856, f1 score: 0.6578947368421052
2026-01-21 16:46:01,003:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:46:03,885:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:46:03,885:INFO: --------Save best model!--------
2026-01-21 16:46:07,233:INFO: Epoch: 26, train loss: 35.228461837768556
2026-01-21 16:46:07,500:INFO: Epoch: 26, dev loss: 412.80841573079425, f1 score: 0.6519823788546255
2026-01-21 16:46:10,918:INFO: Epoch: 27, train loss: 27.741984558105468
2026-01-21 16:46:11,185:INFO: Epoch: 27, dev loss: 395.5396728515625, f1 score: 0.6816143497757848
2026-01-21 16:46:11,186:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:46:14,087:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:46:14,087:INFO: --------Save best model!--------
2026-01-21 16:46:17,565:INFO: Epoch: 28, train loss: 18.29677047729492
2026-01-21 16:46:17,849:INFO: Epoch: 28, dev loss: 462.69678751627606, f1 score: 0.6283185840707964
2026-01-21 16:46:21,257:INFO: Epoch: 29, train loss: 17.187907028198243
2026-01-21 16:46:21,520:INFO: Epoch: 29, dev loss: 452.77159627278644, f1 score: 0.6608695652173914
2026-01-21 16:46:24,958:INFO: Epoch: 30, train loss: 16.923660659790038
2026-01-21 16:46:25,213:INFO: Epoch: 30, dev loss: 485.9810282389323, f1 score: 0.6460176991150443
2026-01-21 16:46:28,738:INFO: Epoch: 31, train loss: 14.729263305664062
2026-01-21 16:46:29,011:INFO: Epoch: 31, dev loss: 513.0395253499349, f1 score: 0.6431718061674009
2026-01-21 16:46:32,257:INFO: Epoch: 32, train loss: 15.298939514160157
2026-01-21 16:46:32,531:INFO: Epoch: 32, dev loss: 492.32183329264325, f1 score: 0.6491228070175439
2026-01-21 16:46:35,896:INFO: Epoch: 33, train loss: 9.885935592651368
2026-01-21 16:46:36,167:INFO: Epoch: 33, dev loss: 498.38609822591144, f1 score: 0.6457399103139014
2026-01-21 16:46:39,484:INFO: Epoch: 34, train loss: 8.191122055053711
2026-01-21 16:46:39,761:INFO: Epoch: 34, dev loss: 502.374267578125, f1 score: 0.6578947368421052
2026-01-21 16:46:43,142:INFO: Epoch: 35, train loss: 6.71116828918457
2026-01-21 16:46:43,399:INFO: Epoch: 35, dev loss: 524.4372253417969, f1 score: 0.6186440677966102
2026-01-21 16:46:46,516:INFO: Epoch: 36, train loss: 6.653631591796875
2026-01-21 16:46:46,765:INFO: Epoch: 36, dev loss: 561.9691263834635, f1 score: 0.6696035242290749
2026-01-21 16:46:49,998:INFO: Epoch: 37, train loss: 4.687351226806641
2026-01-21 16:46:50,244:INFO: Epoch: 37, dev loss: 565.0884195963541, f1 score: 0.6244725738396625
2026-01-21 16:46:50,244:INFO: Best val f1: 0.6816143497757848
2026-01-21 16:46:50,244:INFO: Training Finished!
2026-01-21 16:46:50,252:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:46:50,252:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:46:50,252:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:46:50,252:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:46:50,252:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:46:50,252:INFO: loading file None
2026-01-21 16:46:50,252:INFO: loading file None
2026-01-21 16:46:50,253:INFO: loading file None
2026-01-21 16:46:50,329:INFO: --------Dataset Build!--------
2026-01-21 16:46:50,329:INFO: --------Get Data-loader!--------
2026-01-21 16:46:50,330:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:46:50,330:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:46:50,330:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:46:55,551:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:46:55,552:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:46:55,552:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:46:55,552:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:46:55,552:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:46:55,552:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:46:55,552:INFO: loading file None
2026-01-21 16:46:55,552:INFO: loading file None
2026-01-21 16:46:55,552:INFO: loading file None
2026-01-21 16:46:56,252:INFO: --------Bad Cases reserved !--------
2026-01-21 16:46:56,255:INFO: test loss: 421.9380925496419, f1 score: 0.6581818181818182
2026-01-21 16:46:56,256:INFO: f1 score of ACTION: 0.4423076923076923
2026-01-21 16:46:56,256:INFO: f1 score of LEVEL_KEY: 0.5918367346938774
2026-01-21 16:46:56,256:INFO: f1 score of OBJ: 0.723404255319149
2026-01-21 16:46:56,256:INFO: f1 score of ORG: 0.6724137931034483
2026-01-21 16:46:56,256:INFO: f1 score of VALUE: 0.8571428571428572
2026-01-21 16:48:10,377:INFO: device: cuda:0
2026-01-21 16:48:10,378:INFO: --------Process Done!--------
2026-01-21 16:48:10,384:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:48:10,384:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:48:10,384:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:48:10,384:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:48:10,384:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:48:10,384:INFO: loading file None
2026-01-21 16:48:10,384:INFO: loading file None
2026-01-21 16:48:10,384:INFO: loading file None
2026-01-21 16:48:10,701:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:48:10,701:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:48:10,701:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:48:10,701:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:48:10,701:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:48:10,701:INFO: loading file None
2026-01-21 16:48:10,701:INFO: loading file None
2026-01-21 16:48:10,701:INFO: loading file None
2026-01-21 16:48:10,746:INFO: --------Dataset Build!--------
2026-01-21 16:48:10,747:INFO: --------Get Dataloader!--------
2026-01-21 16:48:10,747:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:48:10,747:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:48:10,748:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:48:16,934:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:48:16,935:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:48:18,409:INFO: --------Start Training!--------
2026-01-21 16:48:22,424:INFO: Epoch: 1, train loss: 1627.5962524414062
2026-01-21 16:48:22,700:INFO: Epoch: 1, dev loss: 977.6229298909506, f1 score: 0
2026-01-21 16:48:26,065:INFO: Epoch: 2, train loss: 883.3847793579101
2026-01-21 16:48:26,330:INFO: Epoch: 2, dev loss: 589.076800028483, f1 score: 0.06153846153846154
2026-01-21 16:48:26,331:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:48:29,014:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:48:29,014:INFO: --------Save best model!--------
2026-01-21 16:48:32,510:INFO: Epoch: 3, train loss: 537.84287109375
2026-01-21 16:48:32,761:INFO: Epoch: 3, dev loss: 392.7128651936849, f1 score: 0.42056074766355145
2026-01-21 16:48:32,762:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:48:33,890:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:48:33,891:INFO: --------Save best model!--------
2026-01-21 16:48:37,281:INFO: Epoch: 4, train loss: 343.1323944091797
2026-01-21 16:48:37,555:INFO: Epoch: 4, dev loss: 316.99049886067706, f1 score: 0.5488372093023256
2026-01-21 16:48:37,556:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:48:40,211:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:48:40,211:INFO: --------Save best model!--------
2026-01-21 16:48:43,626:INFO: Epoch: 5, train loss: 260.44619903564455
2026-01-21 16:48:43,889:INFO: Epoch: 5, dev loss: 293.0834655761719, f1 score: 0.4622222222222222
2026-01-21 16:48:47,333:INFO: Epoch: 6, train loss: 180.29907684326173
2026-01-21 16:48:47,604:INFO: Epoch: 6, dev loss: 289.77886454264325, f1 score: 0.5254237288135594
2026-01-21 16:48:51,152:INFO: Epoch: 7, train loss: 123.38760185241699
2026-01-21 16:48:51,415:INFO: Epoch: 7, dev loss: 295.89177958170575, f1 score: 0.5862068965517241
2026-01-21 16:48:51,415:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:48:53,720:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:48:53,720:INFO: --------Save best model!--------
2026-01-21 16:48:57,154:INFO: Epoch: 8, train loss: 85.552250289917
2026-01-21 16:48:57,405:INFO: Epoch: 8, dev loss: 330.63556416829425, f1 score: 0.5907172995780591
2026-01-21 16:48:57,406:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:48:59,642:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:48:59,642:INFO: --------Save best model!--------
2026-01-21 16:49:03,103:INFO: Epoch: 9, train loss: 77.71530532836914
2026-01-21 16:49:03,379:INFO: Epoch: 9, dev loss: 362.5943298339844, f1 score: 0.5866666666666667
2026-01-21 16:49:06,837:INFO: Epoch: 10, train loss: 59.74000549316406
2026-01-21 16:49:07,115:INFO: Epoch: 10, dev loss: 343.6387227376302, f1 score: 0.6550218340611355
2026-01-21 16:49:07,116:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:49:09,828:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:49:09,829:INFO: --------Save best model!--------
2026-01-21 16:49:13,260:INFO: Epoch: 11, train loss: 57.0847728729248
2026-01-21 16:49:13,515:INFO: Epoch: 11, dev loss: 361.6576894124349, f1 score: 0.6548672566371682
2026-01-21 16:49:16,842:INFO: Epoch: 12, train loss: 48.74914054870605
2026-01-21 16:49:17,100:INFO: Epoch: 12, dev loss: 338.94151306152344, f1 score: 0.638655462184874
2026-01-21 16:49:20,568:INFO: Epoch: 13, train loss: 45.67590065002442
2026-01-21 16:49:20,831:INFO: Epoch: 13, dev loss: 423.8802185058594, f1 score: 0.6147186147186147
2026-01-21 16:49:24,235:INFO: Epoch: 14, train loss: 45.25783157348633
2026-01-21 16:49:24,512:INFO: Epoch: 14, dev loss: 421.84332784016925, f1 score: 0.6333333333333334
2026-01-21 16:49:27,957:INFO: Epoch: 15, train loss: 42.5797061920166
2026-01-21 16:49:28,228:INFO: Epoch: 15, dev loss: 406.5604248046875, f1 score: 0.6491228070175439
2026-01-21 16:49:31,668:INFO: Epoch: 16, train loss: 36.04134979248047
2026-01-21 16:49:31,940:INFO: Epoch: 16, dev loss: 408.0469919840495, f1 score: 0.6244343891402715
2026-01-21 16:49:35,432:INFO: Epoch: 17, train loss: 30.771509170532227
2026-01-21 16:49:35,689:INFO: Epoch: 17, dev loss: 457.9853922526042, f1 score: 0.6311111111111111
2026-01-21 16:49:39,116:INFO: Epoch: 18, train loss: 22.26138687133789
2026-01-21 16:49:39,382:INFO: Epoch: 18, dev loss: 569.5298360188802, f1 score: 0.6355140186915887
2026-01-21 16:49:42,785:INFO: Epoch: 19, train loss: 31.858885955810546
2026-01-21 16:49:43,041:INFO: Epoch: 19, dev loss: 468.31358337402344, f1 score: 0.6172839506172839
2026-01-21 16:49:46,527:INFO: Epoch: 20, train loss: 23.660353469848634
2026-01-21 16:49:46,792:INFO: Epoch: 20, dev loss: 525.3296610514323, f1 score: 0.639269406392694
2026-01-21 16:49:46,792:INFO: Best val f1: 0.6550218340611355
2026-01-21 16:49:46,792:INFO: Training Finished!
2026-01-21 16:49:46,800:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:49:46,800:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:49:46,800:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:49:46,800:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:49:46,800:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:49:46,800:INFO: loading file None
2026-01-21 16:49:46,800:INFO: loading file None
2026-01-21 16:49:46,800:INFO: loading file None
2026-01-21 16:49:46,881:INFO: --------Dataset Build!--------
2026-01-21 16:49:46,881:INFO: --------Get Data-loader!--------
2026-01-21 16:49:46,881:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:49:46,882:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:49:46,882:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:49:52,105:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:49:52,106:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:49:52,106:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:49:52,106:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:49:52,106:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:49:52,106:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:49:52,106:INFO: loading file None
2026-01-21 16:49:52,106:INFO: loading file None
2026-01-21 16:49:52,106:INFO: loading file None
2026-01-21 16:49:52,822:INFO: --------Bad Cases reserved !--------
2026-01-21 16:49:52,826:INFO: test loss: 323.9043146769206, f1 score: 0.6504347826086956
2026-01-21 16:49:52,826:INFO: f1 score of ACTION: 0.4297520661157025
2026-01-21 16:49:52,826:INFO: f1 score of LEVEL_KEY: 0.6
2026-01-21 16:49:52,826:INFO: f1 score of OBJ: 0.7272727272727274
2026-01-21 16:49:52,826:INFO: f1 score of ORG: 0.7027027027027026
2026-01-21 16:49:52,826:INFO: f1 score of VALUE: 0.8089887640449438
2026-01-21 16:51:19,385:INFO: device: cuda:0
2026-01-21 16:51:19,385:INFO: --------Process Done!--------
2026-01-21 16:51:19,391:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:51:19,391:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:51:19,392:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:51:19,392:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:51:19,392:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:51:19,392:INFO: loading file None
2026-01-21 16:51:19,392:INFO: loading file None
2026-01-21 16:51:19,392:INFO: loading file None
2026-01-21 16:51:19,702:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:51:19,702:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:51:19,702:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:51:19,702:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:51:19,702:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:51:19,702:INFO: loading file None
2026-01-21 16:51:19,702:INFO: loading file None
2026-01-21 16:51:19,703:INFO: loading file None
2026-01-21 16:51:19,744:INFO: --------Dataset Build!--------
2026-01-21 16:51:19,744:INFO: --------Get Dataloader!--------
2026-01-21 16:51:19,744:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:51:19,744:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:51:19,745:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:51:26,020:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:51:26,021:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:51:27,413:INFO: --------Start Training!--------
2026-01-21 16:51:30,934:INFO: Epoch: 1, train loss: 3595.6142822265624
2026-01-21 16:51:31,177:INFO: Epoch: 1, dev loss: 1374.835922241211, f1 score: 0
2026-01-21 16:51:34,101:INFO: Epoch: 2, train loss: 2005.7684814453125
2026-01-21 16:51:34,336:INFO: Epoch: 2, dev loss: 1096.13037109375, f1 score: 0
2026-01-21 16:51:37,061:INFO: Epoch: 3, train loss: 1444.6432739257812
2026-01-21 16:51:37,308:INFO: Epoch: 3, dev loss: 705.1102447509766, f1 score: 0.1257142857142857
2026-01-21 16:51:37,308:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:51:39,785:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:51:39,785:INFO: --------Save best model!--------
2026-01-21 16:51:42,556:INFO: Epoch: 4, train loss: 897.8881530761719
2026-01-21 16:51:42,795:INFO: Epoch: 4, dev loss: 527.7174682617188, f1 score: 0.4072398190045249
2026-01-21 16:51:42,795:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:51:44,938:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:51:44,938:INFO: --------Save best model!--------
2026-01-21 16:51:47,878:INFO: Epoch: 5, train loss: 658.1353271484375
2026-01-21 16:51:48,135:INFO: Epoch: 5, dev loss: 442.73038482666016, f1 score: 0.48739495798319327
2026-01-21 16:51:48,135:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:51:50,648:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:51:50,649:INFO: --------Save best model!--------
2026-01-21 16:51:53,408:INFO: Epoch: 6, train loss: 460.53270263671874
2026-01-21 16:51:53,648:INFO: Epoch: 6, dev loss: 458.6898651123047, f1 score: 0.4489795918367347
2026-01-21 16:51:56,519:INFO: Epoch: 7, train loss: 327.2454376220703
2026-01-21 16:51:56,752:INFO: Epoch: 7, dev loss: 435.28150177001953, f1 score: 0.4871794871794872
2026-01-21 16:51:59,681:INFO: Epoch: 8, train loss: 228.04827270507812
2026-01-21 16:51:59,936:INFO: Epoch: 8, dev loss: 484.81268310546875, f1 score: 0.5726495726495726
2026-01-21 16:51:59,937:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:52:02,114:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:52:02,114:INFO: --------Save best model!--------
2026-01-21 16:52:05,053:INFO: Epoch: 9, train loss: 180.24881896972656
2026-01-21 16:52:05,310:INFO: Epoch: 9, dev loss: 557.3148880004883, f1 score: 0.6311111111111111
2026-01-21 16:52:05,311:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:52:07,838:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:52:07,839:INFO: --------Save best model!--------
2026-01-21 16:52:10,621:INFO: Epoch: 10, train loss: 156.38410339355468
2026-01-21 16:52:10,882:INFO: Epoch: 10, dev loss: 515.8545379638672, f1 score: 0.5701754385964912
2026-01-21 16:52:13,798:INFO: Epoch: 11, train loss: 124.88755264282227
2026-01-21 16:52:14,055:INFO: Epoch: 11, dev loss: 536.9963684082031, f1 score: 0.5668016194331983
2026-01-21 16:52:17,004:INFO: Epoch: 12, train loss: 102.76684646606445
2026-01-21 16:52:17,253:INFO: Epoch: 12, dev loss: 591.6260986328125, f1 score: 0.578512396694215
2026-01-21 16:52:20,077:INFO: Epoch: 13, train loss: 90.60117797851562
2026-01-21 16:52:20,333:INFO: Epoch: 13, dev loss: 615.9669036865234, f1 score: 0.591304347826087
2026-01-21 16:52:23,251:INFO: Epoch: 14, train loss: 83.00812072753907
2026-01-21 16:52:23,504:INFO: Epoch: 14, dev loss: 527.0108032226562, f1 score: 0.6068376068376068
2026-01-21 16:52:26,390:INFO: Epoch: 15, train loss: 74.51566772460937
2026-01-21 16:52:26,649:INFO: Epoch: 15, dev loss: 524.4121398925781, f1 score: 0.6228070175438596
2026-01-21 16:52:29,438:INFO: Epoch: 16, train loss: 58.526608276367185
2026-01-21 16:52:29,697:INFO: Epoch: 16, dev loss: 607.4079284667969, f1 score: 0.610878661087866
2026-01-21 16:52:32,493:INFO: Epoch: 17, train loss: 57.96445236206055
2026-01-21 16:52:32,751:INFO: Epoch: 17, dev loss: 636.7565765380859, f1 score: 0.6351931330472103
2026-01-21 16:52:32,751:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:52:35,188:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:52:35,189:INFO: --------Save best model!--------
2026-01-21 16:52:38,099:INFO: Epoch: 18, train loss: 48.200357818603514
2026-01-21 16:52:38,337:INFO: Epoch: 18, dev loss: 665.541748046875, f1 score: 0.591304347826087
2026-01-21 16:52:41,187:INFO: Epoch: 19, train loss: 45.20939483642578
2026-01-21 16:52:41,426:INFO: Epoch: 19, dev loss: 650.7823791503906, f1 score: 0.5948275862068966
2026-01-21 16:52:44,222:INFO: Epoch: 20, train loss: 30.495639038085937
2026-01-21 16:52:44,485:INFO: Epoch: 20, dev loss: 654.4046020507812, f1 score: 0.6160337552742615
2026-01-21 16:52:47,371:INFO: Epoch: 21, train loss: 33.708395385742186
2026-01-21 16:52:47,634:INFO: Epoch: 21, dev loss: 681.7337875366211, f1 score: 0.6127659574468085
2026-01-21 16:52:50,412:INFO: Epoch: 22, train loss: 34.502654266357425
2026-01-21 16:52:50,647:INFO: Epoch: 22, dev loss: 743.0272216796875, f1 score: 0.6382978723404255
2026-01-21 16:52:50,648:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:52:53,145:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:52:53,145:INFO: --------Save best model!--------
2026-01-21 16:52:56,029:INFO: Epoch: 23, train loss: 18.802598571777345
2026-01-21 16:52:56,280:INFO: Epoch: 23, dev loss: 726.8807373046875, f1 score: 0.6212765957446809
2026-01-21 16:52:59,133:INFO: Epoch: 24, train loss: 22.03730163574219
2026-01-21 16:52:59,391:INFO: Epoch: 24, dev loss: 707.8277435302734, f1 score: 0.638655462184874
2026-01-21 16:52:59,392:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:53:01,870:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:53:01,870:INFO: --------Save best model!--------
2026-01-21 16:53:04,771:INFO: Epoch: 25, train loss: 11.353644561767577
2026-01-21 16:53:05,006:INFO: Epoch: 25, dev loss: 704.4601593017578, f1 score: 0.6244725738396625
2026-01-21 16:53:07,939:INFO: Epoch: 26, train loss: 13.49197998046875
2026-01-21 16:53:08,177:INFO: Epoch: 26, dev loss: 782.8107757568359, f1 score: 0.6371681415929203
2026-01-21 16:53:11,053:INFO: Epoch: 27, train loss: 9.747625732421875
2026-01-21 16:53:11,311:INFO: Epoch: 27, dev loss: 816.6181488037109, f1 score: 0.6086956521739131
2026-01-21 16:53:14,096:INFO: Epoch: 28, train loss: 11.304237365722656
2026-01-21 16:53:14,332:INFO: Epoch: 28, dev loss: 827.3972015380859, f1 score: 0.6339285714285714
2026-01-21 16:53:16,981:INFO: Epoch: 29, train loss: 9.40714340209961
2026-01-21 16:53:17,239:INFO: Epoch: 29, dev loss: 830.2297973632812, f1 score: 0.6523605150214593
2026-01-21 16:53:17,239:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:53:19,738:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:53:19,738:INFO: --------Save best model!--------
2026-01-21 16:53:22,626:INFO: Epoch: 30, train loss: 8.980318450927735
2026-01-21 16:53:22,867:INFO: Epoch: 30, dev loss: 831.3879776000977, f1 score: 0.6581196581196581
2026-01-21 16:53:22,868:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:53:25,400:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:53:25,400:INFO: --------Save best model!--------
2026-01-21 16:53:28,068:INFO: Epoch: 31, train loss: 5.879096221923828
2026-01-21 16:53:28,304:INFO: Epoch: 31, dev loss: 856.3987731933594, f1 score: 0.6437768240343348
2026-01-21 16:53:31,214:INFO: Epoch: 32, train loss: 4.466769409179688
2026-01-21 16:53:31,471:INFO: Epoch: 32, dev loss: 899.0784606933594, f1 score: 0.6553191489361702
2026-01-21 16:53:34,411:INFO: Epoch: 33, train loss: 3.5494148254394533
2026-01-21 16:53:34,660:INFO: Epoch: 33, dev loss: 895.2217330932617, f1 score: 0.6553191489361702
2026-01-21 16:53:37,455:INFO: Epoch: 34, train loss: 2.271099853515625
2026-01-21 16:53:37,703:INFO: Epoch: 34, dev loss: 908.4577331542969, f1 score: 0.6468085106382979
2026-01-21 16:53:40,494:INFO: Epoch: 35, train loss: 2.394025421142578
2026-01-21 16:53:40,729:INFO: Epoch: 35, dev loss: 916.4631958007812, f1 score: 0.6495726495726496
2026-01-21 16:53:43,640:INFO: Epoch: 36, train loss: 1.6627174377441407
2026-01-21 16:53:43,900:INFO: Epoch: 36, dev loss: 928.5360717773438, f1 score: 0.6468085106382979
2026-01-21 16:53:46,610:INFO: Epoch: 37, train loss: 1.433953857421875
2026-01-21 16:53:46,867:INFO: Epoch: 37, dev loss: 941.9453277587891, f1 score: 0.6440677966101694
2026-01-21 16:53:49,795:INFO: Epoch: 38, train loss: 1.0321083068847656
2026-01-21 16:53:50,053:INFO: Epoch: 38, dev loss: 951.8790817260742, f1 score: 0.6440677966101694
2026-01-21 16:53:52,983:INFO: Epoch: 39, train loss: 1.5292274475097656
2026-01-21 16:53:53,233:INFO: Epoch: 39, dev loss: 954.3557434082031, f1 score: 0.6440677966101694
2026-01-21 16:53:56,117:INFO: Epoch: 40, train loss: 1.0630058288574218
2026-01-21 16:53:56,368:INFO: Epoch: 40, dev loss: 954.0836486816406, f1 score: 0.6440677966101694
2026-01-21 16:53:56,368:INFO: Best val f1: 0.6581196581196581
2026-01-21 16:53:56,369:INFO: Training Finished!
2026-01-21 16:53:56,376:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:53:56,376:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:53:56,376:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:53:56,376:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:53:56,377:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:53:56,377:INFO: loading file None
2026-01-21 16:53:56,377:INFO: loading file None
2026-01-21 16:53:56,377:INFO: loading file None
2026-01-21 16:53:56,452:INFO: --------Dataset Build!--------
2026-01-21 16:53:56,452:INFO: --------Get Data-loader!--------
2026-01-21 16:53:56,452:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:53:56,453:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:53:56,453:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:54:01,607:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:54:01,608:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:54:01,608:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:54:01,608:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:54:01,608:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:54:01,608:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:54:01,608:INFO: loading file None
2026-01-21 16:54:01,608:INFO: loading file None
2026-01-21 16:54:01,608:INFO: loading file None
2026-01-21 16:54:02,408:INFO: --------Bad Cases reserved !--------
2026-01-21 16:54:02,412:INFO: test loss: 1224.7672526041667, f1 score: 0.6630824372759857
2026-01-21 16:54:02,412:INFO: f1 score of ACTION: 0.4912280701754386
2026-01-21 16:54:02,412:INFO: f1 score of LEVEL_KEY: 0.5656565656565657
2026-01-21 16:54:02,412:INFO: f1 score of OBJ: 0.7123287671232876
2026-01-21 16:54:02,412:INFO: f1 score of ORG: 0.7272727272727273
2026-01-21 16:54:02,412:INFO: f1 score of VALUE: 0.8314606741573034
2026-01-21 16:54:58,671:INFO: device: cuda:0
2026-01-21 16:54:58,671:INFO: --------Process Done!--------
2026-01-21 16:54:58,678:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:54:58,678:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:54:58,678:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:54:58,678:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:54:58,678:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:54:58,678:INFO: loading file None
2026-01-21 16:54:58,679:INFO: loading file None
2026-01-21 16:54:58,679:INFO: loading file None
2026-01-21 16:54:59,006:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:54:59,006:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:54:59,006:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:54:59,006:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:54:59,006:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:54:59,006:INFO: loading file None
2026-01-21 16:54:59,006:INFO: loading file None
2026-01-21 16:54:59,006:INFO: loading file None
2026-01-21 16:54:59,049:INFO: --------Dataset Build!--------
2026-01-21 16:54:59,049:INFO: --------Get Dataloader!--------
2026-01-21 16:54:59,049:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:54:59,050:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:54:59,050:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:55:05,569:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:55:05,570:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:55:07,015:INFO: --------Start Training!--------
2026-01-21 16:55:10,400:INFO: Epoch: 1, train loss: 3614.00869140625
2026-01-21 16:55:10,665:INFO: Epoch: 1, dev loss: 1417.1124267578125, f1 score: 0
2026-01-21 16:55:13,676:INFO: Epoch: 2, train loss: 2006.6116943359375
2026-01-21 16:55:13,934:INFO: Epoch: 2, dev loss: 1139.7554931640625, f1 score: 0
2026-01-21 16:55:16,617:INFO: Epoch: 3, train loss: 1504.1185302734375
2026-01-21 16:55:16,873:INFO: Epoch: 3, dev loss: 789.3803634643555, f1 score: 0.03821656050955414
2026-01-21 16:55:16,874:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:19,307:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:19,308:INFO: --------Save best model!--------
2026-01-21 16:55:22,150:INFO: Epoch: 4, train loss: 988.3768188476563
2026-01-21 16:55:22,391:INFO: Epoch: 4, dev loss: 563.6471786499023, f1 score: 0.35242290748898675
2026-01-21 16:55:22,392:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:24,591:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:24,592:INFO: --------Save best model!--------
2026-01-21 16:55:27,369:INFO: Epoch: 5, train loss: 711.6776794433594
2026-01-21 16:55:27,608:INFO: Epoch: 5, dev loss: 489.81951904296875, f1 score: 0.4326923076923077
2026-01-21 16:55:27,609:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:30,024:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:30,024:INFO: --------Save best model!--------
2026-01-21 16:55:32,916:INFO: Epoch: 6, train loss: 479.39373474121095
2026-01-21 16:55:33,172:INFO: Epoch: 6, dev loss: 566.3123931884766, f1 score: 0.4182509505703422
2026-01-21 16:55:36,033:INFO: Epoch: 7, train loss: 391.9963073730469
2026-01-21 16:55:36,288:INFO: Epoch: 7, dev loss: 486.8749237060547, f1 score: 0.49302325581395345
2026-01-21 16:55:36,289:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:38,871:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:38,871:INFO: --------Save best model!--------
2026-01-21 16:55:41,722:INFO: Epoch: 8, train loss: 304.7963897705078
2026-01-21 16:55:41,970:INFO: Epoch: 8, dev loss: 409.01019287109375, f1 score: 0.5470085470085471
2026-01-21 16:55:41,971:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:44,432:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:44,432:INFO: --------Save best model!--------
2026-01-21 16:55:47,386:INFO: Epoch: 9, train loss: 220.60869140625
2026-01-21 16:55:47,622:INFO: Epoch: 9, dev loss: 492.4057922363281, f1 score: 0.610878661087866
2026-01-21 16:55:47,622:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:55:50,077:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:55:50,077:INFO: --------Save best model!--------
2026-01-21 16:55:52,979:INFO: Epoch: 10, train loss: 183.81719818115235
2026-01-21 16:55:53,237:INFO: Epoch: 10, dev loss: 485.1464080810547, f1 score: 0.5774058577405858
2026-01-21 16:55:56,041:INFO: Epoch: 11, train loss: 149.86148529052736
2026-01-21 16:55:56,301:INFO: Epoch: 11, dev loss: 535.4623947143555, f1 score: 0.5991189427312775
2026-01-21 16:55:59,339:INFO: Epoch: 12, train loss: 116.2469985961914
2026-01-21 16:55:59,602:INFO: Epoch: 12, dev loss: 485.0601806640625, f1 score: 0.6460176991150443
2026-01-21 16:55:59,602:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:56:01,758:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:56:01,758:INFO: --------Save best model!--------
2026-01-21 16:56:04,598:INFO: Epoch: 13, train loss: 98.469921875
2026-01-21 16:56:04,856:INFO: Epoch: 13, dev loss: 575.159294128418, f1 score: 0.6727272727272727
2026-01-21 16:56:04,857:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:56:07,288:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:56:07,288:INFO: --------Save best model!--------
2026-01-21 16:56:10,187:INFO: Epoch: 14, train loss: 79.98815383911133
2026-01-21 16:56:10,430:INFO: Epoch: 14, dev loss: 530.3471832275391, f1 score: 0.6521739130434782
2026-01-21 16:56:13,048:INFO: Epoch: 15, train loss: 77.4804183959961
2026-01-21 16:56:13,303:INFO: Epoch: 15, dev loss: 648.8427963256836, f1 score: 0.6607142857142857
2026-01-21 16:56:16,071:INFO: Epoch: 16, train loss: 57.94259033203125
2026-01-21 16:56:16,328:INFO: Epoch: 16, dev loss: 660.0200500488281, f1 score: 0.6666666666666666
2026-01-21 16:56:19,233:INFO: Epoch: 17, train loss: 63.15098266601562
2026-01-21 16:56:19,491:INFO: Epoch: 17, dev loss: 612.4108810424805, f1 score: 0.6255506607929515
2026-01-21 16:56:22,365:INFO: Epoch: 18, train loss: 48.883629608154294
2026-01-21 16:56:22,603:INFO: Epoch: 18, dev loss: 625.2162322998047, f1 score: 0.6899563318777293
2026-01-21 16:56:22,603:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:56:25,207:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:56:25,207:INFO: --------Save best model!--------
2026-01-21 16:56:28,083:INFO: Epoch: 19, train loss: 53.692726135253906
2026-01-21 16:56:28,326:INFO: Epoch: 19, dev loss: 583.08056640625, f1 score: 0.6371681415929203
2026-01-21 16:56:31,180:INFO: Epoch: 20, train loss: 36.0427604675293
2026-01-21 16:56:31,429:INFO: Epoch: 20, dev loss: 622.4066772460938, f1 score: 0.6320346320346321
2026-01-21 16:56:34,207:INFO: Epoch: 21, train loss: 34.84215545654297
2026-01-21 16:56:34,469:INFO: Epoch: 21, dev loss: 655.3935852050781, f1 score: 0.6343612334801763
2026-01-21 16:56:37,244:INFO: Epoch: 22, train loss: 28.802996826171874
2026-01-21 16:56:37,478:INFO: Epoch: 22, dev loss: 661.3046875, f1 score: 0.6696035242290749
2026-01-21 16:56:40,452:INFO: Epoch: 23, train loss: 28.778214263916016
2026-01-21 16:56:40,712:INFO: Epoch: 23, dev loss: 623.6344375610352, f1 score: 0.6902654867256636
2026-01-21 16:56:40,712:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:56:43,172:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:56:43,172:INFO: --------Save best model!--------
2026-01-21 16:56:46,078:INFO: Epoch: 24, train loss: 21.05484619140625
2026-01-21 16:56:46,339:INFO: Epoch: 24, dev loss: 682.6117858886719, f1 score: 0.6842105263157895
2026-01-21 16:56:49,223:INFO: Epoch: 25, train loss: 15.098494720458984
2026-01-21 16:56:49,461:INFO: Epoch: 25, dev loss: 717.8270874023438, f1 score: 0.6547085201793722
2026-01-21 16:56:52,358:INFO: Epoch: 26, train loss: 18.90507278442383
2026-01-21 16:56:52,596:INFO: Epoch: 26, dev loss: 727.3748779296875, f1 score: 0.6460176991150443
2026-01-21 16:56:55,353:INFO: Epoch: 27, train loss: 14.69429931640625
2026-01-21 16:56:55,595:INFO: Epoch: 27, dev loss: 750.0937271118164, f1 score: 0.6578947368421052
2026-01-21 16:56:58,317:INFO: Epoch: 28, train loss: 12.286412811279297
2026-01-21 16:56:58,557:INFO: Epoch: 28, dev loss: 735.1866760253906, f1 score: 0.6696035242290749
2026-01-21 16:57:01,467:INFO: Epoch: 29, train loss: 10.914260864257812
2026-01-21 16:57:01,726:INFO: Epoch: 29, dev loss: 807.5123596191406, f1 score: 0.6431718061674009
2026-01-21 16:57:04,450:INFO: Epoch: 30, train loss: 7.989363098144532
2026-01-21 16:57:04,686:INFO: Epoch: 30, dev loss: 814.5082550048828, f1 score: 0.6784140969162995
2026-01-21 16:57:07,664:INFO: Epoch: 31, train loss: 5.966743469238281
2026-01-21 16:57:07,907:INFO: Epoch: 31, dev loss: 800.600341796875, f1 score: 0.6696035242290749
2026-01-21 16:57:10,858:INFO: Epoch: 32, train loss: 5.290544128417968
2026-01-21 16:57:11,112:INFO: Epoch: 32, dev loss: 830.9279174804688, f1 score: 0.6607929515418502
2026-01-21 16:57:13,944:INFO: Epoch: 33, train loss: 4.488786315917968
2026-01-21 16:57:14,204:INFO: Epoch: 33, dev loss: 858.7265319824219, f1 score: 0.6431718061674009
2026-01-21 16:57:14,204:INFO: Best val f1: 0.6902654867256636
2026-01-21 16:57:14,204:INFO: Training Finished!
2026-01-21 16:57:14,212:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:57:14,213:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:57:14,213:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:57:14,213:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:57:14,213:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:57:14,213:INFO: loading file None
2026-01-21 16:57:14,213:INFO: loading file None
2026-01-21 16:57:14,213:INFO: loading file None
2026-01-21 16:57:14,288:INFO: --------Dataset Build!--------
2026-01-21 16:57:14,288:INFO: --------Get Data-loader!--------
2026-01-21 16:57:14,288:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:57:14,288:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:57:14,289:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:57:19,466:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 16:57:19,467:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:57:19,467:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:57:19,467:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:57:19,467:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:57:19,467:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:57:19,467:INFO: loading file None
2026-01-21 16:57:19,467:INFO: loading file None
2026-01-21 16:57:19,467:INFO: loading file None
2026-01-21 16:57:20,268:INFO: --------Bad Cases reserved !--------
2026-01-21 16:57:20,272:INFO: test loss: 897.02587890625, f1 score: 0.677536231884058
2026-01-21 16:57:20,273:INFO: f1 score of ACTION: 0.5185185185185185
2026-01-21 16:57:20,273:INFO: f1 score of LEVEL_KEY: 0.7070707070707071
2026-01-21 16:57:20,273:INFO: f1 score of OBJ: 0.7659574468085107
2026-01-21 16:57:20,273:INFO: f1 score of ORG: 0.6428571428571429
2026-01-21 16:57:20,273:INFO: f1 score of VALUE: 0.7391304347826086
2026-01-21 16:57:55,315:INFO: device: cuda:0
2026-01-21 16:57:55,315:INFO: --------Process Done!--------
2026-01-21 16:57:55,321:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:57:55,322:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:57:55,322:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:57:55,322:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:57:55,322:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:57:55,322:INFO: loading file None
2026-01-21 16:57:55,322:INFO: loading file None
2026-01-21 16:57:55,322:INFO: loading file None
2026-01-21 16:57:55,647:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:57:55,647:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:57:55,648:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:57:55,648:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:57:55,648:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:57:55,648:INFO: loading file None
2026-01-21 16:57:55,648:INFO: loading file None
2026-01-21 16:57:55,648:INFO: loading file None
2026-01-21 16:57:55,688:INFO: --------Dataset Build!--------
2026-01-21 16:57:55,689:INFO: --------Get Dataloader!--------
2026-01-21 16:57:55,689:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 16:57:55,689:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:57:55,689:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 16:58:02,135:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 16:58:02,136:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 16:58:03,539:INFO: --------Start Training!--------
2026-01-21 16:58:07,110:INFO: Epoch: 1, train loss: 3719.6152587890624
2026-01-21 16:58:07,378:INFO: Epoch: 1, dev loss: 1432.2305297851562, f1 score: 0
2026-01-21 16:58:10,293:INFO: Epoch: 2, train loss: 2037.2166381835937
2026-01-21 16:58:10,552:INFO: Epoch: 2, dev loss: 1163.9318542480469, f1 score: 0
2026-01-21 16:58:13,472:INFO: Epoch: 3, train loss: 1516.42587890625
2026-01-21 16:58:13,723:INFO: Epoch: 3, dev loss: 825.8844604492188, f1 score: 0.07547169811320754
2026-01-21 16:58:13,724:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:16,236:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:16,236:INFO: --------Save best model!--------
2026-01-21 16:58:19,168:INFO: Epoch: 4, train loss: 939.2470825195312
2026-01-21 16:58:19,424:INFO: Epoch: 4, dev loss: 656.5494079589844, f1 score: 0.3235294117647059
2026-01-21 16:58:19,424:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:21,887:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:21,888:INFO: --------Save best model!--------
2026-01-21 16:58:24,770:INFO: Epoch: 5, train loss: 724.7585998535156
2026-01-21 16:58:25,009:INFO: Epoch: 5, dev loss: 481.49141693115234, f1 score: 0.4716157205240175
2026-01-21 16:58:25,010:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:27,799:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:27,800:INFO: --------Save best model!--------
2026-01-21 16:58:30,741:INFO: Epoch: 6, train loss: 545.3211273193359
2026-01-21 16:58:30,989:INFO: Epoch: 6, dev loss: 464.19588470458984, f1 score: 0.46153846153846156
2026-01-21 16:58:33,855:INFO: Epoch: 7, train loss: 422.0321411132812
2026-01-21 16:58:34,091:INFO: Epoch: 7, dev loss: 490.01683807373047, f1 score: 0.5350877192982456
2026-01-21 16:58:34,092:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:36,287:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:36,288:INFO: --------Save best model!--------
2026-01-21 16:58:39,217:INFO: Epoch: 8, train loss: 305.4425109863281
2026-01-21 16:58:39,453:INFO: Epoch: 8, dev loss: 433.8582305908203, f1 score: 0.6
2026-01-21 16:58:39,453:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:41,883:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:41,883:INFO: --------Save best model!--------
2026-01-21 16:58:44,614:INFO: Epoch: 9, train loss: 223.26128997802735
2026-01-21 16:58:44,871:INFO: Epoch: 9, dev loss: 480.4264221191406, f1 score: 0.5739130434782609
2026-01-21 16:58:47,797:INFO: Epoch: 10, train loss: 177.5808883666992
2026-01-21 16:58:48,034:INFO: Epoch: 10, dev loss: 555.3999633789062, f1 score: 0.5650224215246636
2026-01-21 16:58:50,837:INFO: Epoch: 11, train loss: 133.08017120361328
2026-01-21 16:58:51,094:INFO: Epoch: 11, dev loss: 542.9638214111328, f1 score: 0.5991561181434599
2026-01-21 16:58:54,032:INFO: Epoch: 12, train loss: 112.62592086791992
2026-01-21 16:58:54,290:INFO: Epoch: 12, dev loss: 577.9630432128906, f1 score: 0.6008583690987125
2026-01-21 16:58:54,290:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:58:56,770:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:58:56,770:INFO: --------Save best model!--------
2026-01-21 16:58:59,628:INFO: Epoch: 13, train loss: 108.2208236694336
2026-01-21 16:58:59,864:INFO: Epoch: 13, dev loss: 576.5590515136719, f1 score: 0.6550218340611355
2026-01-21 16:58:59,865:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:59:02,660:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:59:02,660:INFO: --------Save best model!--------
2026-01-21 16:59:05,439:INFO: Epoch: 14, train loss: 95.4078384399414
2026-01-21 16:59:05,683:INFO: Epoch: 14, dev loss: 587.7078399658203, f1 score: 0.6379310344827587
2026-01-21 16:59:08,465:INFO: Epoch: 15, train loss: 83.2023811340332
2026-01-21 16:59:08,723:INFO: Epoch: 15, dev loss: 523.2299041748047, f1 score: 0.6228070175438596
2026-01-21 16:59:11,653:INFO: Epoch: 16, train loss: 64.57012634277343
2026-01-21 16:59:11,892:INFO: Epoch: 16, dev loss: 564.2283630371094, f1 score: 0.6333333333333334
2026-01-21 16:59:14,831:INFO: Epoch: 17, train loss: 68.90391616821289
2026-01-21 16:59:15,088:INFO: Epoch: 17, dev loss: 584.3425979614258, f1 score: 0.6443514644351465
2026-01-21 16:59:17,789:INFO: Epoch: 18, train loss: 51.5809814453125
2026-01-21 16:59:18,039:INFO: Epoch: 18, dev loss: 547.3656463623047, f1 score: 0.6580086580086579
2026-01-21 16:59:18,039:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:59:20,831:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:59:20,831:INFO: --------Save best model!--------
2026-01-21 16:59:23,514:INFO: Epoch: 19, train loss: 45.46869125366211
2026-01-21 16:59:23,766:INFO: Epoch: 19, dev loss: 527.0144348144531, f1 score: 0.6636363636363637
2026-01-21 16:59:23,767:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:59:26,198:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 16:59:26,199:INFO: --------Save best model!--------
2026-01-21 16:59:28,845:INFO: Epoch: 20, train loss: 35.7156478881836
2026-01-21 16:59:29,083:INFO: Epoch: 20, dev loss: 489.4217071533203, f1 score: 0.6465517241379309
2026-01-21 16:59:31,852:INFO: Epoch: 21, train loss: 29.850117492675782
2026-01-21 16:59:32,111:INFO: Epoch: 21, dev loss: 554.0795135498047, f1 score: 0.6437768240343348
2026-01-21 16:59:35,078:INFO: Epoch: 22, train loss: 27.459130096435548
2026-01-21 16:59:35,337:INFO: Epoch: 22, dev loss: 636.6930236816406, f1 score: 0.6434782608695652
2026-01-21 16:59:38,265:INFO: Epoch: 23, train loss: 28.10602798461914
2026-01-21 16:59:38,504:INFO: Epoch: 23, dev loss: 655.7632293701172, f1 score: 0.6434782608695652
2026-01-21 16:59:41,474:INFO: Epoch: 24, train loss: 19.460814666748046
2026-01-21 16:59:41,732:INFO: Epoch: 24, dev loss: 629.9992752075195, f1 score: 0.6495726495726496
2026-01-21 16:59:44,614:INFO: Epoch: 25, train loss: 19.271015167236328
2026-01-21 16:59:44,856:INFO: Epoch: 25, dev loss: 721.4162750244141, f1 score: 0.6577777777777778
2026-01-21 16:59:47,741:INFO: Epoch: 26, train loss: 17.94992599487305
2026-01-21 16:59:47,984:INFO: Epoch: 26, dev loss: 630.8672714233398, f1 score: 0.6580086580086579
2026-01-21 16:59:50,937:INFO: Epoch: 27, train loss: 16.112586975097656
2026-01-21 16:59:51,197:INFO: Epoch: 27, dev loss: 625.7245101928711, f1 score: 0.6440677966101694
2026-01-21 16:59:54,128:INFO: Epoch: 28, train loss: 8.130711364746094
2026-01-21 16:59:54,364:INFO: Epoch: 28, dev loss: 633.2760925292969, f1 score: 0.6609442060085837
2026-01-21 16:59:57,147:INFO: Epoch: 29, train loss: 12.3482421875
2026-01-21 16:59:57,408:INFO: Epoch: 29, dev loss: 643.4141082763672, f1 score: 0.6609442060085837
2026-01-21 16:59:57,408:INFO: Best val f1: 0.6636363636363637
2026-01-21 16:59:57,408:INFO: Training Finished!
2026-01-21 16:59:57,415:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 16:59:57,416:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 16:59:57,416:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 16:59:57,416:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 16:59:57,416:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 16:59:57,416:INFO: loading file None
2026-01-21 16:59:57,416:INFO: loading file None
2026-01-21 16:59:57,416:INFO: loading file None
2026-01-21 16:59:57,491:INFO: --------Dataset Build!--------
2026-01-21 16:59:57,491:INFO: --------Get Data-loader!--------
2026-01-21 16:59:57,491:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 16:59:57,492:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 16:59:57,492:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:00:02,677:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 17:00:02,678:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:00:02,679:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:00:02,679:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:00:02,679:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:00:02,679:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:00:02,679:INFO: loading file None
2026-01-21 17:00:02,679:INFO: loading file None
2026-01-21 17:00:02,679:INFO: loading file None
2026-01-21 17:00:03,480:INFO: --------Bad Cases reserved !--------
2026-01-21 17:00:03,484:INFO: test loss: 929.2926432291666, f1 score: 0.6980108499095841
2026-01-21 17:00:03,485:INFO: f1 score of ACTION: 0.5
2026-01-21 17:00:03,485:INFO: f1 score of LEVEL_KEY: 0.6407766990291262
2026-01-21 17:00:03,485:INFO: f1 score of OBJ: 0.7692307692307693
2026-01-21 17:00:03,485:INFO: f1 score of ORG: 0.7017543859649122
2026-01-21 17:00:03,485:INFO: f1 score of VALUE: 0.8764044943820224
2026-01-21 17:03:09,282:INFO: device: cuda:0
2026-01-21 17:03:09,282:INFO: --------Process Done!--------
2026-01-21 17:03:09,289:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:03:09,289:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:03:09,289:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:03:09,289:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:03:09,289:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:03:09,289:INFO: loading file None
2026-01-21 17:03:09,289:INFO: loading file None
2026-01-21 17:03:09,289:INFO: loading file None
2026-01-21 17:03:09,602:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:03:09,603:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:03:09,603:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:03:09,603:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:03:09,603:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:03:09,603:INFO: loading file None
2026-01-21 17:03:09,603:INFO: loading file None
2026-01-21 17:03:09,603:INFO: loading file None
2026-01-21 17:03:09,643:INFO: --------Dataset Build!--------
2026-01-21 17:03:09,644:INFO: --------Get Dataloader!--------
2026-01-21 17:03:09,644:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 17:03:09,644:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:03:09,645:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 17:03:15,716:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 17:03:15,716:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 17:03:17,064:INFO: --------Start Training!--------
2026-01-21 17:03:20,436:INFO: Epoch: 1, train loss: 3873.5105224609374
2026-01-21 17:03:20,681:INFO: Epoch: 1, dev loss: 1883.8437957763672, f1 score: 0
2026-01-21 17:03:23,609:INFO: Epoch: 2, train loss: 2410.2534423828124
2026-01-21 17:03:23,846:INFO: Epoch: 2, dev loss: 1333.8166885375977, f1 score: 0
2026-01-21 17:03:26,772:INFO: Epoch: 3, train loss: 1792.2630615234375
2026-01-21 17:03:27,032:INFO: Epoch: 3, dev loss: 965.6381683349609, f1 score: 0.015873015873015872
2026-01-21 17:03:27,032:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:03:29,451:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:03:29,451:INFO: --------Save best model!--------
2026-01-21 17:03:32,405:INFO: Epoch: 4, train loss: 1230.1271423339845
2026-01-21 17:03:32,658:INFO: Epoch: 4, dev loss: 707.3463439941406, f1 score: 0.23655913978494622
2026-01-21 17:03:32,659:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:03:35,064:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:03:35,065:INFO: --------Save best model!--------
2026-01-21 17:03:37,873:INFO: Epoch: 5, train loss: 824.3065673828125
2026-01-21 17:03:38,118:INFO: Epoch: 5, dev loss: 512.0493965148926, f1 score: 0.35514018691588783
2026-01-21 17:03:38,119:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:03:40,421:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:03:40,421:INFO: --------Save best model!--------
2026-01-21 17:03:43,211:INFO: Epoch: 6, train loss: 585.80048828125
2026-01-21 17:03:43,453:INFO: Epoch: 6, dev loss: 531.2568817138672, f1 score: 0.5
2026-01-21 17:03:43,454:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:03:45,718:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:03:45,718:INFO: --------Save best model!--------
2026-01-21 17:03:48,741:INFO: Epoch: 7, train loss: 444.24388427734374
2026-01-21 17:03:48,996:INFO: Epoch: 7, dev loss: 407.8824462890625, f1 score: 0.49565217391304345
2026-01-21 17:03:51,968:INFO: Epoch: 8, train loss: 325.8696990966797
2026-01-21 17:03:52,210:INFO: Epoch: 8, dev loss: 516.7688598632812, f1 score: 0.463519313304721
2026-01-21 17:03:54,975:INFO: Epoch: 9, train loss: 254.48197021484376
2026-01-21 17:03:55,237:INFO: Epoch: 9, dev loss: 516.8365783691406, f1 score: 0.5361702127659574
2026-01-21 17:03:55,237:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:03:57,460:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:03:57,460:INFO: --------Save best model!--------
2026-01-21 17:04:00,408:INFO: Epoch: 10, train loss: 200.59842529296876
2026-01-21 17:04:00,645:INFO: Epoch: 10, dev loss: 582.7156372070312, f1 score: 0.5627705627705628
2026-01-21 17:04:00,645:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:04:02,960:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:04:02,961:INFO: --------Save best model!--------
2026-01-21 17:04:05,906:INFO: Epoch: 11, train loss: 167.8255401611328
2026-01-21 17:04:06,148:INFO: Epoch: 11, dev loss: 543.0386352539062, f1 score: 0.5774058577405858
2026-01-21 17:04:06,149:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:04:08,920:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:04:08,920:INFO: --------Save best model!--------
2026-01-21 17:04:11,817:INFO: Epoch: 12, train loss: 146.61627883911132
2026-01-21 17:04:12,058:INFO: Epoch: 12, dev loss: 552.3707580566406, f1 score: 0.6266094420600858
2026-01-21 17:04:12,059:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:04:14,355:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:04:14,355:INFO: --------Save best model!--------
2026-01-21 17:04:17,311:INFO: Epoch: 13, train loss: 117.99413681030273
2026-01-21 17:04:17,555:INFO: Epoch: 13, dev loss: 606.3031158447266, f1 score: 0.5922746781115881
2026-01-21 17:04:20,502:INFO: Epoch: 14, train loss: 110.05206451416015
2026-01-21 17:04:20,742:INFO: Epoch: 14, dev loss: 595.3057327270508, f1 score: 0.5897435897435898
2026-01-21 17:04:23,707:INFO: Epoch: 15, train loss: 97.17729187011719
2026-01-21 17:04:23,964:INFO: Epoch: 15, dev loss: 569.1075592041016, f1 score: 0.5991902834008097
2026-01-21 17:04:26,859:INFO: Epoch: 16, train loss: 77.29278717041015
2026-01-21 17:04:27,101:INFO: Epoch: 16, dev loss: 645.6830978393555, f1 score: 0.6194690265486725
2026-01-21 17:04:29,845:INFO: Epoch: 17, train loss: 80.67219009399415
2026-01-21 17:04:30,084:INFO: Epoch: 17, dev loss: 628.7369918823242, f1 score: 0.5955555555555555
2026-01-21 17:04:33,157:INFO: Epoch: 18, train loss: 64.70297393798828
2026-01-21 17:04:33,419:INFO: Epoch: 18, dev loss: 592.3874969482422, f1 score: 0.6048387096774194
2026-01-21 17:04:36,376:INFO: Epoch: 19, train loss: 56.481178283691406
2026-01-21 17:04:36,638:INFO: Epoch: 19, dev loss: 625.3161773681641, f1 score: 0.6431718061674009
2026-01-21 17:04:36,638:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:04:38,956:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:04:38,956:INFO: --------Save best model!--------
2026-01-21 17:04:41,808:INFO: Epoch: 20, train loss: 43.373452758789064
2026-01-21 17:04:42,072:INFO: Epoch: 20, dev loss: 634.346923828125, f1 score: 0.6212765957446809
2026-01-21 17:04:44,962:INFO: Epoch: 21, train loss: 37.05353546142578
2026-01-21 17:04:45,204:INFO: Epoch: 21, dev loss: 633.4987716674805, f1 score: 0.64
2026-01-21 17:04:48,097:INFO: Epoch: 22, train loss: 32.88386688232422
2026-01-21 17:04:48,340:INFO: Epoch: 22, dev loss: 645.6067199707031, f1 score: 0.6239316239316239
2026-01-21 17:04:51,189:INFO: Epoch: 23, train loss: 24.59876403808594
2026-01-21 17:04:51,452:INFO: Epoch: 23, dev loss: 620.9269714355469, f1 score: 0.6271186440677966
2026-01-21 17:04:54,208:INFO: Epoch: 24, train loss: 21.245018768310548
2026-01-21 17:04:54,454:INFO: Epoch: 24, dev loss: 678.2066650390625, f1 score: 0.6329113924050632
2026-01-21 17:04:57,419:INFO: Epoch: 25, train loss: 18.302853393554688
2026-01-21 17:04:57,682:INFO: Epoch: 25, dev loss: 701.954719543457, f1 score: 0.6351931330472103
2026-01-21 17:05:00,618:INFO: Epoch: 26, train loss: 14.71435546875
2026-01-21 17:05:00,881:INFO: Epoch: 26, dev loss: 725.4974136352539, f1 score: 0.6320346320346321
2026-01-21 17:05:03,836:INFO: Epoch: 27, train loss: 13.415110778808593
2026-01-21 17:05:04,102:INFO: Epoch: 27, dev loss: 755.3911590576172, f1 score: 0.6440677966101694
2026-01-21 17:05:04,102:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:05:06,430:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:05:06,430:INFO: --------Save best model!--------
2026-01-21 17:05:09,448:INFO: Epoch: 28, train loss: 14.74091033935547
2026-01-21 17:05:09,711:INFO: Epoch: 28, dev loss: 767.8835067749023, f1 score: 0.6218487394957983
2026-01-21 17:05:12,675:INFO: Epoch: 29, train loss: 10.769378662109375
2026-01-21 17:05:12,913:INFO: Epoch: 29, dev loss: 740.2515869140625, f1 score: 0.6608695652173914
2026-01-21 17:05:12,914:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:05:15,088:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:05:15,088:INFO: --------Save best model!--------
2026-01-21 17:05:17,826:INFO: Epoch: 30, train loss: 9.325383758544922
2026-01-21 17:05:18,082:INFO: Epoch: 30, dev loss: 751.4990386962891, f1 score: 0.5950413223140496
2026-01-21 17:05:21,039:INFO: Epoch: 31, train loss: 7.385562133789063
2026-01-21 17:05:21,286:INFO: Epoch: 31, dev loss: 740.8399200439453, f1 score: 0.6276150627615062
2026-01-21 17:05:24,288:INFO: Epoch: 32, train loss: 6.7439422607421875
2026-01-21 17:05:24,535:INFO: Epoch: 32, dev loss: 754.8465728759766, f1 score: 0.6351931330472103
2026-01-21 17:05:27,466:INFO: Epoch: 33, train loss: 5.431949615478516
2026-01-21 17:05:27,714:INFO: Epoch: 33, dev loss: 759.2781982421875, f1 score: 0.6468085106382979
2026-01-21 17:05:30,507:INFO: Epoch: 34, train loss: 5.038095855712891
2026-01-21 17:05:30,773:INFO: Epoch: 34, dev loss: 775.0865631103516, f1 score: 0.6495726495726496
2026-01-21 17:05:33,698:INFO: Epoch: 35, train loss: 5.305644226074219
2026-01-21 17:05:33,961:INFO: Epoch: 35, dev loss: 795.7247314453125, f1 score: 0.6413502109704642
2026-01-21 17:05:36,934:INFO: Epoch: 36, train loss: 4.227603149414063
2026-01-21 17:05:37,196:INFO: Epoch: 36, dev loss: 818.582763671875, f1 score: 0.6192468619246861
2026-01-21 17:05:40,226:INFO: Epoch: 37, train loss: 3.597601318359375
2026-01-21 17:05:40,468:INFO: Epoch: 37, dev loss: 810.8203582763672, f1 score: 0.6297872340425532
2026-01-21 17:05:43,470:INFO: Epoch: 38, train loss: 3.6507278442382813
2026-01-21 17:05:43,716:INFO: Epoch: 38, dev loss: 802.7014846801758, f1 score: 0.6324786324786325
2026-01-21 17:05:46,533:INFO: Epoch: 39, train loss: 2.932489776611328
2026-01-21 17:05:46,802:INFO: Epoch: 39, dev loss: 801.2993927001953, f1 score: 0.6324786324786325
2026-01-21 17:05:46,803:INFO: Best val f1: 0.6608695652173914
2026-01-21 17:05:46,803:INFO: Training Finished!
2026-01-21 17:05:46,811:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:05:46,811:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:05:46,811:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:05:46,811:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:05:46,811:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:05:46,811:INFO: loading file None
2026-01-21 17:05:46,811:INFO: loading file None
2026-01-21 17:05:46,811:INFO: loading file None
2026-01-21 17:05:46,885:INFO: --------Dataset Build!--------
2026-01-21 17:05:46,885:INFO: --------Get Data-loader!--------
2026-01-21 17:05:46,885:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:05:46,885:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:05:46,886:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:05:52,042:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 17:05:52,043:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:05:52,043:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:05:52,043:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:05:52,043:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:05:52,043:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:05:52,043:INFO: loading file None
2026-01-21 17:05:52,043:INFO: loading file None
2026-01-21 17:05:52,043:INFO: loading file None
2026-01-21 17:05:52,858:INFO: --------Bad Cases reserved !--------
2026-01-21 17:05:52,862:INFO: test loss: 1078.8439127604167, f1 score: 0.7127659574468086
2026-01-21 17:05:52,862:INFO: f1 score of ACTION: 0.5420560747663551
2026-01-21 17:05:52,862:INFO: f1 score of LEVEL_KEY: 0.68
2026-01-21 17:05:52,863:INFO: f1 score of OBJ: 0.7651006711409397
2026-01-21 17:05:52,863:INFO: f1 score of ORG: 0.7000000000000001
2026-01-21 17:05:52,863:INFO: f1 score of VALUE: 0.8863636363636364
2026-01-21 17:06:44,720:INFO: device: cuda:0
2026-01-21 17:06:44,720:INFO: --------Process Done!--------
2026-01-21 17:06:44,727:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:06:44,727:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:06:44,727:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:06:44,727:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:06:44,727:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:06:44,727:INFO: loading file None
2026-01-21 17:06:44,727:INFO: loading file None
2026-01-21 17:06:44,727:INFO: loading file None
2026-01-21 17:06:45,042:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:06:45,042:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:06:45,042:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:06:45,042:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:06:45,042:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:06:45,042:INFO: loading file None
2026-01-21 17:06:45,042:INFO: loading file None
2026-01-21 17:06:45,042:INFO: loading file None
2026-01-21 17:06:45,084:INFO: --------Dataset Build!--------
2026-01-21 17:06:45,084:INFO: --------Get Dataloader!--------
2026-01-21 17:06:45,084:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 17:06:45,085:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:06:45,085:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 17:06:51,239:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 17:06:51,240:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 17:06:52,603:INFO: --------Start Training!--------
2026-01-21 17:07:05,984:INFO: device: cuda:0
2026-01-21 17:07:05,984:INFO: --------Process Done!--------
2026-01-21 17:07:05,991:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:07:05,991:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:07:05,991:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:07:05,991:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:07:05,991:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:07:05,991:INFO: loading file None
2026-01-21 17:07:05,991:INFO: loading file None
2026-01-21 17:07:05,991:INFO: loading file None
2026-01-21 17:07:06,303:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:07:06,303:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:07:06,303:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:07:06,303:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:07:06,303:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:07:06,303:INFO: loading file None
2026-01-21 17:07:06,303:INFO: loading file None
2026-01-21 17:07:06,303:INFO: loading file None
2026-01-21 17:07:06,344:INFO: --------Dataset Build!--------
2026-01-21 17:07:06,344:INFO: --------Get Dataloader!--------
2026-01-21 17:07:06,344:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 17:07:06,345:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:07:06,345:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 17:07:12,459:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 17:07:12,459:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 17:07:13,820:INFO: --------Start Training!--------
2026-01-21 17:07:17,292:INFO: Epoch: 1, train loss: 3871.324609375
2026-01-21 17:07:17,549:INFO: Epoch: 1, dev loss: 1877.9967651367188, f1 score: 0
2026-01-21 17:07:20,432:INFO: Epoch: 2, train loss: 2376.032470703125
2026-01-21 17:07:20,659:INFO: Epoch: 2, dev loss: 1272.4482727050781, f1 score: 0
2026-01-21 17:07:23,546:INFO: Epoch: 3, train loss: 1734.7651489257812
2026-01-21 17:07:23,779:INFO: Epoch: 3, dev loss: 899.6887969970703, f1 score: 0.015151515151515152
2026-01-21 17:07:23,779:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:25,919:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:25,919:INFO: --------Save best model!--------
2026-01-21 17:07:28,850:INFO: Epoch: 4, train loss: 1175.767919921875
2026-01-21 17:07:29,087:INFO: Epoch: 4, dev loss: 610.9925384521484, f1 score: 0.29556650246305416
2026-01-21 17:07:29,088:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:31,248:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:31,248:INFO: --------Save best model!--------
2026-01-21 17:07:34,118:INFO: Epoch: 5, train loss: 798.3041931152344
2026-01-21 17:07:34,368:INFO: Epoch: 5, dev loss: 526.6855049133301, f1 score: 0.3877551020408163
2026-01-21 17:07:34,369:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:36,497:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:36,497:INFO: --------Save best model!--------
2026-01-21 17:07:39,069:INFO: Epoch: 6, train loss: 592.3361236572266
2026-01-21 17:07:39,304:INFO: Epoch: 6, dev loss: 432.82228088378906, f1 score: 0.5042016806722689
2026-01-21 17:07:39,305:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:42,170:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:42,170:INFO: --------Save best model!--------
2026-01-21 17:07:44,944:INFO: Epoch: 7, train loss: 447.92369384765624
2026-01-21 17:07:45,199:INFO: Epoch: 7, dev loss: 482.68872833251953, f1 score: 0.45378151260504196
2026-01-21 17:07:48,069:INFO: Epoch: 8, train loss: 350.35669860839846
2026-01-21 17:07:48,327:INFO: Epoch: 8, dev loss: 423.95580291748047, f1 score: 0.5258964143426295
2026-01-21 17:07:48,328:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:50,479:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:50,479:INFO: --------Save best model!--------
2026-01-21 17:07:53,360:INFO: Epoch: 9, train loss: 259.8359680175781
2026-01-21 17:07:53,595:INFO: Epoch: 9, dev loss: 458.25496673583984, f1 score: 0.5587044534412955
2026-01-21 17:07:53,596:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:07:54,675:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:07:54,675:INFO: --------Save best model!--------
2026-01-21 17:07:57,522:INFO: Epoch: 10, train loss: 215.99285888671875
2026-01-21 17:07:57,783:INFO: Epoch: 10, dev loss: 511.2684631347656, f1 score: 0.5966386554621849
2026-01-21 17:07:57,784:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:00,225:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:00,225:INFO: --------Save best model!--------
2026-01-21 17:08:03,154:INFO: Epoch: 11, train loss: 167.73174514770508
2026-01-21 17:08:03,411:INFO: Epoch: 11, dev loss: 527.0625228881836, f1 score: 0.6017699115044247
2026-01-21 17:08:03,412:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:05,780:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:05,780:INFO: --------Save best model!--------
2026-01-21 17:08:08,730:INFO: Epoch: 12, train loss: 149.13519439697265
2026-01-21 17:08:08,989:INFO: Epoch: 12, dev loss: 512.9731597900391, f1 score: 0.5910931174089068
2026-01-21 17:08:11,749:INFO: Epoch: 13, train loss: 114.39932479858399
2026-01-21 17:08:11,998:INFO: Epoch: 13, dev loss: 586.1471252441406, f1 score: 0.6306306306306306
2026-01-21 17:08:11,998:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:14,465:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:14,465:INFO: --------Save best model!--------
2026-01-21 17:08:17,324:INFO: Epoch: 14, train loss: 101.25978164672851
2026-01-21 17:08:17,574:INFO: Epoch: 14, dev loss: 528.0001068115234, f1 score: 0.5916666666666667
2026-01-21 17:08:20,487:INFO: Epoch: 15, train loss: 80.82732696533203
2026-01-21 17:08:20,721:INFO: Epoch: 15, dev loss: 514.2970733642578, f1 score: 0.6233766233766234
2026-01-21 17:08:23,539:INFO: Epoch: 16, train loss: 75.63130187988281
2026-01-21 17:08:23,777:INFO: Epoch: 16, dev loss: 508.96678161621094, f1 score: 0.6266094420600858
2026-01-21 17:08:26,677:INFO: Epoch: 17, train loss: 69.34400024414063
2026-01-21 17:08:26,903:INFO: Epoch: 17, dev loss: 545.3768920898438, f1 score: 0.6382978723404255
2026-01-21 17:08:26,904:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:29,311:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:29,311:INFO: --------Save best model!--------
2026-01-21 17:08:32,080:INFO: Epoch: 18, train loss: 55.45519180297852
2026-01-21 17:08:32,338:INFO: Epoch: 18, dev loss: 565.2323532104492, f1 score: 0.6343612334801763
2026-01-21 17:08:35,210:INFO: Epoch: 19, train loss: 44.18653259277344
2026-01-21 17:08:35,447:INFO: Epoch: 19, dev loss: 556.7444152832031, f1 score: 0.6406926406926408
2026-01-21 17:08:35,447:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:37,924:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:37,924:INFO: --------Save best model!--------
2026-01-21 17:08:40,827:INFO: Epoch: 20, train loss: 34.049408721923825
2026-01-21 17:08:41,072:INFO: Epoch: 20, dev loss: 574.2759704589844, f1 score: 0.6297872340425532
2026-01-21 17:08:43,902:INFO: Epoch: 21, train loss: 29.66417694091797
2026-01-21 17:08:44,149:INFO: Epoch: 21, dev loss: 613.8224182128906, f1 score: 0.6457399103139014
2026-01-21 17:08:44,149:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:46,982:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:46,983:INFO: --------Save best model!--------
2026-01-21 17:08:49,886:INFO: Epoch: 22, train loss: 21.82932891845703
2026-01-21 17:08:50,145:INFO: Epoch: 22, dev loss: 616.2758407592773, f1 score: 0.6521739130434782
2026-01-21 17:08:50,146:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:08:52,526:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:08:52,526:INFO: --------Save best model!--------
2026-01-21 17:08:55,226:INFO: Epoch: 23, train loss: 23.87757568359375
2026-01-21 17:08:55,485:INFO: Epoch: 23, dev loss: 614.785774230957, f1 score: 0.6493506493506492
2026-01-21 17:08:58,262:INFO: Epoch: 24, train loss: 14.9025390625
2026-01-21 17:08:58,523:INFO: Epoch: 24, dev loss: 644.9440765380859, f1 score: 0.6548672566371682
2026-01-21 17:08:58,524:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:09:00,687:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:09:00,688:INFO: --------Save best model!--------
2026-01-21 17:09:03,534:INFO: Epoch: 25, train loss: 12.892364501953125
2026-01-21 17:09:03,779:INFO: Epoch: 25, dev loss: 688.6123962402344, f1 score: 0.6460176991150443
2026-01-21 17:09:06,735:INFO: Epoch: 26, train loss: 10.251673126220703
2026-01-21 17:09:06,973:INFO: Epoch: 26, dev loss: 713.2588806152344, f1 score: 0.6696035242290749
2026-01-21 17:09:06,973:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:09:09,721:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:09:09,722:INFO: --------Save best model!--------
2026-01-21 17:09:12,630:INFO: Epoch: 27, train loss: 10.348756408691406
2026-01-21 17:09:12,888:INFO: Epoch: 27, dev loss: 710.2030563354492, f1 score: 0.6315789473684211
2026-01-21 17:09:15,693:INFO: Epoch: 28, train loss: 9.234122467041015
2026-01-21 17:09:15,953:INFO: Epoch: 28, dev loss: 732.2389221191406, f1 score: 0.6324786324786325
2026-01-21 17:09:18,659:INFO: Epoch: 29, train loss: 8.255783843994141
2026-01-21 17:09:18,909:INFO: Epoch: 29, dev loss: 688.3535003662109, f1 score: 0.6468085106382979
2026-01-21 17:09:21,774:INFO: Epoch: 30, train loss: 8.402494812011719
2026-01-21 17:09:22,027:INFO: Epoch: 30, dev loss: 740.9078979492188, f1 score: 0.6521739130434782
2026-01-21 17:09:24,931:INFO: Epoch: 31, train loss: 6.58896484375
2026-01-21 17:09:25,182:INFO: Epoch: 31, dev loss: 768.6519012451172, f1 score: 0.6465517241379309
2026-01-21 17:09:28,051:INFO: Epoch: 32, train loss: 5.562792205810547
2026-01-21 17:09:28,313:INFO: Epoch: 32, dev loss: 776.2127075195312, f1 score: 0.6320346320346321
2026-01-21 17:09:31,256:INFO: Epoch: 33, train loss: 4.176502990722656
2026-01-21 17:09:31,516:INFO: Epoch: 33, dev loss: 784.0260925292969, f1 score: 0.6375545851528384
2026-01-21 17:09:34,351:INFO: Epoch: 34, train loss: 3.7336799621582033
2026-01-21 17:09:34,609:INFO: Epoch: 34, dev loss: 779.7951965332031, f1 score: 0.6493506493506492
2026-01-21 17:09:37,463:INFO: Epoch: 35, train loss: 3.207560729980469
2026-01-21 17:09:37,726:INFO: Epoch: 35, dev loss: 788.6676406860352, f1 score: 0.6521739130434782
2026-01-21 17:09:40,596:INFO: Epoch: 36, train loss: 3.4649642944335937
2026-01-21 17:09:40,859:INFO: Epoch: 36, dev loss: 786.4908752441406, f1 score: 0.6550218340611355
2026-01-21 17:09:40,859:INFO: Best val f1: 0.6696035242290749
2026-01-21 17:09:40,860:INFO: Training Finished!
2026-01-21 17:09:40,867:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:09:40,868:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:09:40,868:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:09:40,868:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:09:40,868:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:09:40,868:INFO: loading file None
2026-01-21 17:09:40,868:INFO: loading file None
2026-01-21 17:09:40,868:INFO: loading file None
2026-01-21 17:09:40,943:INFO: --------Dataset Build!--------
2026-01-21 17:09:40,943:INFO: --------Get Data-loader!--------
2026-01-21 17:09:40,943:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:09:40,944:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:09:40,944:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:09:46,093:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 17:09:46,094:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:09:46,094:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:09:46,094:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:09:46,094:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:09:46,094:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:09:46,094:INFO: loading file None
2026-01-21 17:09:46,094:INFO: loading file None
2026-01-21 17:09:46,094:INFO: loading file None
2026-01-21 17:09:46,896:INFO: --------Bad Cases reserved !--------
2026-01-21 17:09:46,900:INFO: test loss: 996.1788330078125, f1 score: 0.6845878136200717
2026-01-21 17:09:46,900:INFO: f1 score of ACTION: 0.4814814814814815
2026-01-21 17:09:46,900:INFO: f1 score of LEVEL_KEY: 0.6862745098039215
2026-01-21 17:09:46,900:INFO: f1 score of OBJ: 0.7714285714285714
2026-01-21 17:09:46,900:INFO: f1 score of ORG: 0.6333333333333333
2026-01-21 17:09:46,900:INFO: f1 score of VALUE: 0.8636363636363636
2026-01-21 17:10:28,325:INFO: device: cuda:0
2026-01-21 17:10:28,325:INFO: --------Process Done!--------
2026-01-21 17:10:28,331:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:10:28,331:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:10:28,331:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:10:28,331:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:10:28,331:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:10:28,332:INFO: loading file None
2026-01-21 17:10:28,332:INFO: loading file None
2026-01-21 17:10:28,332:INFO: loading file None
2026-01-21 17:10:28,636:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:10:28,636:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:10:28,636:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:10:28,636:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:10:28,636:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:10:28,636:INFO: loading file None
2026-01-21 17:10:28,636:INFO: loading file None
2026-01-21 17:10:28,636:INFO: loading file None
2026-01-21 17:10:28,676:INFO: --------Dataset Build!--------
2026-01-21 17:10:28,676:INFO: --------Get Dataloader!--------
2026-01-21 17:10:28,676:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-21 17:10:28,677:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:10:28,677:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-21 17:10:34,806:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_hh_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'bilstm.weight_ih_l1_reverse', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-21 17:10:34,806:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-21 17:10:36,261:INFO: --------Start Training!--------
2026-01-21 17:10:39,887:INFO: Epoch: 1, train loss: 3869.8932373046873
2026-01-21 17:10:40,130:INFO: Epoch: 1, dev loss: 1906.1461791992188, f1 score: 0
2026-01-21 17:10:42,965:INFO: Epoch: 2, train loss: 2463.7739868164062
2026-01-21 17:10:43,205:INFO: Epoch: 2, dev loss: 1456.902359008789, f1 score: 0
2026-01-21 17:10:46,160:INFO: Epoch: 3, train loss: 1931.1554931640626
2026-01-21 17:10:46,416:INFO: Epoch: 3, dev loss: 1128.6885528564453, f1 score: 0
2026-01-21 17:10:49,166:INFO: Epoch: 4, train loss: 1595.360791015625
2026-01-21 17:10:49,406:INFO: Epoch: 4, dev loss: 848.4787445068359, f1 score: 0.03053435114503817
2026-01-21 17:10:49,406:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:10:51,831:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:10:51,831:INFO: --------Save best model!--------
2026-01-21 17:10:54,701:INFO: Epoch: 5, train loss: 1088.56884765625
2026-01-21 17:10:54,960:INFO: Epoch: 5, dev loss: 657.3822708129883, f1 score: 0.3125
2026-01-21 17:10:54,961:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:10:57,410:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:10:57,410:INFO: --------Save best model!--------
2026-01-21 17:11:00,249:INFO: Epoch: 6, train loss: 764.9881469726563
2026-01-21 17:11:00,488:INFO: Epoch: 6, dev loss: 525.2508087158203, f1 score: 0.4474885844748858
2026-01-21 17:11:00,488:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:02,921:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:02,921:INFO: --------Save best model!--------
2026-01-21 17:11:05,544:INFO: Epoch: 7, train loss: 570.6117858886719
2026-01-21 17:11:05,799:INFO: Epoch: 7, dev loss: 496.6760940551758, f1 score: 0.42857142857142855
2026-01-21 17:11:08,741:INFO: Epoch: 8, train loss: 447.39610595703124
2026-01-21 17:11:08,981:INFO: Epoch: 8, dev loss: 452.97403717041016, f1 score: 0.5422222222222222
2026-01-21 17:11:08,982:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:11,457:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:11,457:INFO: --------Save best model!--------
2026-01-21 17:11:14,352:INFO: Epoch: 9, train loss: 332.40084075927734
2026-01-21 17:11:14,586:INFO: Epoch: 9, dev loss: 501.2168731689453, f1 score: 0.5405405405405406
2026-01-21 17:11:17,449:INFO: Epoch: 10, train loss: 269.99581756591795
2026-01-21 17:11:17,704:INFO: Epoch: 10, dev loss: 500.8292694091797, f1 score: 0.5205479452054794
2026-01-21 17:11:20,479:INFO: Epoch: 11, train loss: 215.90868377685547
2026-01-21 17:11:20,713:INFO: Epoch: 11, dev loss: 494.5911636352539, f1 score: 0.5454545454545455
2026-01-21 17:11:20,714:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:23,225:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:23,225:INFO: --------Save best model!--------
2026-01-21 17:11:26,016:INFO: Epoch: 12, train loss: 173.46819152832032
2026-01-21 17:11:26,266:INFO: Epoch: 12, dev loss: 539.7770080566406, f1 score: 0.6106194690265486
2026-01-21 17:11:26,266:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:27,374:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:27,375:INFO: --------Save best model!--------
2026-01-21 17:11:30,366:INFO: Epoch: 13, train loss: 143.43699340820314
2026-01-21 17:11:30,623:INFO: Epoch: 13, dev loss: 626.0948028564453, f1 score: 0.5462962962962964
2026-01-21 17:11:33,400:INFO: Epoch: 14, train loss: 137.33161468505858
2026-01-21 17:11:33,659:INFO: Epoch: 14, dev loss: 567.1728744506836, f1 score: 0.5897435897435898
2026-01-21 17:11:36,423:INFO: Epoch: 15, train loss: 96.79944915771485
2026-01-21 17:11:36,665:INFO: Epoch: 15, dev loss: 575.2514724731445, f1 score: 0.6147186147186147
2026-01-21 17:11:36,665:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:38,835:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:38,836:INFO: --------Save best model!--------
2026-01-21 17:11:41,602:INFO: Epoch: 16, train loss: 73.88187026977539
2026-01-21 17:11:41,861:INFO: Epoch: 16, dev loss: 625.8098449707031, f1 score: 0.6167400881057269
2026-01-21 17:11:41,861:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:44,282:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:44,283:INFO: --------Save best model!--------
2026-01-21 17:11:47,200:INFO: Epoch: 17, train loss: 67.98572006225587
2026-01-21 17:11:47,452:INFO: Epoch: 17, dev loss: 620.0605926513672, f1 score: 0.6228070175438596
2026-01-21 17:11:47,452:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:11:49,558:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:11:49,558:INFO: --------Save best model!--------
2026-01-21 17:11:52,286:INFO: Epoch: 18, train loss: 57.59069137573242
2026-01-21 17:11:52,544:INFO: Epoch: 18, dev loss: 629.5906066894531, f1 score: 0.6153846153846154
2026-01-21 17:11:55,300:INFO: Epoch: 19, train loss: 47.94579391479492
2026-01-21 17:11:55,537:INFO: Epoch: 19, dev loss: 637.3484039306641, f1 score: 0.611353711790393
2026-01-21 17:11:58,477:INFO: Epoch: 20, train loss: 37.627550506591795
2026-01-21 17:11:58,735:INFO: Epoch: 20, dev loss: 663.0429077148438, f1 score: 0.6068376068376068
2026-01-21 17:12:01,527:INFO: Epoch: 21, train loss: 38.96734619140625
2026-01-21 17:12:01,787:INFO: Epoch: 21, dev loss: 705.4931488037109, f1 score: 0.5974025974025975
2026-01-21 17:12:04,695:INFO: Epoch: 22, train loss: 31.58820495605469
2026-01-21 17:12:04,955:INFO: Epoch: 22, dev loss: 639.9959411621094, f1 score: 0.6068376068376068
2026-01-21 17:12:07,797:INFO: Epoch: 23, train loss: 27.908080291748046
2026-01-21 17:12:08,046:INFO: Epoch: 23, dev loss: 654.3740997314453, f1 score: 0.6271186440677966
2026-01-21 17:12:08,047:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:12:10,861:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:12:10,862:INFO: --------Save best model!--------
2026-01-21 17:12:13,802:INFO: Epoch: 24, train loss: 28.024530792236327
2026-01-21 17:12:14,044:INFO: Epoch: 24, dev loss: 622.9806365966797, f1 score: 0.6147186147186147
2026-01-21 17:12:16,901:INFO: Epoch: 25, train loss: 22.30080795288086
2026-01-21 17:12:17,159:INFO: Epoch: 25, dev loss: 668.0758361816406, f1 score: 0.6297872340425532
2026-01-21 17:12:17,160:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:12:19,578:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:12:19,578:INFO: --------Save best model!--------
2026-01-21 17:12:22,467:INFO: Epoch: 26, train loss: 17.492254638671874
2026-01-21 17:12:22,708:INFO: Epoch: 26, dev loss: 692.3532409667969, f1 score: 0.6437768240343348
2026-01-21 17:12:22,709:INFO: Configuration saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:12:25,135:INFO: Model weights saved in /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:12:25,136:INFO: --------Save best model!--------
2026-01-21 17:12:28,078:INFO: Epoch: 27, train loss: 17.475414276123047
2026-01-21 17:12:28,336:INFO: Epoch: 27, dev loss: 682.1399383544922, f1 score: 0.6127659574468085
2026-01-21 17:12:31,202:INFO: Epoch: 28, train loss: 14.810929107666016
2026-01-21 17:12:31,458:INFO: Epoch: 28, dev loss: 684.1863861083984, f1 score: 0.6297872340425532
2026-01-21 17:12:34,328:INFO: Epoch: 29, train loss: 11.198486328125
2026-01-21 17:12:34,571:INFO: Epoch: 29, dev loss: 700.9268341064453, f1 score: 0.62882096069869
2026-01-21 17:12:37,471:INFO: Epoch: 30, train loss: 8.574909973144532
2026-01-21 17:12:37,729:INFO: Epoch: 30, dev loss: 717.2837219238281, f1 score: 0.6153846153846154
2026-01-21 17:12:40,431:INFO: Epoch: 31, train loss: 9.587854766845703
2026-01-21 17:12:40,669:INFO: Epoch: 31, dev loss: 728.9299774169922, f1 score: 0.6173913043478261
2026-01-21 17:12:43,534:INFO: Epoch: 32, train loss: 7.465425872802735
2026-01-21 17:12:43,774:INFO: Epoch: 32, dev loss: 748.8053131103516, f1 score: 0.6086956521739131
2026-01-21 17:12:46,677:INFO: Epoch: 33, train loss: 6.072981262207032
2026-01-21 17:12:46,934:INFO: Epoch: 33, dev loss: 754.367431640625, f1 score: 0.611353711790393
2026-01-21 17:12:49,864:INFO: Epoch: 34, train loss: 4.988935852050782
2026-01-21 17:12:50,107:INFO: Epoch: 34, dev loss: 769.2135772705078, f1 score: 0.6293103448275862
2026-01-21 17:12:53,019:INFO: Epoch: 35, train loss: 4.791015625
2026-01-21 17:12:53,258:INFO: Epoch: 35, dev loss: 781.0002746582031, f1 score: 0.611353711790393
2026-01-21 17:12:56,170:INFO: Epoch: 36, train loss: 5.3106239318847654
2026-01-21 17:12:56,413:INFO: Epoch: 36, dev loss: 780.2192916870117, f1 score: 0.611353711790393
2026-01-21 17:12:56,413:INFO: Best val f1: 0.6437768240343348
2026-01-21 17:12:56,413:INFO: Training Finished!
2026-01-21 17:12:56,421:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:12:56,421:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:12:56,421:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:12:56,421:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:12:56,421:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:12:56,421:INFO: loading file None
2026-01-21 17:12:56,421:INFO: loading file None
2026-01-21 17:12:56,421:INFO: loading file None
2026-01-21 17:12:56,496:INFO: --------Dataset Build!--------
2026-01-21 17:12:56,497:INFO: --------Get Data-loader!--------
2026-01-21 17:12:56,497:INFO: loading configuration file /root/msy/ner/BERT-LSTM-CRF/experiments/my/config.json
2026-01-21 17:12:56,497:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "lstm_dropout_prob": 0.5,
  "lstm_embedding_size": 1024,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-21 17:12:56,498:INFO: loading weights file /root/msy/ner/BERT-LSTM-CRF/experiments/my/pytorch_model.bin
2026-01-21 17:13:01,672:INFO: --------Load model from /root/msy/ner/BERT-LSTM-CRF/experiments/my/--------
2026-01-21 17:13:01,673:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-21 17:13:01,674:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-21 17:13:01,674:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-21 17:13:01,674:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-21 17:13:01,674:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-21 17:13:01,674:INFO: loading file None
2026-01-21 17:13:01,674:INFO: loading file None
2026-01-21 17:13:01,674:INFO: loading file None
2026-01-21 17:13:02,481:INFO: --------Bad Cases reserved !--------
2026-01-21 17:13:02,485:INFO: test loss: 966.9871622721354, f1 score: 0.6571936056838366
2026-01-21 17:13:02,485:INFO: f1 score of ACTION: 0.46017699115044247
2026-01-21 17:13:02,485:INFO: f1 score of LEVEL_KEY: 0.5252525252525253
2026-01-21 17:13:02,485:INFO: f1 score of OBJ: 0.7619047619047619
2026-01-21 17:13:02,485:INFO: f1 score of ORG: 0.6842105263157895
2026-01-21 17:13:02,485:INFO: f1 score of VALUE: 0.8444444444444444
