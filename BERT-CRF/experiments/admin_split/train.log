2026-01-14 17:54:06,705:INFO: device: cuda:0
2026-01-14 17:54:06,705:WARNING: Fixed: 水库主流长
2026-01-14 17:54:06,705:WARNING: Fixed: 集雨面积
2026-01-14 17:54:06,705:WARNING: Fixed: 8km
2026-01-14 17:54:06,705:WARNING: Fixed: 20.67km2
2026-01-14 17:54:06,705:WARNING: Fixed: 观测站
2026-01-14 17:54:06,705:WARNING: Fixed: 水情调度中心
2026-01-14 17:54:06,705:WARNING: Fixed: 平江县防汛抗旱指挥部
2026-01-14 17:54:06,706:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,706:WARNING: Fixed: 审批
2026-01-14 17:54:06,706:WARNING: Fixed: 库水位
2026-01-14 17:54:06,706:WARNING: Fixed: 取水闸
2026-01-14 17:54:06,706:WARNING: Fixed: 死水位
2026-01-14 17:54:06,706:WARNING: Fixed: 85.0m
2026-01-14 17:54:06,706:WARNING: Fixed: 开启
2026-01-14 17:54:06,706:WARNING: Fixed: 调度
2026-01-14 17:54:06,706:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-14 17:54:06,706:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,706:WARNING: Fixed: 汨罗江流域
2026-01-14 17:54:06,706:WARNING: Fixed: 水库控制集雨面积
2026-01-14 17:54:06,706:WARNING: Fixed: 水库总库容
2026-01-14 17:54:06,706:WARNING: Fixed: 正常蓄水位
2026-01-14 17:54:06,706:WARNING: Fixed: 20 年一遇设计洪水位
2026-01-14 17:54:06,706:WARNING: Fixed: 200年一遇校核洪水位
2026-01-14 17:54:06,706:WARNING: Fixed: 死水位
2026-01-14 17:54:06,706:WARNING: Fixed: 1.34 km²
2026-01-14 17:54:06,706:WARNING: Fixed: 44.30 万m³
2026-01-14 17:54:06,706:WARNING: Fixed: 97.48 m
2026-01-14 17:54:06,706:WARNING: Fixed: 97.21 m
2026-01-14 17:54:06,706:WARNING: Fixed: 97.51 m
2026-01-14 17:54:06,707:WARNING: Fixed: 89.36 m
2026-01-14 17:54:06,707:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,707:WARNING: Fixed: 库水位
2026-01-14 17:54:06,707:WARNING: Fixed: 64.73～60.00m
2026-01-14 17:54:06,707:WARNING: Fixed:  64.73～62.00m 
2026-01-14 17:54:06,707:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,707:WARNING: Fixed: 浦阳江
2026-01-14 17:54:06,707:WARNING: Fixed: 坝底海拔高程
2026-01-14 17:54:06,707:WARNING: Fixed: 海拔高程
2026-01-14 17:54:06,707:WARNING: Fixed: 自然落差
2026-01-14 17:54:06,707:WARNING: Fixed: 318.12m
2026-01-14 17:54:06,707:WARNING: Fixed: 70.12m
2026-01-14 17:54:06,707:WARNING: Fixed: 248m
2026-01-14 17:54:06,707:WARNING: Fixed: 水位
2026-01-14 17:54:06,707:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,707:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,707:WARNING: Fixed: 防洪特征水位
2026-01-14 17:54:06,707:WARNING: Fixed: 集雨面积
2026-01-14 17:54:06,707:WARNING: Fixed: 集雨面积
2026-01-14 17:54:06,707:WARNING: Fixed: 控制泄量
2026-01-14 17:54:06,707:WARNING: Fixed: 56.5km²
2026-01-14 17:54:06,708:WARNING: Fixed: 47.97km²
2026-01-14 17:54:06,708:WARNING: Fixed: 300m³/s
2026-01-14 17:54:06,708:WARNING: Fixed: 水库库水位
2026-01-14 17:54:06,708:WARNING: Fixed: 库内水位
2026-01-14 17:54:06,708:WARNING: Fixed: 96.39 m
2026-01-14 17:54:06,708:WARNING: Fixed:  50 （mm）
2026-01-14 17:54:06,708:WARNING: Fixed: 95.39 m
2026-01-14 17:54:06,708:WARNING: Fixed: 预报降雨
2026-01-14 17:54:06,708:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,708:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,708:WARNING: Fixed: 62.99m
2026-01-14 17:54:06,708:WARNING: Fixed:  300m³/s 
2026-01-14 17:54:06,708:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,708:WARNING: Fixed: 62.99～64.73m
2026-01-14 17:54:06,708:WARNING: Fixed: 下游下王控制断面安全泄量
2026-01-14 17:54:06,708:WARNING: Fixed: 最大下泄流量
2026-01-14 17:54:06,708:WARNING: Fixed: 关闸蓄洪
2026-01-14 17:54:06,708:WARNING: Fixed: 水库管理及水库调度
2026-01-14 17:54:06,708:WARNING: Fixed: 妥善保管水库调度运行有关资料并归档
2026-01-14 17:54:06,708:WARNING: Fixed: 上级主管部门
2026-01-14 17:54:06,708:WARNING: Fixed: 库水位
2026-01-14 17:54:06,709:WARNING: Fixed: 水库
2026-01-14 17:54:06,709:WARNING: Fixed:  60.48m
2026-01-14 17:54:06,709:WARNING: Fixed: 60.68m
2026-01-14 17:54:06,709:WARNING: Fixed: 关闸蓄洪
2026-01-14 17:54:06,709:WARNING: Fixed: 控制水位
2026-01-14 17:54:06,709:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,709:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,709:WARNING: Fixed: 水位
2026-01-14 17:54:06,709:WARNING: Fixed: 紧急水位
2026-01-14 17:54:06,709:WARNING: Fixed: 指挥所全体人员及物资进入现场
2026-01-14 17:54:06,709:WARNING: Fixed: 并通知下游作好防洪避险准备
2026-01-14 17:54:06,709:WARNING: Fixed: 库水位
2026-01-14 17:54:06,709:WARNING: Fixed: 150m
2026-01-14 17:54:06,709:WARNING: Fixed: 155m
2026-01-14 17:54:06,709:WARNING: Fixed: 采取错峰调度
2026-01-14 17:54:06,709:WARNING: Fixed: 水库
2026-01-14 17:54:06,709:WARNING: Fixed: 防洪调度
2026-01-14 17:54:06,709:WARNING: Fixed: 兴利调度
2026-01-14 17:54:06,709:WARNING: Fixed: 梅汛期限制水位
2026-01-14 17:54:06,709:WARNING: Fixed: 台汛期限制水位
2026-01-14 17:54:06,710:WARNING: Fixed: 批复水位
2026-01-14 17:54:06,710:WARNING: Fixed: 非汛期正常蓄水位
2026-01-14 17:54:06,710:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,710:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,710:WARNING: Fixed: 62.00m
2026-01-14 17:54:06,710:WARNING: Fixed: 防汛抗旱指挥机构
2026-01-14 17:54:06,710:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,710:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,710:WARNING: Fixed: 下泄流量
2026-01-14 17:54:06,710:WARNING: Fixed: 设计洪水位
2026-01-14 17:54:06,710:WARNING: Fixed: 天然洪峰
2026-01-14 17:54:06,710:WARNING: Fixed: 汛限水位
2026-01-14 17:54:06,710:WARNING: Fixed: 泄洪
2026-01-14 17:54:06,710:WARNING: Fixed: 农业灌溉
2026-01-14 17:54:06,710:WARNING: Fixed: 启动抗旱应急调度
2026-01-14 17:54:06,710:WARNING: Fixed: 正常蓄水位
2026-01-14 17:54:06,710:WARNING: Fixed: 105.00m
2026-01-14 17:54:06,710:WARNING: Fixed: 水库流域
2026-01-14 17:54:06,710:WARNING: Fixed: 钱塘江流域
2026-01-14 17:54:06,710:WARNING: Fixed: 壶源江
2026-01-14 17:54:06,710:WARNING: Fixed: 库岭溪
2026-01-14 17:54:06,710:WARNING: Fixed: 富春江
2026-01-14 17:54:06,711:WARNING: Fixed: 海拔
2026-01-14 17:54:06,711:WARNING: Fixed: 流域面积
2026-01-14 17:54:06,711:WARNING: Fixed: 流域面积
2026-01-14 17:54:06,711:WARNING: Fixed: 818m
2026-01-14 17:54:06,711:WARNING: Fixed: 760.9km2
2026-01-14 17:54:06,711:WARNING: Fixed: 383.1km2
2026-01-14 17:54:06,711:WARNING: Fixed: 雨量站
2026-01-14 17:54:06,711:WARNING: Fixed: 鹤山市气象站
2026-01-14 17:54:06,711:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,711:WARNING: Fixed: 金坑岭水库
2026-01-14 17:54:06,711:WARNING: Fixed: 年平均供水总量
2026-01-14 17:54:06,711:WARNING: Fixed:  868 万 m3
2026-01-14 17:54:06,711:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,711:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,711:WARNING: Fixed:  20 年一遇洪水位
2026-01-14 17:54:06,711:WARNING: Fixed: 水库最大下泄流量
2026-01-14 17:54:06,711:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,711:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,711:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,711:WARNING: Fixed: 200年一遇校核洪水位
2026-01-14 17:54:06,712:WARNING: Fixed:  200 年一遇洪水位
2026-01-14 17:54:06,712:WARNING: Fixed: 165.82m
2026-01-14 17:54:06,712:WARNING: Fixed: 165.82m
2026-01-14 17:54:06,712:WARNING: Fixed: 水库及下游抗洪抢险工作
2026-01-14 17:54:06,712:WARNING: Fixed: 址山镇三防指挥
2026-01-14 17:54:06,712:WARNING: Fixed: 址山镇三防指挥所
2026-01-14 17:54:06,712:WARNING: Fixed: 市三防指挥部
2026-01-14 17:54:06,712:WARNING: Fixed: 调度运行计划
2026-01-14 17:54:06,712:WARNING: Fixed: 备案
2026-01-14 17:54:06,712:WARNING: Fixed: 水利局
2026-01-14 17:54:06,712:WARNING: Fixed: 省防指
2026-01-14 17:54:06,712:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-14 17:54:06,712:WARNING: Fixed: 水情中心站
2026-01-14 17:54:06,712:WARNING: Fixed: 水情中心站
2026-01-14 17:54:06,712:WARNING: Fixed: 遥测站
2026-01-14 17:54:06,712:WARNING: Fixed: 水库调度管理
2026-01-14 17:54:06,712:WARNING: Fixed: 水库险情监测与巡视检查
2026-01-14 17:54:06,712:WARNING: Fixed: 抢险
2026-01-14 17:54:06,712:WARNING: Fixed: 应急调度
2026-01-14 17:54:06,712:WARNING: Fixed: 信息报告
2026-01-14 17:54:06,712:WARNING: Fixed: 参与预案的全过程
2026-01-14 17:54:06,713:WARNING: Fixed: 参与应急会商
2026-01-14 17:54:06,713:WARNING: Fixed: 成应急指挥机构交办的任务
2026-01-14 17:54:06,713:WARNING: Fixed: 址山镇三防指挥所
2026-01-14 17:54:06,713:WARNING: Fixed: 奉化区水文站
2026-01-14 17:54:06,713:WARNING: Fixed: 宁波市葛岙水库开发有限公司
2026-01-14 17:54:06,713:WARNING: Fixed: 宁波市水利水电规划设计研究院有限公司
2026-01-14 17:54:06,713:WARNING: Fixed: 浙江省正邦水电建设有限公司
2026-01-14 17:54:06,713:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,713:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,713:WARNING: Fixed: 水库库区
2026-01-14 17:54:06,713:WARNING: Fixed: 水库枢纽
2026-01-14 17:54:06,713:WARNING: Fixed: 水库保护范围面积
2026-01-14 17:54:06,713:WARNING: Fixed: 0.792km2
2026-01-14 17:54:06,713:WARNING: Fixed: 下泄流量
2026-01-14 17:54:06,713:WARNING: Fixed: 下游城镇防洪标准
2026-01-14 17:54:06,713:WARNING: Fixed: 库水位
2026-01-14 17:54:06,713:WARNING: Fixed: 库容
2026-01-14 17:54:06,713:WARNING: Fixed: 60.00～62.00m
2026-01-14 17:54:06,713:WARNING: Fixed: 重叠库容
2026-01-14 17:54:06,714:WARNING: Fixed: 水库
2026-01-14 17:54:06,714:WARNING: Fixed: 水库
2026-01-14 17:54:06,714:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,714:WARNING: Fixed: 正常水位
2026-01-14 17:54:06,714:WARNING: Fixed:  163m
2026-01-14 17:54:06,714:WARNING: Fixed: 设计洪水标准
2026-01-14 17:54:06,714:WARNING: Fixed: 校核洪水标准
2026-01-14 17:54:06,714:WARNING: Fixed: 白云水库
2026-01-14 17:54:06,714:WARNING: Fixed: 大坝
2026-01-14 17:54:06,714:WARNING: Fixed: 放水涵管
2026-01-14 17:54:06,714:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,714:WARNING: Fixed: 防洪调度任务
2026-01-14 17:54:06,714:WARNING: Fixed: 水库大坝
2026-01-14 17:54:06,714:WARNING: Fixed: 20年一遇设计洪水标准
2026-01-14 17:54:06,714:WARNING: Fixed: 200年一遇的校核洪水标准
2026-01-14 17:54:06,714:WARNING: Fixed: 水库防洪控制断面
2026-01-14 17:54:06,714:WARNING: Fixed: 安全泄量
2026-01-14 17:54:06,714:WARNING: Fixed: 10.75*1.7m
2026-01-14 17:54:06,714:WARNING: Fixed: 15.72   m3/s
2026-01-14 17:54:06,715:WARNING: Fixed: 库水位
2026-01-14 17:54:06,715:WARNING: Fixed: 水库
2026-01-14 17:54:06,715:WARNING: Fixed: 61.22m
2026-01-14 17:54:06,715:WARNING: Fixed: 加大下泄流量
2026-01-14 17:54:06,715:WARNING: Fixed: 敞泄
2026-01-14 17:54:06,715:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,715:WARNING: Fixed: 启动
2026-01-14 17:54:06,715:WARNING: Fixed: 气象台
2026-01-14 17:54:06,715:WARNING: Fixed: 水库管理处
2026-01-14 17:54:06,715:WARNING: Fixed: 100mm
2026-01-14 17:54:06,715:WARNING: Fixed: 大坝
2026-01-14 17:54:06,715:WARNING: Fixed: 巡视检查
2026-01-14 17:54:06,715:WARNING: Fixed: 库水位
2026-01-14 17:54:06,715:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,715:WARNING: Fixed: 大坝
2026-01-14 17:54:06,715:WARNING: Fixed: 72.50m
2026-01-14 17:54:06,715:WARNING: Fixed: 自由溢流
2026-01-14 17:54:06,715:WARNING: Fixed: 三市镇
2026-01-14 17:54:06,716:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,716:WARNING: Fixed: 审批
2026-01-14 17:54:06,716:WARNING: Fixed: 协调
2026-01-14 17:54:06,716:WARNING: Fixed: 最高水位
2026-01-14 17:54:06,716:WARNING: Fixed: 1000年一遇校核洪水位
2026-01-14 17:54:06,716:WARNING: Fixed: 66.33m
2026-01-14 17:54:06,716:WARNING: Fixed: 水库
2026-01-14 17:54:06,716:WARNING: Fixed: 水位
2026-01-14 17:54:06,716:WARNING: Fixed: 水底孔
2026-01-14 17:54:06,716:WARNING: Fixed: 165.50m
2026-01-14 17:54:06,716:WARNING: Fixed: 开启
2026-01-14 17:54:06,716:WARNING: Fixed: 排沙
2026-01-14 17:54:06,716:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,716:WARNING: Fixed: 水库
2026-01-14 17:54:06,716:WARNING: Fixed: 库水位
2026-01-14 17:54:06,716:WARNING: Fixed: 校核洪水标
2026-01-14 17:54:06,716:WARNING: Fixed: 66.33m
2026-01-14 17:54:06,716:WARNING: Fixed: 泄洪
2026-01-14 17:54:06,716:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,716:WARNING: Fixed: 防限水位
2026-01-14 17:54:06,717:WARNING: Fixed: 163m
2026-01-14 17:54:06,717:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,717:WARNING: Fixed: 每隔一小时将水库水位、降雨量等情况报告
2026-01-14 17:54:06,717:WARNING: Fixed: 汛期水位
2026-01-14 17:54:06,717:WARNING: Fixed: 355.12m 
2026-01-14 17:54:06,717:WARNING: Fixed: 鹤山市址山镇农业综合服务中心
2026-01-14 17:54:06,717:WARNING: Fixed: 大坝、输水涵管和溢洪道
2026-01-14 17:54:06,717:WARNING: Fixed: 抢险
2026-01-14 17:54:06,717:WARNING: Fixed: 采取切实有效的保坝措施
2026-01-14 17:54:06,717:WARNING: Fixed: 防洪限制水位
2026-01-14 17:54:06,717:WARNING: Fixed: 96.39m
2026-01-14 17:54:06,717:WARNING: Fixed: 万安水库
2026-01-14 17:54:06,717:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,717:WARNING: Fixed: 防洪限制水位
2026-01-14 17:54:06,717:WARNING: Fixed: 145.0m
2026-01-14 17:54:06,717:WARNING: Fixed: 市水务局
2026-01-14 17:54:06,717:WARNING: Fixed: 下达调度令
2026-01-14 17:54:06,717:WARNING: Fixed: 开启
2026-01-14 17:54:06,717:WARNING: Fixed: 长山水库
2026-01-14 17:54:06,717:WARNING: Fixed: 1500
2026-01-14 17:54:06,717:WARNING: Fixed: 2000
2026-01-14 17:54:06,718:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,718:WARNING: Fixed: 金坑岭水库
2026-01-14 17:54:06,718:WARNING: Fixed: 壶源江电站
2026-01-14 17:54:06,718:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,718:WARNING: Fixed: 下游下王控制断面安全泄量
2026-01-14 17:54:06,718:WARNING: Fixed: 水库最大下泄流量
2026-01-14 17:54:06,718:WARNING: Fixed: 300m³/s
2026-01-14 17:54:06,718:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,718:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,718:WARNING: Fixed: 20年一遇设计洪水位
2026-01-14 17:54:06,718:WARNING: Fixed: 200年一遇校核洪水位
2026-01-14 17:54:06,718:WARNING: Fixed: 溢洪水深
2026-01-14 17:54:06,718:WARNING: Fixed: 165.20m
2026-01-14 17:54:06,718:WARNING: Fixed: 165.82m
2026-01-14 17:54:06,718:WARNING: Fixed: 2.82m
2026-01-14 17:54:06,718:WARNING: Fixed: 水库防汛工作
2026-01-14 17:54:06,718:WARNING: Fixed: 址山镇三防指挥
2026-01-14 17:54:06,718:WARNING: Fixed: 金坑岭水库
2026-01-14 17:54:06,718:WARNING: Fixed: 库水位
2026-01-14 17:54:06,718:WARNING: Fixed: 最低运行水位
2026-01-14 17:54:06,719:WARNING: Fixed: 324.92m
2026-01-14 17:54:06,719:WARNING: Fixed:  338.12m
2026-01-14 17:54:06,719:WARNING: Fixed: 停止发电
2026-01-14 17:54:06,719:WARNING: Fixed: 坝址
2026-01-14 17:54:06,719:WARNING: Fixed: 防洪控制点
2026-01-14 17:54:06,719:WARNING: Fixed: 水库下泄流量
2026-01-14 17:54:06,719:WARNING: Fixed: 固定泄量调度
2026-01-14 17:54:06,719:WARNING: Fixed: 累计降雨量
2026-01-14 17:54:06,719:WARNING: Fixed: 1小时降雨量
2026-01-14 17:54:06,719:WARNING: Fixed: 降雨量
2026-01-14 17:54:06,719:WARNING: Fixed: 100mm
2026-01-14 17:54:06,719:WARNING: Fixed: 50mm
2026-01-14 17:54:06,719:WARNING: Fixed: 30mm
2026-01-14 17:54:06,719:WARNING: Fixed: 在30分钟内将降雨情况报告
2026-01-14 17:54:06,719:WARNING: Fixed: 加报一次
2026-01-14 17:54:06,719:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,719:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,719:WARNING: Fixed: 汛限水位
2026-01-14 17:54:06,719:WARNING: Fixed: 65.5m
2026-01-14 17:54:06,719:WARNING: Fixed: 镇农办
2026-01-14 17:54:06,719:WARNING: Fixed: 汇报
2026-01-14 17:54:06,719:WARNING: Fixed: 梅汛期限制水位
2026-01-14 17:54:06,720:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,720:WARNING: Fixed: 60.00m 
2026-01-14 17:54:06,720:WARNING: Fixed: 库水位
2026-01-14 17:54:06,720:WARNING: Fixed: 下泄
2026-01-14 17:54:06,720:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,720:WARNING: Fixed: 东江河道
2026-01-14 17:54:06,720:WARNING: Fixed: 汛期水位
2026-01-14 17:54:06,720:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,720:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,720:WARNING: Fixed: 发电放水
2026-01-14 17:54:06,720:WARNING: Fixed: 自由泄洪
2026-01-14 17:54:06,720:WARNING: Fixed: 水位
2026-01-14 17:54:06,720:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,720:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,720:WARNING: Fixed: 坝址
2026-01-14 17:54:06,720:WARNING: Fixed: 防洪控制点
2026-01-14 17:54:06,720:WARNING: Fixed: 防洪控制点洪水
2026-01-14 17:54:06,720:WARNING: Fixed: 补偿调度
2026-01-14 17:54:06,720:WARNING: Fixed: 坝体
2026-01-14 17:54:06,720:WARNING: Fixed: 坝基
2026-01-14 17:54:06,721:WARNING: Fixed: 坝肩渗漏点
2026-01-14 17:54:06,721:WARNING: Fixed: 输水涵管
2026-01-14 17:54:06,721:WARNING: Fixed: 定期进行重点巡视检查
2026-01-14 17:54:06,721:WARNING: Fixed: 防洪调度
2026-01-14 17:54:06,721:WARNING: Fixed: 奉化区水利局
2026-01-14 17:54:06,721:WARNING: Fixed: 宁波市水利局
2026-01-14 17:54:06,721:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,721:WARNING: Fixed:  338.12m
2026-01-14 17:54:06,721:WARNING: Fixed: 灌溉
2026-01-14 17:54:06,721:WARNING: Fixed: 供水
2026-01-14 17:54:06,721:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,721:WARNING: Fixed:  338.12m
2026-01-14 17:54:06,721:WARNING: Fixed: 供水调度
2026-01-14 17:54:06,721:WARNING: Fixed: 年平均相对湿度
2026-01-14 17:54:06,721:WARNING: Fixed: 年平均陆面蒸发量
2026-01-14 17:54:06,721:WARNING: Fixed: 平均径流
2026-01-14 17:54:06,721:WARNING: Fixed:  80%以上
2026-01-14 17:54:06,721:WARNING: Fixed: 75%以上
2026-01-14 17:54:06,721:WARNING: Fixed: 650mm～700mm
2026-01-14 17:54:06,721:WARNING: Fixed: 750mm～800mm
2026-01-14 17:54:06,721:WARNING: Fixed: 1000mm～400mm
2026-01-14 17:54:06,722:WARNING: Fixed: 720mm
2026-01-14 17:54:06,722:WARNING: Fixed: 控制水位
2026-01-14 17:54:06,722:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,722:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,722:WARNING: Fixed: 水库坝前水位
2026-01-14 17:54:06,722:WARNING: Fixed: 设计水位
2026-01-14 17:54:06,722:WARNING: Fixed: 防汛领导小组
2026-01-14 17:54:06,722:WARNING: Fixed: 向上级有关部门报告
2026-01-14 17:54:06,722:WARNING: Fixed: 启动上下游群众转移预案
2026-01-14 17:54:06,722:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,722:WARNING: Fixed: 61.22m
2026-01-14 17:54:06,722:WARNING: Fixed: 加大下泄流量
2026-01-14 17:54:06,722:WARNING: Fixed: 敞泄
2026-01-14 17:54:06,722:WARNING: Fixed: 汛限水位
2026-01-14 17:54:06,722:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,722:WARNING: Fixed: 奉化江干流
2026-01-14 17:54:06,722:WARNING: Fixed: 奉化江
2026-01-14 17:54:06,722:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,722:WARNING: Fixed: 雨量将
2026-01-14 17:54:06,723:WARNING: Fixed: 150mm
2026-01-14 17:54:06,723:WARNING: Fixed: 水库
2026-01-14 17:54:06,723:WARNING: Fixed: 泄洪
2026-01-14 17:54:06,723:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,723:WARNING: Fixed: 河床
2026-01-14 17:54:06,723:WARNING: Fixed: 高程
2026-01-14 17:54:06,723:WARNING: Fixed: 10～35m
2026-01-14 17:54:06,723:WARNING: Fixed: 65.0～36.0m
2026-01-14 17:54:06,723:WARNING: Fixed: 库水位
2026-01-14 17:54:06,723:WARNING: Fixed: 水库
2026-01-14 17:54:06,723:WARNING: Fixed: 历史最高水位
2026-01-14 17:54:06,723:WARNING: Fixed: 设计洪水位
2026-01-14 17:54:06,723:WARNING: Fixed: 设计死水位
2026-01-14 17:54:06,723:WARNING: Fixed: 356.86 m
2026-01-14 17:54:06,723:WARNING: Fixed: 324.92m
2026-01-14 17:54:06,723:WARNING: Fixed: 库水位
2026-01-14 17:54:06,723:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,723:WARNING: Fixed: 20mm
2026-01-14 17:54:06,723:WARNING: Fixed: 三市镇人民政府
2026-01-14 17:54:06,723:WARNING: Fixed: 死水位
2026-01-14 17:54:06,723:WARNING: Fixed: 130.0m
2026-01-14 17:54:06,723:WARNING: Fixed: 水库
2026-01-14 17:54:06,724:WARNING: Fixed: 停止发电
2026-01-14 17:54:06,724:WARNING: Fixed: 水位
2026-01-14 17:54:06,724:WARNING: Fixed: 水库
2026-01-14 17:54:06,724:WARNING: Fixed: 危急水位
2026-01-14 17:54:06,724:WARNING: Fixed: 总指所
2026-01-14 17:54:06,724:WARNING: Fixed: 决策
2026-01-14 17:54:06,724:WARNING: Fixed: 视情况采取措施
2026-01-14 17:54:06,724:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,724:WARNING: Fixed: 浦江县水电局
2026-01-14 17:54:06,724:WARNING: Fixed: 东江
2026-01-14 17:54:06,724:WARNING: Fixed: 山顶高程
2026-01-14 17:54:06,724:WARNING: Fixed: 岸坡坡度
2026-01-14 17:54:06,724:WARNING: Fixed: 120～600m
2026-01-14 17:54:06,724:WARNING: Fixed: 10～35°
2026-01-14 17:54:06,724:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-14 17:54:06,724:WARNING: Fixed: 组织协调有关职能部门工作
2026-01-14 17:54:06,724:WARNING: Fixed: 水库设计防洪标准
2026-01-14 17:54:06,724:WARNING: Fixed: 水位
2026-01-14 17:54:06,724:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,724:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,725:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,725:WARNING: Fixed: 壶源江
2026-01-14 17:54:06,725:WARNING: Fixed: 主流长
2026-01-14 17:54:06,725:WARNING: Fixed: 集雨面积
2026-01-14 17:54:06,725:WARNING: Fixed: 8km
2026-01-14 17:54:06,725:WARNING: Fixed: 20.67km2
2026-01-14 17:54:06,725:WARNING: Fixed: 起调水位
2026-01-14 17:54:06,725:WARNING: Fixed: 水库最大下泄流量
2026-01-14 17:54:06,725:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,725:WARNING: Fixed: 60.48m
2026-01-14 17:54:06,725:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,725:WARNING: Fixed: 库水位
2026-01-14 17:54:06,725:WARNING: Fixed: 管理单位
2026-01-14 17:54:06,725:WARNING: Fixed: 防汛预案
2026-01-14 17:54:06,725:WARNING: Fixed: 库水位
2026-01-14 17:54:06,725:WARNING: Fixed: 水库
2026-01-14 17:54:06,725:WARNING: Fixed: 下泄
2026-01-14 17:54:06,725:WARNING: Fixed: 汛限水位
2026-01-14 17:54:06,725:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,725:WARNING: Fixed: 横山水库
2026-01-14 17:54:06,725:WARNING: Fixed: 灌溉面积
2026-01-14 17:54:06,725:WARNING: Fixed: 3766亩
2026-01-14 17:54:06,726:WARNING: Fixed: 启动防御超标准洪水预案
2026-01-14 17:54:06,726:WARNING: Fixed: 应急指挥机构
2026-01-14 17:54:06,726:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,726:WARNING: Fixed: 控制蓄水位
2026-01-14 17:54:06,726:WARNING: Fixed: 62.00m
2026-01-14 17:54:06,726:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,726:WARNING: Fixed: 设计洪水位
2026-01-14 17:54:06,726:WARNING: Fixed: 校核洪水位
2026-01-14 17:54:06,726:WARNING: Fixed:  65.31m
2026-01-14 17:54:06,726:WARNING: Fixed: 66.33m
2026-01-14 17:54:06,726:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,726:WARNING: Fixed: 浦江县人民政府防汛防旱指挥部
2026-01-14 17:54:06,726:WARNING: Fixed: 浦江县水务局
2026-01-14 17:54:06,726:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,726:WARNING: Fixed: 水库
2026-01-14 17:54:06,726:WARNING: Fixed: 库水位
2026-01-14 17:54:06,726:WARNING: Fixed: 设计洪水标准
2026-01-14 17:54:06,727:WARNING: Fixed: 65.31m
2026-01-14 17:54:06,727:WARNING: Fixed: 全开泄洪
2026-01-14 17:54:06,727:WARNING: Fixed: 大坝
2026-01-14 17:54:06,727:WARNING: Fixed: 大坝
2026-01-14 17:54:06,727:WARNING: Fixed: 管理所
2026-01-14 17:54:06,727:WARNING: Fixed: 东江
2026-01-14 17:54:06,727:WARNING: Fixed: 流域面积
2026-01-14 17:54:06,727:WARNING: Fixed: 116km²
2026-01-14 17:54:06,727:WARNING: Fixed: 水情监测站
2026-01-14 17:54:06,727:WARNING: Fixed: 入库流量
2026-01-14 17:54:06,727:WARNING: Fixed: 库水位数据
2026-01-14 17:54:06,727:WARNING: Fixed: 实时采集
2026-01-14 17:54:06,727:WARNING: Fixed: 县防汛指挥部
2026-01-14 17:54:06,727:WARNING: Fixed: 应急抢险
2026-01-14 17:54:06,727:WARNING: Fixed: 信息报送
2026-01-14 17:54:06,727:WARNING: Fixed: 水库
2026-01-14 17:54:06,727:WARNING: Fixed: 调度运行管理工作
2026-01-14 17:54:06,727:WARNING: Fixed: 100年一遇校核洪水
2026-01-14 17:54:06,727:WARNING: Fixed: 启动特大洪水应急预案
2026-01-14 17:54:06,727:WARNING: Fixed: 省水利厅
2026-01-14 17:54:06,728:WARNING: Fixed: 大坝
2026-01-14 17:54:06,728:WARNING: Fixed: 市三防办
2026-01-14 17:54:06,728:WARNING: Fixed: 报告
2026-01-14 17:54:06,728:WARNING: Fixed: 水位
2026-01-14 17:54:06,728:WARNING: Fixed: 112.3m
2026-01-14 17:54:06,728:WARNING: Fixed: 防灾预警线
2026-01-14 17:54:06,728:WARNING: Fixed: 正常蓄水位
2026-01-14 17:54:06,728:WARNING: Fixed: 控制水位
2026-01-14 17:54:06,728:WARNING: Fixed:  355.12m
2026-01-14 17:54:06,728:WARNING: Fixed: 355.12m
2026-01-14 17:54:06,728:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,728:WARNING: Fixed: 浦江县西水东调管理处
2026-01-14 17:54:06,728:WARNING: Fixed: 浦江县水务局
2026-01-14 17:54:06,728:WARNING: Fixed: 启闭闸门
2026-01-14 17:54:06,728:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,728:WARNING: Fixed: 东江干流
2026-01-14 17:54:06,728:WARNING: Fixed: 枢纽工程
2026-01-14 17:54:06,728:WARNING: Fixed: 非溢流坝
2026-01-14 17:54:06,728:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,728:WARNING: Fixed: 灌溉电站
2026-01-14 17:54:06,729:WARNING: Fixed: 防洪调度指令
2026-01-14 17:54:06,729:WARNING: Fixed: 操作闸门
2026-01-14 17:54:06,729:WARNING: Fixed: 省水利厅
2026-01-14 17:54:06,729:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,729:WARNING: Fixed: 163m
2026-01-14 17:54:06,729:WARNING: Fixed: 15分钟内，要将水库的水位、降雨量等情况报告
2026-01-14 17:54:06,729:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,729:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,729:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,729:WARNING: Fixed: 水库库水位
2026-01-14 17:54:06,729:WARNING: Fixed:  96.39 m
2026-01-14 17:54:06,729:WARNING: Fixed: 100 （mm
2026-01-14 17:54:06,729:WARNING: Fixed: 94.39 m
2026-01-14 17:54:06,729:WARNING: Fixed: 预报降雨
2026-01-14 17:54:06,729:WARNING: Fixed: 库内水位
2026-01-14 17:54:06,729:WARNING: Fixed: 上级主管部门
2026-01-14 17:54:06,729:WARNING: Fixed: 调蓄
2026-01-14 17:54:06,729:WARNING: Fixed: 大坝
2026-01-14 17:54:06,729:WARNING: Fixed: 奉化气象站
2026-01-14 17:54:06,729:WARNING: Fixed: 平均气温
2026-01-14 17:54:06,729:WARNING: Fixed: 极端最高气温
2026-01-14 17:54:06,729:WARNING: Fixed: 极端最低气温
2026-01-14 17:54:06,730:WARNING: Fixed: 16.3℃
2026-01-14 17:54:06,730:WARNING: Fixed: 39.0℃
2026-01-14 17:54:06,730:WARNING: Fixed: -11.1℃
2026-01-14 17:54:06,730:WARNING: Fixed: 库水位
2026-01-14 17:54:06,730:WARNING: Fixed: 62.00m
2026-01-14 17:54:06,730:WARNING: Fixed:  20mm
2026-01-14 17:54:06,730:WARNING: Fixed: 库水位
2026-01-14 17:54:06,730:WARNING: Fixed: 60.68m
2026-01-14 17:54:06,730:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,730:WARNING: Fixed: 水库最大下泄流量
2026-01-14 17:54:06,730:WARNING: Fixed: 水库坝前水位
2026-01-14 17:54:06,730:WARNING: Fixed: 超标准洪水
2026-01-14 17:54:06,730:WARNING: Fixed: 应急指挥机构
2026-01-14 17:54:06,730:WARNING: Fixed: 启动防御超标准洪水预案
2026-01-14 17:54:06,730:WARNING: Fixed: 下泄流量
2026-01-14 17:54:06,730:WARNING: Fixed: 下游城镇防洪标准
2026-01-14 17:54:06,730:WARNING: Fixed: 启动
2026-01-14 17:54:06,730:WARNING: Fixed: 县江
2026-01-14 17:54:06,730:WARNING: Fixed: 横山水库
2026-01-14 17:54:06,730:WARNING: Fixed: 编制相应的调度方案
2026-01-14 17:54:06,731:WARNING: Fixed: 大坝
2026-01-14 17:54:06,731:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,731:WARNING: Fixed: 输水设备
2026-01-14 17:54:06,731:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,731:WARNING: Fixed: 最大泄量
2026-01-14 17:54:06,731:WARNING: Fixed: 1200m³/s
2026-01-14 17:54:06,731:WARNING: Fixed: 敞泄
2026-01-14 17:54:06,731:WARNING: Fixed: 防洪限制水位
2026-01-14 17:54:06,731:WARNING: Fixed: 163m
2026-01-14 17:54:06,731:WARNING: Fixed: 三市镇渡头村
2026-01-14 17:54:06,731:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,731:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,731:WARNING: Fixed: 杭坪镇外胡村
2026-01-14 17:54:06,731:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,731:WARNING: Fixed:  64.73～65.31m
2026-01-14 17:54:06,731:WARNING: Fixed:  50m³/s
2026-01-14 17:54:06,731:WARNING: Fixed: 20～50年一遇洪水位
2026-01-14 17:54:06,731:WARNING: Fixed: 最大下泄流量
2026-01-14 17:54:06,731:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,731:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,731:WARNING: Fixed: 多年平均供水
2026-01-14 17:54:06,732:WARNING: Fixed: 优质水供水量
2026-01-14 17:54:06,732:WARNING: Fixed: 供水保证率
2026-01-14 17:54:06,732:WARNING: Fixed: 2800万m³
2026-01-14 17:54:06,732:WARNING: Fixed: 2272 万 m³/年
2026-01-14 17:54:06,732:WARNING: Fixed: 95%
2026-01-14 17:54:06,732:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,732:WARNING: Fixed: 54.09m
2026-01-14 17:54:06,732:WARNING: Fixed: 2022年底水位
2026-01-14 17:54:06,732:WARNING: Fixed: 防洪高水位
2026-01-14 17:54:06,732:WARNING: Fixed: 校核洪水位
2026-01-14 17:54:06,732:WARNING: Fixed: 正常蓄水位
2026-01-14 17:54:06,732:WARNING: Fixed: 158.20m
2026-01-14 17:54:06,732:WARNING: Fixed: 160.50m
2026-01-14 17:54:06,732:WARNING: Fixed: 155.00m
2026-01-14 17:54:06,732:WARNING: Fixed: 青山水库
2026-01-14 17:54:06,732:WARNING: Fixed: 总库容
2026-01-14 17:54:06,732:WARNING: Fixed: 1200 万m³
2026-01-14 17:54:06,732:WARNING: Fixed: 坝高
2026-01-14 17:54:06,732:WARNING: Fixed: 总库容
2026-01-14 17:54:06,732:WARNING: Fixed: 40m
2026-01-14 17:54:06,732:WARNING: Fixed: 655万m³
2026-01-14 17:54:06,732:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,733:WARNING: Fixed: 科学调蓄
2026-01-14 17:54:06,733:WARNING: Fixed: 补水
2026-01-14 17:54:06,733:WARNING: Fixed: 水库
2026-01-14 17:54:06,733:WARNING: Fixed: 下游河道
2026-01-14 17:54:06,733:WARNING: Fixed: 水库溢洪道
2026-01-14 17:54:06,733:WARNING: Fixed: 水库排洪调度工作
2026-01-14 17:54:06,733:WARNING: Fixed: 库区
2026-01-14 17:54:06,733:WARNING: Fixed: 大坝
2026-01-14 17:54:06,733:WARNING: Fixed: 水政监察大队
2026-01-14 17:54:06,733:WARNING: Fixed: 巡查
2026-01-14 17:54:06,733:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,733:WARNING: Fixed: 报告水库情况并做好记录
2026-01-14 17:54:06,744:INFO: --------admin_train data process DONE!--------
2026-01-14 17:54:06,745:WARNING: Fixed: 宁波市葛岙水库开发有限公司
2026-01-14 17:54:06,745:WARNING: Fixed: 宁波市水利局
2026-01-14 17:54:06,745:WARNING: Fixed: 奉化区水利局
2026-01-14 17:54:06,745:WARNING: Fixed: 实施调度
2026-01-14 17:54:06,745:WARNING: Fixed: 调度
2026-01-14 17:54:06,745:WARNING: Fixed: 洪水调度
2026-01-14 17:54:06,745:WARNING: Fixed: 水位
2026-01-14 17:54:06,745:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,745:WARNING: Fixed: 溢洪道
2026-01-14 17:54:06,745:WARNING: Fixed: 正常水位
2026-01-14 17:54:06,745:WARNING: Fixed: 预计水位
2026-01-14 17:54:06,745:WARNING: Fixed: 163m
2026-01-14 17:54:06,745:WARNING: Fixed: 下游
2026-01-14 17:54:06,745:WARNING: Fixed: 保护耕地
2026-01-14 17:54:06,745:WARNING: Fixed: 5.2 万人
2026-01-14 17:54:06,745:WARNING: Fixed:  3.5 万亩
2026-01-14 17:54:06,745:WARNING: Fixed: 启闭机房
2026-01-14 17:54:06,745:WARNING: Fixed: 发电机组
2026-01-14 17:54:06,745:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,745:WARNING: Fixed: 平均灌溉供水总量
2026-01-14 17:54:06,745:WARNING: Fixed: 年平均生态供水总量
2026-01-14 17:54:06,745:WARNING: Fixed: 鄞江
2026-01-14 17:54:06,746:WARNING: Fixed: 周公宅水库
2026-01-14 17:54:06,746:WARNING: Fixed: 皎口水库
2026-01-14 17:54:06,746:WARNING: Fixed: 河长
2026-01-14 17:54:06,746:WARNING: Fixed: 69km
2026-01-14 17:54:06,746:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,746:WARNING: Fixed: 正常水位
2026-01-14 17:54:06,746:WARNING: Fixed: 20年一遇设计洪水位
2026-01-14 17:54:06,746:WARNING: Fixed: 溢洪水深
2026-01-14 17:54:06,746:WARNING: Fixed: 163m
2026-01-14 17:54:06,746:WARNING: Fixed: 165.20m
2026-01-14 17:54:06,746:WARNING: Fixed: 2.20m
2026-01-14 17:54:06,746:WARNING: Fixed: 水库防汛工作
2026-01-14 17:54:06,746:WARNING: Fixed: 址山镇三防指挥所
2026-01-14 17:54:06,746:WARNING: Fixed: 库水位
2026-01-14 17:54:06,746:WARNING: Fixed:  62.99m 
2026-01-14 17:54:06,746:WARNING: Fixed: 64.73m
2026-01-14 17:54:06,746:WARNING: Fixed: 水库关闸蓄洪
2026-01-14 17:54:06,746:WARNING: Fixed: 库水位
2026-01-14 17:54:06,746:WARNING: Fixed: 64.73m
2026-01-14 17:54:06,746:WARNING: Fixed: 65.31m
2026-01-14 17:54:06,746:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,746:WARNING: Fixed: 水库控制最大下泄流量
2026-01-14 17:54:06,746:WARNING: Fixed: 水库
2026-01-14 17:54:06,747:WARNING: Fixed: 敞泄
2026-01-14 17:54:06,747:WARNING: Fixed: 最大下泄流量
2026-01-14 17:54:06,747:WARNING: Fixed: 设计洪水的洪峰流量
2026-01-14 17:54:06,747:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,747:WARNING: Fixed: 关闸蓄洪
2026-01-14 17:54:06,747:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,747:WARNING: Fixed: 163m
2026-01-14 17:54:06,747:WARNING: Fixed: 流域累计面雨量
2026-01-14 17:54:06,747:WARNING: Fixed: 瞬时流量
2026-01-14 17:54:06,747:WARNING: Fixed: 100mm
2026-01-14 17:54:06,747:WARNING: Fixed: 100m³/s
2026-01-14 17:54:06,747:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,747:WARNING: Fixed: 60.48m
2026-01-14 17:54:06,747:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,747:WARNING: Fixed: 水库最大下泄流量
2026-01-14 17:54:06,747:WARNING: Fixed: 防洪高水位线
2026-01-14 17:54:06,747:WARNING: Fixed: 防洪高水位线
2026-01-14 17:54:06,747:WARNING: Fixed: 校核洪水位线
2026-01-14 17:54:06,747:WARNING: Fixed: 防洪限制水位线
2026-01-14 17:54:06,747:WARNING: Fixed: 水库防洪调度区
2026-01-14 17:54:06,747:WARNING: Fixed: 下游防洪调度区
2026-01-14 17:54:06,747:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,747:WARNING: Fixed: 校核洪水位
2026-01-14 17:54:06,747:WARNING: Fixed: 设计洪水位
2026-01-14 17:54:06,748:WARNING: Fixed:  357.52m
2026-01-14 17:54:06,748:WARNING: Fixed: 356.86m
2026-01-14 17:54:06,748:WARNING: Fixed: 排洪闸
2026-01-14 17:54:06,748:WARNING: Fixed: 开度及下泄流量
2026-01-14 17:54:06,748:WARNING: Fixed: 上报
2026-01-14 17:54:06,748:WARNING: Fixed: 镇三防办
2026-01-14 17:54:06,748:WARNING: Fixed: 编制
2026-01-14 17:54:06,748:WARNING: Fixed: 奉化区水利局
2026-01-14 17:54:06,748:WARNING: Fixed: 宁波市水利局
2026-01-14 17:54:06,748:WARNING: Fixed: 奉化江
2026-01-14 17:54:06,748:WARNING: Fixed: 海拔
2026-01-14 17:54:06,748:WARNING: Fixed: 976m
2026-01-14 17:54:06,748:WARNING: Fixed: 限制水位
2026-01-14 17:54:06,748:WARNING: Fixed: 限制水位
2026-01-14 17:54:06,748:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,748:WARNING: Fixed: 60.00m
2026-01-14 17:54:06,748:WARNING: Fixed: 库水位
2026-01-14 17:54:06,748:WARNING: Fixed: 下泄
2026-01-14 17:54:06,748:WARNING: Fixed: 剡江
2026-01-14 17:54:06,748:WARNING: Fixed: 亭下水库
2026-01-14 17:54:06,748:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,748:WARNING: Fixed: 163m
2026-01-14 17:54:06,748:WARNING: Fixed: 防洪调度
2026-01-14 17:54:06,749:WARNING: Fixed: 大坝
2026-01-14 17:54:06,749:WARNING: Fixed: 汛前水位
2026-01-14 17:54:06,749:WARNING: Fixed: 正常水位
2026-01-14 17:54:06,749:WARNING: Fixed: 163m
2026-01-14 17:54:06,749:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,749:WARNING: Fixed: 外胡水库
2026-01-14 17:54:06,749:WARNING: Fixed: 金坑岭水库
2026-01-14 17:54:06,749:WARNING: Fixed: 浦江县电网
2026-01-14 17:54:06,749:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,749:WARNING: Fixed: 62.99m
2026-01-14 17:54:06,749:WARNING: Fixed: 300m³/s 
2026-01-14 17:54:06,749:WARNING: Fixed: 50m³/s
2026-01-14 17:54:06,749:WARNING: Fixed: 下游河道下王控制断面安全泄量
2026-01-14 17:54:06,749:WARNING: Fixed: 最大下泄流量
2026-01-14 17:54:06,749:WARNING: Fixed: 渡头村
2026-01-14 17:54:06,749:WARNING: Fixed: 斗岭水库
2026-01-14 17:54:06,749:WARNING: Fixed: 执行
2026-01-14 17:54:06,749:WARNING: Fixed: 葛岙水库
2026-01-14 17:54:06,749:WARNING: Fixed: 水库
2026-01-14 17:54:06,749:WARNING: Fixed: 控制蓄水位
2026-01-14 17:54:06,749:WARNING: Fixed:  60.00m
2026-01-14 17:54:06,750:WARNING: Fixed: 水库水位
2026-01-14 17:54:06,750:WARNING: Fixed: 88.0m
2026-01-14 17:54:06,750:WARNING: Fixed: 警戒水位
2026-01-14 17:54:06,750:WARNING: Fixed: 防汛抢险队
2026-01-14 17:54:06,750:WARNING: Fixed: 坝高
2026-01-14 17:54:06,750:WARNING: Fixed: 坝顶高程
2026-01-14 17:54:06,750:WARNING: Fixed: 坝顶轴线长
2026-01-14 17:54:06,750:WARNING: Fixed: 坝顶宽
2026-01-14 17:54:06,750:WARNING: Fixed: 8.6 m
2026-01-14 17:54:06,750:WARNING: Fixed: 99.08 m
2026-01-14 17:54:06,750:WARNING: Fixed: 139 m
2026-01-14 17:54:06,750:WARNING: Fixed: 6.00m
2026-01-14 17:54:06,750:WARNING: Fixed: 东江
2026-01-14 17:54:06,750:WARNING: Fixed: 东江
2026-01-14 17:54:06,750:WARNING: Fixed: 东江干流
2026-01-14 17:54:06,750:WARNING: Fixed: 镇三防指挥所
2026-01-14 17:54:06,750:WARNING: Fixed: 防汛机构
2026-01-14 17:54:06,750:WARNING: Fixed: 输水涵管
2026-01-14 17:54:06,750:WARNING: Fixed: 启闭设备
2026-01-14 17:54:06,750:WARNING: Fixed: 维修养护
2026-01-14 17:54:06,750:WARNING: Fixed: 大坝
2026-01-14 17:54:06,750:WARNING: Fixed: 撤离预警
2026-01-14 17:54:06,750:WARNING: Fixed: 市三防指挥所
2026-01-14 17:54:06,750:WARNING: Fixed: 水库
2026-01-14 17:54:06,751:WARNING: Fixed: 监测
2026-01-14 17:54:06,751:WARNING: Fixed: 入库流量
2026-01-14 17:54:06,751:WARNING: Fixed: 500m³/s
2026-01-14 17:54:06,751:WARNING: Fixed: 市应急管理局
2026-01-14 17:54:06,754:INFO: --------admin_test data process DONE!--------
2026-01-14 17:54:06,754:INFO: --------Process Done!--------
2026-01-14 17:54:06,761:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:54:06,761:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:54:06,761:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:54:06,761:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:54:06,761:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:54:06,761:INFO: loading file None
2026-01-14 17:54:06,761:INFO: loading file None
2026-01-14 17:54:06,761:INFO: loading file None
2026-01-14 17:54:07,099:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:54:07,099:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:54:07,099:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:54:07,099:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:54:07,100:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:54:07,100:INFO: loading file None
2026-01-14 17:54:07,100:INFO: loading file None
2026-01-14 17:54:07,100:INFO: loading file None
2026-01-14 17:54:07,146:INFO: --------Dataset Build!--------
2026-01-14 17:54:07,146:INFO: --------Get Dataloader!--------
2026-01-14 17:54:07,146:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-14 17:54:07,146:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-14 17:54:07,146:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-14 17:54:15,441:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-14 17:54:15,441:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-14 17:54:16,972:INFO: --------Start Training!--------
2026-01-14 17:54:41,214:INFO: device: cuda:0
2026-01-14 17:54:41,214:INFO: --------Process Done!--------
2026-01-14 17:54:41,221:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:54:41,221:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:54:41,221:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:54:41,221:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:54:41,222:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:54:41,222:INFO: loading file None
2026-01-14 17:54:41,222:INFO: loading file None
2026-01-14 17:54:41,222:INFO: loading file None
2026-01-14 17:54:41,567:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:54:41,567:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:54:41,568:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:54:41,568:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:54:41,568:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:54:41,568:INFO: loading file None
2026-01-14 17:54:41,568:INFO: loading file None
2026-01-14 17:54:41,568:INFO: loading file None
2026-01-14 17:54:41,615:INFO: --------Dataset Build!--------
2026-01-14 17:54:41,616:INFO: --------Get Dataloader!--------
2026-01-14 17:54:41,616:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-14 17:54:41,616:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-14 17:54:41,616:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-14 17:54:48,504:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-14 17:54:48,504:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-14 17:54:49,996:INFO: --------Start Training!--------
2026-01-14 17:54:55,727:INFO: Epoch: 1, train loss: 646.7825578962054
2026-01-14 17:54:55,957:INFO: Epoch: 1, dev loss: 169.4457950592041, f1 score: 0.20192307692307693
2026-01-14 17:54:55,958:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:54:57,755:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:54:57,755:INFO: --------Save best model!--------
2026-01-14 17:55:02,409:INFO: Epoch: 2, train loss: 189.56388064793177
2026-01-14 17:55:02,649:INFO: Epoch: 2, dev loss: 81.36262512207031, f1 score: 0.5821596244131455
2026-01-14 17:55:02,650:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:55:07,465:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:55:07,465:INFO: --------Save best model!--------
2026-01-14 17:55:12,054:INFO: Epoch: 3, train loss: 98.3636109488351
2026-01-14 17:55:12,282:INFO: Epoch: 3, dev loss: 97.02805709838867, f1 score: 0.5632653061224488
2026-01-14 17:55:16,837:INFO: Epoch: 4, train loss: 68.4154908316476
2026-01-14 17:55:17,065:INFO: Epoch: 4, dev loss: 122.73533248901367, f1 score: 0.5608856088560886
2026-01-14 17:55:21,593:INFO: Epoch: 5, train loss: 61.49203736441476
2026-01-14 17:55:21,824:INFO: Epoch: 5, dev loss: 95.63207244873047, f1 score: 0.631578947368421
2026-01-14 17:55:21,825:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:55:26,517:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:55:26,517:INFO: --------Save best model!--------
2026-01-14 17:55:31,126:INFO: Epoch: 6, train loss: 61.32385989597866
2026-01-14 17:55:31,356:INFO: Epoch: 6, dev loss: 114.27766799926758, f1 score: 0.5637065637065637
2026-01-14 17:55:35,905:INFO: Epoch: 7, train loss: 48.52794592721121
2026-01-14 17:55:36,126:INFO: Epoch: 7, dev loss: 102.89844512939453, f1 score: 0.6470588235294118
2026-01-14 17:55:36,127:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:55:39,655:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:55:39,655:INFO: --------Save best model!--------
2026-01-14 17:55:44,174:INFO: Epoch: 8, train loss: 35.158312933785574
2026-01-14 17:55:44,395:INFO: Epoch: 8, dev loss: 134.4620246887207, f1 score: 0.6057692307692307
2026-01-14 17:55:48,786:INFO: Epoch: 9, train loss: 29.288076673235214
2026-01-14 17:55:49,013:INFO: Epoch: 9, dev loss: 121.28433609008789, f1 score: 0.6495726495726495
2026-01-14 17:55:49,014:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:55:53,881:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:55:53,881:INFO: --------Save best model!--------
2026-01-14 17:55:58,203:INFO: Epoch: 10, train loss: 21.32053347996303
2026-01-14 17:55:58,422:INFO: Epoch: 10, dev loss: 85.91369247436523, f1 score: 0.6751054852320676
2026-01-14 17:55:58,423:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:56:03,145:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:56:03,146:INFO: --------Save best model!--------
2026-01-14 17:56:07,468:INFO: Epoch: 11, train loss: 19.731780460902623
2026-01-14 17:56:07,694:INFO: Epoch: 11, dev loss: 145.67163848876953, f1 score: 0.658008658008658
2026-01-14 17:56:12,091:INFO: Epoch: 12, train loss: 19.91665131705148
2026-01-14 17:56:12,319:INFO: Epoch: 12, dev loss: 179.70476531982422, f1 score: 0.6015625
2026-01-14 17:56:16,706:INFO: Epoch: 13, train loss: 16.490568705967494
2026-01-14 17:56:16,935:INFO: Epoch: 13, dev loss: 153.3507843017578, f1 score: 0.6804979253112033
2026-01-14 17:56:16,936:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:56:21,565:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:56:21,565:INFO: --------Save best model!--------
2026-01-14 17:56:25,928:INFO: Epoch: 14, train loss: 15.179930278233119
2026-01-14 17:56:26,150:INFO: Epoch: 14, dev loss: 119.86022186279297, f1 score: 0.7004608294930875
2026-01-14 17:56:26,151:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:56:30,849:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:56:30,850:INFO: --------Save best model!--------
2026-01-14 17:56:35,361:INFO: Epoch: 15, train loss: 8.212691170828682
2026-01-14 17:56:35,590:INFO: Epoch: 15, dev loss: 179.9184455871582, f1 score: 0.6359832635983264
2026-01-14 17:56:40,457:INFO: Epoch: 16, train loss: 11.014314379010882
2026-01-14 17:56:40,675:INFO: Epoch: 16, dev loss: 142.77293014526367, f1 score: 0.7238095238095238
2026-01-14 17:56:40,676:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:56:45,410:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:56:45,410:INFO: --------Save best model!--------
2026-01-14 17:56:49,989:INFO: Epoch: 17, train loss: 7.7471049172537665
2026-01-14 17:56:50,205:INFO: Epoch: 17, dev loss: 187.02506256103516, f1 score: 0.6666666666666666
2026-01-14 17:56:54,664:INFO: Epoch: 18, train loss: 8.737783976963588
2026-01-14 17:56:54,889:INFO: Epoch: 18, dev loss: 154.72409057617188, f1 score: 0.6812227074235807
2026-01-14 17:56:59,385:INFO: Epoch: 19, train loss: 7.692071642194476
2026-01-14 17:56:59,616:INFO: Epoch: 19, dev loss: 184.5287971496582, f1 score: 0.6872246696035241
2026-01-14 17:57:04,131:INFO: Epoch: 20, train loss: 10.294184003557477
2026-01-14 17:57:04,345:INFO: Epoch: 20, dev loss: 176.68963241577148, f1 score: 0.6727272727272728
2026-01-14 17:57:08,726:INFO: Epoch: 21, train loss: 3.980114528111049
2026-01-14 17:57:08,941:INFO: Epoch: 21, dev loss: 179.15911865234375, f1 score: 0.6666666666666666
2026-01-14 17:57:13,305:INFO: Epoch: 22, train loss: 2.767991747174944
2026-01-14 17:57:13,516:INFO: Epoch: 22, dev loss: 187.15331649780273, f1 score: 0.6604651162790699
2026-01-14 17:57:17,945:INFO: Epoch: 23, train loss: 2.4409566606794084
2026-01-14 17:57:18,179:INFO: Epoch: 23, dev loss: 177.9774932861328, f1 score: 0.6547085201793722
2026-01-14 17:57:22,485:INFO: Epoch: 24, train loss: 9.200949805123466
2026-01-14 17:57:22,697:INFO: Epoch: 24, dev loss: 197.59503173828125, f1 score: 0.6905829596412556
2026-01-14 17:57:26,997:INFO: Epoch: 25, train loss: 5.177800859723773
2026-01-14 17:57:27,219:INFO: Epoch: 25, dev loss: 198.40696716308594, f1 score: 0.6724890829694323
2026-01-14 17:57:31,606:INFO: Epoch: 26, train loss: 2.232403891427176
2026-01-14 17:57:31,823:INFO: Epoch: 26, dev loss: 190.99749755859375, f1 score: 0.696035242290749
2026-01-14 17:57:31,824:INFO: Best val f1: 0.7238095238095238
2026-01-14 17:57:31,824:INFO: Training Finished!
2026-01-14 17:57:31,830:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:57:31,831:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:57:31,831:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:57:31,831:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:57:31,831:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:57:31,831:INFO: loading file None
2026-01-14 17:57:31,831:INFO: loading file None
2026-01-14 17:57:31,831:INFO: loading file None
2026-01-14 17:57:31,929:INFO: --------Dataset Build!--------
2026-01-14 17:57:31,929:INFO: --------Get Data-loader!--------
2026-01-14 17:57:31,929:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-14 17:57:31,929:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-14 17:57:31,929:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-14 17:57:38,810:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-14 17:57:38,812:INFO: Model name 'pretrained_bert_models/bert-base-chinese/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/bert-base-chinese/' is a path or url to a directory containing tokenizer files.
2026-01-14 17:57:38,812:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/added_tokens.json. We won't load it.
2026-01-14 17:57:38,812:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/special_tokens_map.json. We won't load it.
2026-01-14 17:57:38,812:INFO: Didn't find file pretrained_bert_models/bert-base-chinese/tokenizer_config.json. We won't load it.
2026-01-14 17:57:38,812:INFO: loading file pretrained_bert_models/bert-base-chinese/vocab.txt
2026-01-14 17:57:38,812:INFO: loading file None
2026-01-14 17:57:38,812:INFO: loading file None
2026-01-14 17:57:38,812:INFO: loading file None
2026-01-14 17:57:39,488:INFO: --------Bad Cases reserved !--------
2026-01-14 17:57:39,494:INFO: test loss: 281.22524070739746, f1 score: 0.7465940054495912
2026-01-14 17:57:39,494:INFO: f1 score of ORG: 0.8297872340425533
2026-01-14 17:57:39,494:INFO: f1 score of ACTION: 0.6280991735537191
2026-01-14 17:57:39,494:INFO: f1 score of OBJ: 0.736318407960199
2026-01-14 17:57:39,494:INFO: f1 score of LEVEL_KEY: 0.7407407407407408
2026-01-14 17:57:39,494:INFO: f1 score of VALUE: 0.8076923076923076
2026-01-18 17:39:00,051:INFO: device: cuda:0
2026-01-18 17:39:00,051:INFO: --------Process Done!--------
2026-01-18 17:39:00,060:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:39:00,060:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:39:00,060:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:39:00,060:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:39:00,060:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:39:00,060:INFO: loading file None
2026-01-18 17:39:00,060:INFO: loading file None
2026-01-18 17:39:00,060:INFO: loading file None
2026-01-18 17:39:00,417:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:39:00,417:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:39:00,417:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:39:00,417:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:39:00,417:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:39:00,417:INFO: loading file None
2026-01-18 17:39:00,417:INFO: loading file None
2026-01-18 17:39:00,417:INFO: loading file None
2026-01-18 17:39:00,463:INFO: --------Dataset Build!--------
2026-01-18 17:39:00,464:INFO: --------Get Dataloader!--------
2026-01-18 17:39:00,464:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-18 17:39:00,464:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:39:00,465:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-18 17:39:08,739:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-18 17:39:08,740:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-18 17:39:10,725:INFO: --------Start Training!--------
2026-01-18 17:39:20,278:INFO: Epoch: 1, train loss: 617.1480015345982
2026-01-18 17:39:20,503:INFO: Epoch: 1, dev loss: 145.39534759521484, f1 score: 0.1149425287356322
2026-01-18 17:39:20,504:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:39:24,998:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:39:24,998:INFO: --------Save best model!--------
2026-01-18 17:39:32,956:INFO: Epoch: 2, train loss: 201.85212789263045
2026-01-18 17:39:33,179:INFO: Epoch: 2, dev loss: 91.15781021118164, f1 score: 0.5676855895196506
2026-01-18 17:39:33,180:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:39:37,991:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:39:37,991:INFO: --------Save best model!--------
2026-01-18 17:39:45,808:INFO: Epoch: 3, train loss: 109.5152290889195
2026-01-18 17:39:46,034:INFO: Epoch: 3, dev loss: 91.29524993896484, f1 score: 0.5619834710743802
2026-01-18 17:39:53,746:INFO: Epoch: 4, train loss: 67.34625516619
2026-01-18 17:39:53,960:INFO: Epoch: 4, dev loss: 82.18539428710938, f1 score: 0.5844748858447489
2026-01-18 17:39:53,961:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:39:58,634:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:39:58,634:INFO: --------Save best model!--------
2026-01-18 17:40:06,538:INFO: Epoch: 5, train loss: 55.9207444872175
2026-01-18 17:40:06,752:INFO: Epoch: 5, dev loss: 117.05407333374023, f1 score: 0.6262626262626262
2026-01-18 17:40:06,753:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:40:11,440:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:40:11,441:INFO: --------Save best model!--------
2026-01-18 17:40:19,079:INFO: Epoch: 6, train loss: 44.37576075962612
2026-01-18 17:40:19,290:INFO: Epoch: 6, dev loss: 105.46611404418945, f1 score: 0.6
2026-01-18 17:40:26,801:INFO: Epoch: 7, train loss: 33.18903160095215
2026-01-18 17:40:27,021:INFO: Epoch: 7, dev loss: 103.53939437866211, f1 score: 0.5843621399176955
2026-01-18 17:40:34,994:INFO: Epoch: 8, train loss: 32.43854086739676
2026-01-18 17:40:35,213:INFO: Epoch: 8, dev loss: 113.40798568725586, f1 score: 0.6320754716981132
2026-01-18 17:40:35,214:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:40:39,848:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:40:39,848:INFO: --------Save best model!--------
2026-01-18 17:40:47,705:INFO: Epoch: 9, train loss: 28.11294228690011
2026-01-18 17:40:47,934:INFO: Epoch: 9, dev loss: 101.47186279296875, f1 score: 0.6542056074766355
2026-01-18 17:40:47,935:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:40:52,552:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:40:52,552:INFO: --------Save best model!--------
2026-01-18 17:41:00,492:INFO: Epoch: 10, train loss: 20.465210233415878
2026-01-18 17:41:00,697:INFO: Epoch: 10, dev loss: 126.31912994384766, f1 score: 0.6636771300448431
2026-01-18 17:41:00,698:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:41:05,315:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:41:05,316:INFO: --------Save best model!--------
2026-01-18 17:41:12,918:INFO: Epoch: 11, train loss: 19.513140269688197
2026-01-18 17:41:13,139:INFO: Epoch: 11, dev loss: 117.25006866455078, f1 score: 0.6551724137931033
2026-01-18 17:41:20,679:INFO: Epoch: 12, train loss: 12.493595940726143
2026-01-18 17:41:20,875:INFO: Epoch: 12, dev loss: 125.76232147216797, f1 score: 0.6297872340425532
2026-01-18 17:41:28,580:INFO: Epoch: 13, train loss: 12.170937674386161
2026-01-18 17:41:28,784:INFO: Epoch: 13, dev loss: 123.85822677612305, f1 score: 0.6551724137931033
2026-01-18 17:41:36,574:INFO: Epoch: 14, train loss: 10.830782754080635
2026-01-18 17:41:36,796:INFO: Epoch: 14, dev loss: 136.5350456237793, f1 score: 0.6122448979591836
2026-01-18 17:41:44,757:INFO: Epoch: 15, train loss: 8.062136786324638
2026-01-18 17:41:44,983:INFO: Epoch: 15, dev loss: 109.19822692871094, f1 score: 0.6926406926406926
2026-01-18 17:41:44,984:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:41:49,597:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:41:49,597:INFO: --------Save best model!--------
2026-01-18 17:41:57,196:INFO: Epoch: 16, train loss: 8.484248570033483
2026-01-18 17:41:57,409:INFO: Epoch: 16, dev loss: 146.83851623535156, f1 score: 0.654867256637168
2026-01-18 17:42:04,914:INFO: Epoch: 17, train loss: 5.273874827793667
2026-01-18 17:42:05,119:INFO: Epoch: 17, dev loss: 176.76283645629883, f1 score: 0.6260162601626017
2026-01-18 17:42:12,734:INFO: Epoch: 18, train loss: 5.358586720057896
2026-01-18 17:42:12,949:INFO: Epoch: 18, dev loss: 163.2153549194336, f1 score: 0.6638297872340426
2026-01-18 17:42:20,568:INFO: Epoch: 19, train loss: 6.479616437639509
2026-01-18 17:42:20,788:INFO: Epoch: 19, dev loss: 127.84053421020508, f1 score: 0.7109004739336492
2026-01-18 17:42:20,789:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:42:25,423:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:42:25,423:INFO: --------Save best model!--------
2026-01-18 17:42:33,016:INFO: Epoch: 20, train loss: 3.725864682878767
2026-01-18 17:42:33,222:INFO: Epoch: 20, dev loss: 158.18216705322266, f1 score: 0.6835443037974683
2026-01-18 17:42:40,925:INFO: Epoch: 21, train loss: 4.806171962193081
2026-01-18 17:42:41,149:INFO: Epoch: 21, dev loss: 148.72750091552734, f1 score: 0.6697674418604651
2026-01-18 17:42:48,616:INFO: Epoch: 22, train loss: 3.965778078351702
2026-01-18 17:42:48,828:INFO: Epoch: 22, dev loss: 160.52693939208984, f1 score: 0.6550218340611353
2026-01-18 17:42:56,583:INFO: Epoch: 23, train loss: 1.2776647295270647
2026-01-18 17:42:56,799:INFO: Epoch: 23, dev loss: 185.06654357910156, f1 score: 0.6844444444444445
2026-01-18 17:43:04,732:INFO: Epoch: 24, train loss: 1.0065926143101283
2026-01-18 17:43:04,946:INFO: Epoch: 24, dev loss: 197.16293716430664, f1 score: 0.6637554585152838
2026-01-18 17:43:12,723:INFO: Epoch: 25, train loss: 1.3260958535330636
2026-01-18 17:43:12,942:INFO: Epoch: 25, dev loss: 203.16637420654297, f1 score: 0.6694560669456068
2026-01-18 17:43:20,575:INFO: Epoch: 26, train loss: 1.2315526689801897
2026-01-18 17:43:20,783:INFO: Epoch: 26, dev loss: 194.05678176879883, f1 score: 0.6754385964912281
2026-01-18 17:43:28,354:INFO: Epoch: 27, train loss: 1.4512639726911272
2026-01-18 17:43:28,579:INFO: Epoch: 27, dev loss: 189.71699142456055, f1 score: 0.696035242290749
2026-01-18 17:43:36,055:INFO: Epoch: 28, train loss: 0.6341040475027901
2026-01-18 17:43:36,256:INFO: Epoch: 28, dev loss: 178.42288970947266, f1 score: 0.6986899563318777
2026-01-18 17:43:44,030:INFO: Epoch: 29, train loss: 0.37264578683035715
2026-01-18 17:43:44,228:INFO: Epoch: 29, dev loss: 189.90108489990234, f1 score: 0.6726457399103138
2026-01-18 17:43:44,228:INFO: Best val f1: 0.7109004739336492
2026-01-18 17:43:44,229:INFO: Training Finished!
2026-01-18 17:43:44,236:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:43:44,237:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:43:44,237:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:43:44,237:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:43:44,237:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:43:44,237:INFO: loading file None
2026-01-18 17:43:44,237:INFO: loading file None
2026-01-18 17:43:44,237:INFO: loading file None
2026-01-18 17:43:44,337:INFO: --------Dataset Build!--------
2026-01-18 17:43:44,337:INFO: --------Get Data-loader!--------
2026-01-18 17:43:44,337:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:43:44,338:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:43:44,338:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:43:51,045:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-18 17:43:51,047:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:43:51,047:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:43:51,047:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:43:51,047:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:43:51,047:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:43:51,048:INFO: loading file None
2026-01-18 17:43:51,048:INFO: loading file None
2026-01-18 17:43:51,048:INFO: loading file None
2026-01-18 17:43:51,710:INFO: --------Bad Cases reserved !--------
2026-01-18 17:43:51,716:INFO: test loss: 240.9227352142334, f1 score: 0.7194630872483221
2026-01-18 17:43:51,717:INFO: f1 score of ORG: 0.7741935483870969
2026-01-18 17:43:51,717:INFO: f1 score of ACTION: 0.6268656716417911
2026-01-18 17:43:51,717:INFO: f1 score of OBJ: 0.6990291262135924
2026-01-18 17:43:51,717:INFO: f1 score of LEVEL_KEY: 0.6375
2026-01-18 17:43:51,717:INFO: f1 score of VALUE: 0.881578947368421
2026-01-18 17:47:52,316:INFO: device: cuda:0
2026-01-18 17:47:52,316:INFO: --------Process Done!--------
2026-01-18 17:47:52,323:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:47:52,323:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:47:52,323:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:47:52,323:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:47:52,323:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:47:52,323:INFO: loading file None
2026-01-18 17:47:52,323:INFO: loading file None
2026-01-18 17:47:52,323:INFO: loading file None
2026-01-18 17:47:52,671:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:47:52,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:47:52,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:47:52,671:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:47:52,672:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:47:52,672:INFO: loading file None
2026-01-18 17:47:52,672:INFO: loading file None
2026-01-18 17:47:52,672:INFO: loading file None
2026-01-18 17:47:52,719:INFO: --------Dataset Build!--------
2026-01-18 17:47:52,719:INFO: --------Get Dataloader!--------
2026-01-18 17:47:52,719:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-18 17:47:52,719:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:47:52,720:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-18 17:48:00,258:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-18 17:48:00,258:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-18 17:48:01,773:INFO: --------Start Training!--------
2026-01-18 17:48:10,816:INFO: Epoch: 1, train loss: 742.49021257673
2026-01-18 17:48:11,039:INFO: Epoch: 1, dev loss: 228.76343154907227, f1 score: 0.05217391304347826
2026-01-18 17:48:11,041:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:48:15,700:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:48:15,701:INFO: --------Save best model!--------
2026-01-18 17:48:23,511:INFO: Epoch: 2, train loss: 273.4057535443987
2026-01-18 17:48:23,723:INFO: Epoch: 2, dev loss: 103.56439590454102, f1 score: 0.41666666666666663
2026-01-18 17:48:23,724:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:48:28,330:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:48:28,330:INFO: --------Save best model!--------
2026-01-18 17:48:36,095:INFO: Epoch: 3, train loss: 131.77666228158134
2026-01-18 17:48:36,312:INFO: Epoch: 3, dev loss: 90.038330078125, f1 score: 0.5811965811965812
2026-01-18 17:48:36,313:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:48:40,960:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:48:40,960:INFO: --------Save best model!--------
2026-01-18 17:48:48,777:INFO: Epoch: 4, train loss: 73.31344332013812
2026-01-18 17:48:49,008:INFO: Epoch: 4, dev loss: 88.59400177001953, f1 score: 0.5829959514170041
2026-01-18 17:48:49,009:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:48:53,644:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:48:53,645:INFO: --------Save best model!--------
2026-01-18 17:49:01,288:INFO: Epoch: 5, train loss: 53.291666575840544
2026-01-18 17:49:01,512:INFO: Epoch: 5, dev loss: 92.70980072021484, f1 score: 0.6218487394957983
2026-01-18 17:49:01,512:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:49:06,103:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:49:06,103:INFO: --------Save best model!--------
2026-01-18 17:49:13,901:INFO: Epoch: 6, train loss: 31.023247855050222
2026-01-18 17:49:14,117:INFO: Epoch: 6, dev loss: 85.7257194519043, f1 score: 0.6849315068493151
2026-01-18 17:49:14,118:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:49:18,717:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:49:18,717:INFO: --------Save best model!--------
2026-01-18 17:49:26,777:INFO: Epoch: 7, train loss: 23.090769359043666
2026-01-18 17:49:27,001:INFO: Epoch: 7, dev loss: 88.07955551147461, f1 score: 0.632034632034632
2026-01-18 17:49:34,827:INFO: Epoch: 8, train loss: 13.74982179914202
2026-01-18 17:49:35,050:INFO: Epoch: 8, dev loss: 109.39178848266602, f1 score: 0.6757990867579908
2026-01-18 17:49:42,724:INFO: Epoch: 9, train loss: 15.219475609915596
2026-01-18 17:49:42,944:INFO: Epoch: 9, dev loss: 96.32808303833008, f1 score: 0.6695278969957081
2026-01-18 17:49:50,834:INFO: Epoch: 10, train loss: 12.84532710484096
2026-01-18 17:49:51,044:INFO: Epoch: 10, dev loss: 114.09197998046875, f1 score: 0.6666666666666666
2026-01-18 17:49:58,767:INFO: Epoch: 11, train loss: 7.60351916721889
2026-01-18 17:49:58,990:INFO: Epoch: 11, dev loss: 148.09412002563477, f1 score: 0.6260869565217392
2026-01-18 17:50:06,632:INFO: Epoch: 12, train loss: 5.08262825012207
2026-01-18 17:50:06,849:INFO: Epoch: 12, dev loss: 143.20857620239258, f1 score: 0.6972477064220183
2026-01-18 17:50:06,850:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:50:11,471:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:50:11,471:INFO: --------Save best model!--------
2026-01-18 17:50:19,041:INFO: Epoch: 13, train loss: 3.956185749598912
2026-01-18 17:50:19,254:INFO: Epoch: 13, dev loss: 158.55250549316406, f1 score: 0.6515837104072398
2026-01-18 17:50:26,947:INFO: Epoch: 14, train loss: 2.5805323464529857
2026-01-18 17:50:27,152:INFO: Epoch: 14, dev loss: 188.34030151367188, f1 score: 0.6297872340425532
2026-01-18 17:50:34,924:INFO: Epoch: 15, train loss: 2.554379871913365
2026-01-18 17:50:35,130:INFO: Epoch: 15, dev loss: 177.66620635986328, f1 score: 0.6636771300448431
2026-01-18 17:50:42,622:INFO: Epoch: 16, train loss: 2.7334741864885603
2026-01-18 17:50:42,847:INFO: Epoch: 16, dev loss: 196.44707107543945, f1 score: 0.6283185840707964
2026-01-18 17:50:50,603:INFO: Epoch: 17, train loss: 2.3672101157052174
2026-01-18 17:50:50,834:INFO: Epoch: 17, dev loss: 210.21737670898438, f1 score: 0.6206896551724138
2026-01-18 17:50:58,743:INFO: Epoch: 18, train loss: 2.4034756251743863
2026-01-18 17:50:58,948:INFO: Epoch: 18, dev loss: 201.63333892822266, f1 score: 0.658008658008658
2026-01-18 17:51:06,794:INFO: Epoch: 19, train loss: 2.024920872279576
2026-01-18 17:51:07,012:INFO: Epoch: 19, dev loss: 203.37086486816406, f1 score: 0.6122448979591836
2026-01-18 17:51:14,693:INFO: Epoch: 20, train loss: 0.9272071293422154
2026-01-18 17:51:14,921:INFO: Epoch: 20, dev loss: 218.58354949951172, f1 score: 0.6488888888888888
2026-01-18 17:51:22,709:INFO: Epoch: 21, train loss: 0.8454230172293526
2026-01-18 17:51:22,938:INFO: Epoch: 21, dev loss: 239.6208152770996, f1 score: 0.628099173553719
2026-01-18 17:51:30,717:INFO: Epoch: 22, train loss: 1.0380807604108537
2026-01-18 17:51:30,941:INFO: Epoch: 22, dev loss: 224.70796585083008, f1 score: 0.6371681415929203
2026-01-18 17:51:30,941:INFO: Best val f1: 0.6972477064220183
2026-01-18 17:51:30,941:INFO: Training Finished!
2026-01-18 17:51:30,948:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:51:30,948:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:51:30,948:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:51:30,948:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:51:30,949:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:51:30,949:INFO: loading file None
2026-01-18 17:51:30,949:INFO: loading file None
2026-01-18 17:51:30,949:INFO: loading file None
2026-01-18 17:51:31,052:INFO: --------Dataset Build!--------
2026-01-18 17:51:31,053:INFO: --------Get Data-loader!--------
2026-01-18 17:51:31,053:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:51:31,053:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:51:31,053:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:51:37,770:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-18 17:51:37,771:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:51:37,772:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:51:37,772:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:51:37,772:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:51:37,772:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:51:37,772:INFO: loading file None
2026-01-18 17:51:37,772:INFO: loading file None
2026-01-18 17:51:37,772:INFO: loading file None
2026-01-18 17:51:38,437:INFO: --------Bad Cases reserved !--------
2026-01-18 17:51:38,444:INFO: test loss: 265.71300506591797, f1 score: 0.6944818304172274
2026-01-18 17:51:38,444:INFO: f1 score of ORG: 0.7755102040816326
2026-01-18 17:51:38,444:INFO: f1 score of ACTION: 0.5891472868217055
2026-01-18 17:51:38,444:INFO: f1 score of OBJ: 0.6764705882352942
2026-01-18 17:51:38,444:INFO: f1 score of LEVEL_KEY: 0.6242038216560509
2026-01-18 17:51:38,444:INFO: f1 score of VALUE: 0.8258064516129032
2026-01-18 17:53:03,340:INFO: device: cuda:0
2026-01-18 17:53:03,340:INFO: --------Process Done!--------
2026-01-18 17:53:03,348:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:53:03,348:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:53:03,348:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:53:03,348:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:53:03,348:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:53:03,348:INFO: loading file None
2026-01-18 17:53:03,348:INFO: loading file None
2026-01-18 17:53:03,348:INFO: loading file None
2026-01-18 17:53:03,697:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:53:03,698:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:53:03,698:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:53:03,698:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:53:03,698:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:53:03,698:INFO: loading file None
2026-01-18 17:53:03,698:INFO: loading file None
2026-01-18 17:53:03,698:INFO: loading file None
2026-01-18 17:53:03,744:INFO: --------Dataset Build!--------
2026-01-18 17:53:03,745:INFO: --------Get Dataloader!--------
2026-01-18 17:53:03,745:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-18 17:53:03,745:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:53:03,746:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-18 17:53:10,829:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-18 17:53:10,829:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-18 17:53:12,519:INFO: --------Start Training!--------
2026-01-18 17:53:21,374:INFO: Epoch: 1, train loss: 624.3697814941406
2026-01-18 17:53:21,583:INFO: Epoch: 1, dev loss: 187.27606201171875, f1 score: 0.1435406698564593
2026-01-18 17:53:21,585:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:53:26,226:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:53:26,227:INFO: --------Save best model!--------
2026-01-18 17:53:33,789:INFO: Epoch: 2, train loss: 204.57886832101005
2026-01-18 17:53:34,011:INFO: Epoch: 2, dev loss: 85.31306838989258, f1 score: 0.5829145728643217
2026-01-18 17:53:34,012:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:53:38,573:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:53:38,573:INFO: --------Save best model!--------
2026-01-18 17:53:46,444:INFO: Epoch: 3, train loss: 95.50004168919155
2026-01-18 17:53:46,666:INFO: Epoch: 3, dev loss: 102.59519958496094, f1 score: 0.5511811023622049
2026-01-18 17:53:54,532:INFO: Epoch: 4, train loss: 58.969047273908345
2026-01-18 17:53:54,752:INFO: Epoch: 4, dev loss: 65.24052047729492, f1 score: 0.6606334841628959
2026-01-18 17:53:54,753:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:53:58,734:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:53:58,735:INFO: --------Save best model!--------
2026-01-18 17:54:06,275:INFO: Epoch: 5, train loss: 48.830745697021484
2026-01-18 17:54:06,500:INFO: Epoch: 5, dev loss: 95.25255584716797, f1 score: 0.6384976525821596
2026-01-18 17:54:14,267:INFO: Epoch: 6, train loss: 33.61271204267229
2026-01-18 17:54:14,489:INFO: Epoch: 6, dev loss: 132.2086639404297, f1 score: 0.6206896551724138
2026-01-18 17:54:22,126:INFO: Epoch: 7, train loss: 27.104540416172572
2026-01-18 17:54:22,348:INFO: Epoch: 7, dev loss: 104.3528060913086, f1 score: 0.6538461538461539
2026-01-18 17:54:30,153:INFO: Epoch: 8, train loss: 26.439280101231166
2026-01-18 17:54:30,368:INFO: Epoch: 8, dev loss: 72.6520004272461, f1 score: 0.7037037037037038
2026-01-18 17:54:30,369:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:54:34,966:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:54:34,967:INFO: --------Save best model!--------
2026-01-18 17:54:42,793:INFO: Epoch: 9, train loss: 17.512089048113143
2026-01-18 17:54:43,009:INFO: Epoch: 9, dev loss: 104.86582565307617, f1 score: 0.6431718061674009
2026-01-18 17:54:50,600:INFO: Epoch: 10, train loss: 21.4475462777274
2026-01-18 17:54:50,809:INFO: Epoch: 10, dev loss: 107.44043350219727, f1 score: 0.6948356807511737
2026-01-18 17:54:58,557:INFO: Epoch: 11, train loss: 10.613197871616908
2026-01-18 17:54:58,779:INFO: Epoch: 11, dev loss: 134.49352264404297, f1 score: 0.6097560975609756
2026-01-18 17:55:06,495:INFO: Epoch: 12, train loss: 8.27513939993722
2026-01-18 17:55:06,700:INFO: Epoch: 12, dev loss: 167.0185089111328, f1 score: 0.5991902834008097
2026-01-18 17:55:14,495:INFO: Epoch: 13, train loss: 7.847116470336914
2026-01-18 17:55:14,715:INFO: Epoch: 13, dev loss: 153.66527938842773, f1 score: 0.654867256637168
2026-01-18 17:55:22,172:INFO: Epoch: 14, train loss: 7.788515090942383
2026-01-18 17:55:22,393:INFO: Epoch: 14, dev loss: 161.9897232055664, f1 score: 0.6278026905829597
2026-01-18 17:55:30,202:INFO: Epoch: 15, train loss: 5.0868650163922995
2026-01-18 17:55:30,428:INFO: Epoch: 15, dev loss: 151.86176300048828, f1 score: 0.6513761467889908
2026-01-18 17:55:38,128:INFO: Epoch: 16, train loss: 4.289758954729352
2026-01-18 17:55:38,333:INFO: Epoch: 16, dev loss: 191.61783981323242, f1 score: 0.6173913043478261
2026-01-18 17:55:46,174:INFO: Epoch: 17, train loss: 6.1708463941301614
2026-01-18 17:55:46,383:INFO: Epoch: 17, dev loss: 184.8682098388672, f1 score: 0.6276150627615064
2026-01-18 17:55:54,088:INFO: Epoch: 18, train loss: 3.992069789341518
2026-01-18 17:55:54,299:INFO: Epoch: 18, dev loss: 191.95879364013672, f1 score: 0.6606334841628959
2026-01-18 17:55:54,299:INFO: Best val f1: 0.7037037037037038
2026-01-18 17:55:54,299:INFO: Training Finished!
2026-01-18 17:55:54,308:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:55:54,308:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:55:54,308:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:55:54,308:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:55:54,308:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:55:54,308:INFO: loading file None
2026-01-18 17:55:54,308:INFO: loading file None
2026-01-18 17:55:54,308:INFO: loading file None
2026-01-18 17:55:54,410:INFO: --------Dataset Build!--------
2026-01-18 17:55:54,410:INFO: --------Get Data-loader!--------
2026-01-18 17:55:54,410:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 17:55:54,411:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 17:55:54,411:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 17:56:01,130:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-18 17:56:01,132:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 17:56:01,132:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 17:56:01,132:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 17:56:01,132:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 17:56:01,132:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 17:56:01,132:INFO: loading file None
2026-01-18 17:56:01,132:INFO: loading file None
2026-01-18 17:56:01,132:INFO: loading file None
2026-01-18 17:56:02,007:INFO: --------Bad Cases reserved !--------
2026-01-18 17:56:02,013:INFO: test loss: 196.4960594177246, f1 score: 0.6861702127659576
2026-01-18 17:56:02,013:INFO: f1 score of ORG: 0.702127659574468
2026-01-18 17:56:02,013:INFO: f1 score of ACTION: 0.588235294117647
2026-01-18 17:56:02,013:INFO: f1 score of OBJ: 0.6530612244897959
2026-01-18 17:56:02,013:INFO: f1 score of LEVEL_KEY: 0.6707317073170732
2026-01-18 17:56:02,013:INFO: f1 score of VALUE: 0.8148148148148148
2026-01-18 19:53:40,910:INFO: device: cuda:0
2026-01-18 19:53:40,910:INFO: --------Process Done!--------
2026-01-18 19:53:40,918:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 19:53:40,918:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 19:53:40,919:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 19:53:40,919:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 19:53:40,919:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 19:53:40,919:INFO: loading file None
2026-01-18 19:53:40,919:INFO: loading file None
2026-01-18 19:53:40,919:INFO: loading file None
2026-01-18 19:53:41,277:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 19:53:41,277:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 19:53:41,277:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 19:53:41,277:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 19:53:41,277:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 19:53:41,277:INFO: loading file None
2026-01-18 19:53:41,277:INFO: loading file None
2026-01-18 19:53:41,277:INFO: loading file None
2026-01-18 19:53:41,323:INFO: --------Dataset Build!--------
2026-01-18 19:53:41,323:INFO: --------Get Dataloader!--------
2026-01-18 19:53:41,323:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-18 19:53:41,324:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 19:53:41,324:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-18 19:53:49,624:INFO: Weights of BertNER not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-18 19:53:49,624:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-18 19:53:51,653:INFO: --------Start Training!--------
2026-01-18 19:54:01,349:INFO: Epoch: 1, train loss: 704.003780909947
2026-01-18 19:54:01,570:INFO: Epoch: 1, dev loss: 190.01596450805664, f1 score: 0.15763546798029557
2026-01-18 19:54:01,572:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:54:05,987:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:54:05,988:INFO: --------Save best model!--------
2026-01-18 19:54:14,018:INFO: Epoch: 2, train loss: 213.1278899056571
2026-01-18 19:54:14,248:INFO: Epoch: 2, dev loss: 78.8823127746582, f1 score: 0.5925925925925927
2026-01-18 19:54:14,250:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:54:19,952:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:54:19,953:INFO: --------Save best model!--------
2026-01-18 19:54:27,541:INFO: Epoch: 3, train loss: 105.58371489388603
2026-01-18 19:54:27,767:INFO: Epoch: 3, dev loss: 72.71706771850586, f1 score: 0.6181818181818182
2026-01-18 19:54:27,769:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:54:32,630:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:54:32,630:INFO: --------Save best model!--------
2026-01-18 19:54:40,307:INFO: Epoch: 4, train loss: 75.60630171639579
2026-01-18 19:54:40,529:INFO: Epoch: 4, dev loss: 71.5940055847168, f1 score: 0.6603773584905661
2026-01-18 19:54:40,530:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:54:45,230:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:54:45,231:INFO: --------Save best model!--------
2026-01-18 19:54:53,279:INFO: Epoch: 5, train loss: 45.59585871015276
2026-01-18 19:54:53,504:INFO: Epoch: 5, dev loss: 170.88043975830078, f1 score: 0.5639097744360902
2026-01-18 19:55:01,605:INFO: Epoch: 6, train loss: 38.191374914986746
2026-01-18 19:55:01,826:INFO: Epoch: 6, dev loss: 121.98831939697266, f1 score: 0.6234817813765183
2026-01-18 19:55:09,620:INFO: Epoch: 7, train loss: 29.589829308646067
2026-01-18 19:55:09,839:INFO: Epoch: 7, dev loss: 117.88077926635742, f1 score: 0.6301369863013698
2026-01-18 19:55:18,092:INFO: Epoch: 8, train loss: 20.760651452200754
2026-01-18 19:55:18,329:INFO: Epoch: 8, dev loss: 126.30686950683594, f1 score: 0.613953488372093
2026-01-18 19:55:26,275:INFO: Epoch: 9, train loss: 17.574796676635742
2026-01-18 19:55:26,484:INFO: Epoch: 9, dev loss: 120.92723846435547, f1 score: 0.5991902834008097
2026-01-18 19:55:34,255:INFO: Epoch: 10, train loss: 15.985673904418945
2026-01-18 19:55:34,487:INFO: Epoch: 10, dev loss: 110.60162353515625, f1 score: 0.6224066390041494
2026-01-18 19:55:42,585:INFO: Epoch: 11, train loss: 10.191452026367188
2026-01-18 19:55:42,832:INFO: Epoch: 11, dev loss: 127.35988998413086, f1 score: 0.6375545851528385
2026-01-18 19:55:50,646:INFO: Epoch: 12, train loss: 11.97996439252581
2026-01-18 19:55:50,867:INFO: Epoch: 12, dev loss: 128.42480087280273, f1 score: 0.6637168141592921
2026-01-18 19:55:50,868:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:55:55,584:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:55:55,585:INFO: --------Save best model!--------
2026-01-18 19:56:03,448:INFO: Epoch: 13, train loss: 7.430287224905832
2026-01-18 19:56:03,669:INFO: Epoch: 13, dev loss: 157.03811645507812, f1 score: 0.632034632034632
2026-01-18 19:56:11,382:INFO: Epoch: 14, train loss: 6.285255432128906
2026-01-18 19:56:11,582:INFO: Epoch: 14, dev loss: 174.4990463256836, f1 score: 0.6359832635983264
2026-01-18 19:56:19,549:INFO: Epoch: 15, train loss: 3.7387619018554688
2026-01-18 19:56:19,787:INFO: Epoch: 15, dev loss: 163.00808715820312, f1 score: 0.6403508771929826
2026-01-18 19:56:27,653:INFO: Epoch: 16, train loss: 4.454602922712054
2026-01-18 19:56:27,885:INFO: Epoch: 16, dev loss: 176.67053604125977, f1 score: 0.6434782608695652
2026-01-18 19:56:35,771:INFO: Epoch: 17, train loss: 4.547213963099888
2026-01-18 19:56:35,977:INFO: Epoch: 17, dev loss: 187.3406867980957, f1 score: 0.6550218340611353
2026-01-18 19:56:43,907:INFO: Epoch: 18, train loss: 3.792144230433873
2026-01-18 19:56:44,117:INFO: Epoch: 18, dev loss: 196.34264373779297, f1 score: 0.6302521008403361
2026-01-18 19:56:52,147:INFO: Epoch: 19, train loss: 2.042834690638951
2026-01-18 19:56:52,385:INFO: Epoch: 19, dev loss: 213.09592819213867, f1 score: 0.672566371681416
2026-01-18 19:56:52,387:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:56:57,140:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:56:57,140:INFO: --------Save best model!--------
2026-01-18 19:57:05,271:INFO: Epoch: 20, train loss: 1.903390611921038
2026-01-18 19:57:05,475:INFO: Epoch: 20, dev loss: 232.29854202270508, f1 score: 0.6666666666666667
2026-01-18 19:57:13,187:INFO: Epoch: 21, train loss: 1.8946380615234375
2026-01-18 19:57:13,410:INFO: Epoch: 21, dev loss: 225.4029998779297, f1 score: 0.6785714285714285
2026-01-18 19:57:13,411:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:57:17,989:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:57:17,990:INFO: --------Save best model!--------
2026-01-18 19:57:26,060:INFO: Epoch: 22, train loss: 1.147784641810826
2026-01-18 19:57:26,291:INFO: Epoch: 22, dev loss: 208.73065948486328, f1 score: 0.6727272727272728
2026-01-18 19:57:34,442:INFO: Epoch: 23, train loss: 1.6136913299560547
2026-01-18 19:57:34,658:INFO: Epoch: 23, dev loss: 208.88380432128906, f1 score: 0.6757990867579908
2026-01-18 19:57:42,953:INFO: Epoch: 24, train loss: 0.7810047694614956
2026-01-18 19:57:43,187:INFO: Epoch: 24, dev loss: 215.3238525390625, f1 score: 0.6787330316742082
2026-01-18 19:57:43,189:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:57:47,908:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:57:47,908:INFO: --------Save best model!--------
2026-01-18 19:57:55,858:INFO: Epoch: 25, train loss: 0.4550263541085379
2026-01-18 19:57:56,076:INFO: Epoch: 25, dev loss: 212.36022186279297, f1 score: 0.6517857142857143
2026-01-18 19:58:04,116:INFO: Epoch: 26, train loss: 0.49326515197753906
2026-01-18 19:58:04,327:INFO: Epoch: 26, dev loss: 212.38907623291016, f1 score: 0.6727272727272728
2026-01-18 19:58:12,206:INFO: Epoch: 27, train loss: 0.4689562661307199
2026-01-18 19:58:12,433:INFO: Epoch: 27, dev loss: 220.61667251586914, f1 score: 0.6756756756756757
2026-01-18 19:58:20,424:INFO: Epoch: 28, train loss: 0.1899700164794922
2026-01-18 19:58:20,668:INFO: Epoch: 28, dev loss: 223.37264251708984, f1 score: 0.669683257918552
2026-01-18 19:58:28,623:INFO: Epoch: 29, train loss: 0.0756209237234933
2026-01-18 19:58:28,847:INFO: Epoch: 29, dev loss: 223.893798828125, f1 score: 0.6636363636363637
2026-01-18 19:58:36,787:INFO: Epoch: 30, train loss: 0.3390977042061942
2026-01-18 19:58:37,018:INFO: Epoch: 30, dev loss: 229.29123306274414, f1 score: 0.6636771300448431
2026-01-18 19:58:45,036:INFO: Epoch: 31, train loss: 0.03117234366280692
2026-01-18 19:58:45,260:INFO: Epoch: 31, dev loss: 227.6360969543457, f1 score: 0.6636771300448431
2026-01-18 19:58:45,260:INFO: Best val f1: 0.6787330316742082
2026-01-18 19:58:45,260:INFO: Training Finished!
2026-01-18 19:58:45,268:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 19:58:45,268:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 19:58:45,268:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 19:58:45,268:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 19:58:45,268:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 19:58:45,268:INFO: loading file None
2026-01-18 19:58:45,268:INFO: loading file None
2026-01-18 19:58:45,268:INFO: loading file None
2026-01-18 19:58:45,368:INFO: --------Dataset Build!--------
2026-01-18 19:58:45,368:INFO: --------Get Data-loader!--------
2026-01-18 19:58:45,369:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 19:58:45,369:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 19:58:45,369:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 19:58:52,225:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-18 19:58:52,227:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 19:58:52,227:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 19:58:52,227:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 19:58:52,227:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 19:58:52,227:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 19:58:52,227:INFO: loading file None
2026-01-18 19:58:52,227:INFO: loading file None
2026-01-18 19:58:52,227:INFO: loading file None
2026-01-18 19:58:52,898:INFO: --------Bad Cases reserved !--------
2026-01-18 19:58:52,904:INFO: test loss: 403.4714641571045, f1 score: 0.72751677852349
2026-01-18 19:58:52,904:INFO: f1 score of ORG: 0.7526881720430108
2026-01-18 19:58:52,904:INFO: f1 score of ACTION: 0.6277372262773722
2026-01-18 19:58:52,904:INFO: f1 score of OBJ: 0.71356783919598
2026-01-18 19:58:52,904:INFO: f1 score of LEVEL_KEY: 0.6987951807228916
2026-01-18 19:58:52,904:INFO: f1 score of VALUE: 0.8533333333333334
2026-01-18 23:49:55,447:INFO: device: cuda:0
2026-01-18 23:49:55,448:INFO: --------Process Done!--------
2026-01-18 23:49:55,456:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 23:49:55,456:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 23:49:55,456:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 23:49:55,456:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 23:49:55,456:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 23:49:55,456:INFO: loading file None
2026-01-18 23:49:55,456:INFO: loading file None
2026-01-18 23:49:55,456:INFO: loading file None
2026-01-18 23:49:55,809:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 23:49:55,809:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 23:49:55,809:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 23:49:55,809:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 23:49:55,809:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 23:49:55,809:INFO: loading file None
2026-01-18 23:49:55,809:INFO: loading file None
2026-01-18 23:49:55,809:INFO: loading file None
2026-01-18 23:49:55,856:INFO: --------Dataset Build!--------
2026-01-18 23:49:55,857:INFO: --------Get Dataloader!--------
2026-01-18 23:49:55,857:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-18 23:49:55,857:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 23:49:55,857:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-18 23:50:04,135:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-18 23:50:04,135:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-18 23:50:07,842:INFO: --------Start Training!--------
2026-01-18 23:50:16,425:INFO: Epoch: 1, train loss: 702.721435546875
2026-01-18 23:50:16,668:INFO: Epoch: 1, dev loss: 211.6286334991455, f1 score: 0.03278688524590165
2026-01-18 23:50:16,669:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 23:50:21,141:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 23:50:21,141:INFO: --------Save best model!--------
2026-01-18 23:50:29,287:INFO: Epoch: 2, train loss: 253.09392465863908
2026-01-18 23:50:29,513:INFO: Epoch: 2, dev loss: 101.13883590698242, f1 score: 0.3712574850299401
2026-01-18 23:50:29,514:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 23:50:34,414:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 23:50:34,415:INFO: --------Save best model!--------
2026-01-18 23:50:42,700:INFO: Epoch: 3, train loss: 130.76111030578613
2026-01-18 23:50:42,942:INFO: Epoch: 3, dev loss: 89.21191024780273, f1 score: 0.6086956521739131
2026-01-18 23:50:42,943:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 23:50:47,629:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 23:50:47,630:INFO: --------Save best model!--------
2026-01-18 23:50:55,800:INFO: Epoch: 4, train loss: 81.8845043182373
2026-01-18 23:50:56,039:INFO: Epoch: 4, dev loss: 81.86992263793945, f1 score: 0.7199999999999999
2026-01-18 23:50:56,040:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 23:51:00,832:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 23:51:00,832:INFO: --------Save best model!--------
2026-01-18 23:51:08,816:INFO: Epoch: 5, train loss: 59.58887890407017
2026-01-18 23:51:09,031:INFO: Epoch: 5, dev loss: 72.2294692993164, f1 score: 0.6909090909090909
2026-01-18 23:51:16,890:INFO: Epoch: 6, train loss: 49.65621703011649
2026-01-18 23:51:17,131:INFO: Epoch: 6, dev loss: 152.31053924560547, f1 score: 0.5287356321839081
2026-01-18 23:51:25,167:INFO: Epoch: 7, train loss: 41.66693632943289
2026-01-18 23:51:25,401:INFO: Epoch: 7, dev loss: 97.13104629516602, f1 score: 0.6194690265486725
2026-01-18 23:51:33,396:INFO: Epoch: 8, train loss: 31.56973675319127
2026-01-18 23:51:33,605:INFO: Epoch: 8, dev loss: 114.74229049682617, f1 score: 0.6481481481481483
2026-01-18 23:51:41,830:INFO: Epoch: 9, train loss: 22.017322540283203
2026-01-18 23:51:42,050:INFO: Epoch: 9, dev loss: 114.38097381591797, f1 score: 0.6267281105990784
2026-01-18 23:51:50,179:INFO: Epoch: 10, train loss: 19.60590716770717
2026-01-18 23:51:50,414:INFO: Epoch: 10, dev loss: 109.02042007446289, f1 score: 0.6883720930232557
2026-01-18 23:51:58,482:INFO: Epoch: 11, train loss: 14.83320563180106
2026-01-18 23:51:58,716:INFO: Epoch: 11, dev loss: 138.34532165527344, f1 score: 0.6757990867579908
2026-01-18 23:52:06,810:INFO: Epoch: 12, train loss: 11.675260543823242
2026-01-18 23:52:07,030:INFO: Epoch: 12, dev loss: 117.19571304321289, f1 score: 0.6609442060085837
2026-01-18 23:52:15,269:INFO: Epoch: 13, train loss: 12.126496996198382
2026-01-18 23:52:15,490:INFO: Epoch: 13, dev loss: 138.4706802368164, f1 score: 0.6812227074235807
2026-01-18 23:52:23,385:INFO: Epoch: 14, train loss: 7.197719573974609
2026-01-18 23:52:23,617:INFO: Epoch: 14, dev loss: 153.55068969726562, f1 score: 0.676328502415459
2026-01-18 23:52:23,618:INFO: Best val f1: 0.7199999999999999
2026-01-18 23:52:23,618:INFO: Training Finished!
2026-01-18 23:52:23,627:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 23:52:23,627:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 23:52:23,627:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 23:52:23,627:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 23:52:23,627:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 23:52:23,627:INFO: loading file None
2026-01-18 23:52:23,628:INFO: loading file None
2026-01-18 23:52:23,628:INFO: loading file None
2026-01-18 23:52:23,728:INFO: --------Dataset Build!--------
2026-01-18 23:52:23,729:INFO: --------Get Data-loader!--------
2026-01-18 23:52:23,729:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-18 23:52:23,729:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-18 23:52:23,729:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-18 23:52:30,464:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-18 23:52:30,465:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-18 23:52:30,466:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-18 23:52:30,466:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-18 23:52:30,466:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-18 23:52:30,466:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-18 23:52:30,466:INFO: loading file None
2026-01-18 23:52:30,466:INFO: loading file None
2026-01-18 23:52:30,466:INFO: loading file None
2026-01-18 23:52:31,168:INFO: --------Bad Cases reserved !--------
2026-01-18 23:52:31,174:INFO: test loss: 172.61065673828125, f1 score: 0.6675675675675675
2026-01-18 23:52:31,174:INFO: f1 score of ORG: 0.8222222222222222
2026-01-18 23:52:31,174:INFO: f1 score of ACTION: 0.43243243243243246
2026-01-18 23:52:31,174:INFO: f1 score of OBJ: 0.6829268292682927
2026-01-18 23:52:31,174:INFO: f1 score of LEVEL_KEY: 0.6627218934911242
2026-01-18 23:52:31,174:INFO: f1 score of VALUE: 0.7272727272727273
2026-01-19 00:19:31,605:INFO: device: cuda:0
2026-01-19 00:19:31,605:INFO: --------Process Done!--------
2026-01-19 00:19:31,612:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 00:19:31,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 00:19:31,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 00:19:31,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 00:19:31,612:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 00:19:31,612:INFO: loading file None
2026-01-19 00:19:31,612:INFO: loading file None
2026-01-19 00:19:31,612:INFO: loading file None
2026-01-19 00:19:31,960:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 00:19:31,960:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 00:19:31,960:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 00:19:31,961:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 00:19:31,961:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 00:19:31,961:INFO: loading file None
2026-01-19 00:19:31,961:INFO: loading file None
2026-01-19 00:19:31,961:INFO: loading file None
2026-01-19 00:19:32,007:INFO: --------Dataset Build!--------
2026-01-19 00:19:32,007:INFO: --------Get Dataloader!--------
2026-01-19 00:19:32,008:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 00:19:32,008:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 00:19:32,008:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 00:19:39,106:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 00:19:39,107:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 00:19:41,686:INFO: --------Start Training!--------
2026-01-19 00:19:53,706:INFO: Epoch: 1, train loss: 822.1751044137137
2026-01-19 00:19:53,935:INFO: Epoch: 1, dev loss: 288.7090835571289, f1 score: 0
2026-01-19 00:20:05,575:INFO: Epoch: 2, train loss: 380.5738525390625
2026-01-19 00:20:05,813:INFO: Epoch: 2, dev loss: 141.07392120361328, f1 score: 0.1219512195121951
2026-01-19 00:20:05,814:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:20:10,520:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:20:10,520:INFO: --------Save best model!--------
2026-01-19 00:20:21,734:INFO: Epoch: 3, train loss: 211.39237594604492
2026-01-19 00:20:21,978:INFO: Epoch: 3, dev loss: 96.8682689666748, f1 score: 0.3756906077348066
2026-01-19 00:20:21,979:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:20:26,777:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:20:26,783:INFO: --------Save best model!--------
2026-01-19 00:20:38,780:INFO: Epoch: 4, train loss: 132.88248470851354
2026-01-19 00:20:39,021:INFO: Epoch: 4, dev loss: 80.56177520751953, f1 score: 0.5858585858585859
2026-01-19 00:20:39,022:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:20:43,775:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:20:43,775:INFO: --------Save best model!--------
2026-01-19 00:20:55,544:INFO: Epoch: 5, train loss: 87.3368057523455
2026-01-19 00:20:55,763:INFO: Epoch: 5, dev loss: 86.49821853637695, f1 score: 0.6044444444444445
2026-01-19 00:20:55,764:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:21:00,548:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:21:00,549:INFO: --------Save best model!--------
2026-01-19 00:21:11,914:INFO: Epoch: 6, train loss: 61.324900354657856
2026-01-19 00:21:12,152:INFO: Epoch: 6, dev loss: 80.35843276977539, f1 score: 0.6396396396396397
2026-01-19 00:21:12,154:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:21:16,914:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:21:16,915:INFO: --------Save best model!--------
2026-01-19 00:21:28,298:INFO: Epoch: 7, train loss: 42.99026993342808
2026-01-19 00:21:28,520:INFO: Epoch: 7, dev loss: 80.14338684082031, f1 score: 0.6604651162790699
2026-01-19 00:21:28,521:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:21:31,838:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:21:31,839:INFO: --------Save best model!--------
2026-01-19 00:21:42,999:INFO: Epoch: 8, train loss: 35.11140426567623
2026-01-19 00:21:43,237:INFO: Epoch: 8, dev loss: 102.92816162109375, f1 score: 0.6605504587155963
2026-01-19 00:21:43,239:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:21:48,027:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:21:48,027:INFO: --------Save best model!--------
2026-01-19 00:21:59,873:INFO: Epoch: 9, train loss: 26.86626706804548
2026-01-19 00:22:00,113:INFO: Epoch: 9, dev loss: 92.83818054199219, f1 score: 0.6351931330472104
2026-01-19 00:22:11,665:INFO: Epoch: 10, train loss: 22.962806923048838
2026-01-19 00:22:11,899:INFO: Epoch: 10, dev loss: 116.16977310180664, f1 score: 0.6371681415929203
2026-01-19 00:22:23,652:INFO: Epoch: 11, train loss: 22.173748514481954
2026-01-19 00:22:23,872:INFO: Epoch: 11, dev loss: 103.41277694702148, f1 score: 0.6666666666666667
2026-01-19 00:22:23,874:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:22:28,653:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:22:28,654:INFO: --------Save best model!--------
2026-01-19 00:22:39,989:INFO: Epoch: 12, train loss: 15.109419247933797
2026-01-19 00:22:40,235:INFO: Epoch: 12, dev loss: 128.25288772583008, f1 score: 0.6547085201793722
2026-01-19 00:22:51,699:INFO: Epoch: 13, train loss: 16.86173239350319
2026-01-19 00:22:51,925:INFO: Epoch: 13, dev loss: 131.90188217163086, f1 score: 0.6550218340611353
2026-01-19 00:23:03,246:INFO: Epoch: 14, train loss: 12.252535956246513
2026-01-19 00:23:03,465:INFO: Epoch: 14, dev loss: 134.7835464477539, f1 score: 0.663594470046083
2026-01-19 00:23:14,699:INFO: Epoch: 15, train loss: 8.394924175526414
2026-01-19 00:23:14,918:INFO: Epoch: 15, dev loss: 151.81588745117188, f1 score: 0.6666666666666666
2026-01-19 00:23:26,452:INFO: Epoch: 16, train loss: 8.506219207708325
2026-01-19 00:23:26,682:INFO: Epoch: 16, dev loss: 175.49519729614258, f1 score: 0.6324786324786325
2026-01-19 00:23:38,450:INFO: Epoch: 17, train loss: 7.948945987969637
2026-01-19 00:23:38,688:INFO: Epoch: 17, dev loss: 142.44597625732422, f1 score: 0.6784140969162996
2026-01-19 00:23:38,689:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:23:43,453:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:23:43,453:INFO: --------Save best model!--------
2026-01-19 00:23:55,121:INFO: Epoch: 18, train loss: 5.353144272097519
2026-01-19 00:23:55,356:INFO: Epoch: 18, dev loss: 179.1764373779297, f1 score: 0.6666666666666666
2026-01-19 00:24:07,240:INFO: Epoch: 19, train loss: 6.995344442847584
2026-01-19 00:24:07,479:INFO: Epoch: 19, dev loss: 179.42959594726562, f1 score: 0.6637168141592921
2026-01-19 00:24:19,168:INFO: Epoch: 20, train loss: 6.899211966432631
2026-01-19 00:24:19,391:INFO: Epoch: 20, dev loss: 182.9083366394043, f1 score: 0.646288209606987
2026-01-19 00:24:31,190:INFO: Epoch: 21, train loss: 4.230729282501021
2026-01-19 00:24:31,430:INFO: Epoch: 21, dev loss: 200.20230102539062, f1 score: 0.6212765957446807
2026-01-19 00:24:42,967:INFO: Epoch: 22, train loss: 7.320329495106956
2026-01-19 00:24:43,197:INFO: Epoch: 22, dev loss: 179.8288116455078, f1 score: 0.6425339366515838
2026-01-19 00:24:54,677:INFO: Epoch: 23, train loss: 5.566637478835348
2026-01-19 00:24:54,896:INFO: Epoch: 23, dev loss: 195.45746231079102, f1 score: 0.6578947368421053
2026-01-19 00:25:06,660:INFO: Epoch: 24, train loss: 1.7904504392562168
2026-01-19 00:25:06,900:INFO: Epoch: 24, dev loss: 210.3349380493164, f1 score: 0.6493506493506493
2026-01-19 00:25:18,202:INFO: Epoch: 25, train loss: 1.4820118692503976
2026-01-19 00:25:18,437:INFO: Epoch: 25, dev loss: 199.63718795776367, f1 score: 0.6637168141592921
2026-01-19 00:25:29,931:INFO: Epoch: 26, train loss: 1.2938018474461777
2026-01-19 00:25:30,162:INFO: Epoch: 26, dev loss: 197.77152633666992, f1 score: 0.6607929515418502
2026-01-19 00:25:41,719:INFO: Epoch: 27, train loss: 0.4880308554773884
2026-01-19 00:25:41,957:INFO: Epoch: 27, dev loss: 197.37018585205078, f1 score: 0.6696035242290749
2026-01-19 00:25:41,957:INFO: Best val f1: 0.6784140969162996
2026-01-19 00:25:41,958:INFO: Training Finished!
2026-01-19 00:25:41,967:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 00:25:41,967:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 00:25:41,967:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 00:25:41,967:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 00:25:41,967:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 00:25:41,967:INFO: loading file None
2026-01-19 00:25:41,967:INFO: loading file None
2026-01-19 00:25:41,967:INFO: loading file None
2026-01-19 00:25:42,067:INFO: --------Dataset Build!--------
2026-01-19 00:25:42,067:INFO: --------Get Data-loader!--------
2026-01-19 00:25:42,068:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 00:25:42,068:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 00:25:42,068:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 00:25:48,932:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 00:25:48,933:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 00:25:48,934:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 00:25:48,934:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 00:25:48,934:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 00:25:48,934:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 00:25:48,934:INFO: loading file None
2026-01-19 00:25:48,934:INFO: loading file None
2026-01-19 00:25:48,934:INFO: loading file None
2026-01-19 00:25:49,635:INFO: --------Bad Cases reserved !--------
2026-01-19 00:25:49,641:INFO: test loss: 273.6014747619629, f1 score: 0.7386215864759428
2026-01-19 00:25:49,641:INFO: f1 score of ORG: 0.8297872340425533
2026-01-19 00:25:49,641:INFO: f1 score of ACTION: 0.6285714285714286
2026-01-19 00:25:49,641:INFO: f1 score of OBJ: 0.7177033492822967
2026-01-19 00:25:49,641:INFO: f1 score of LEVEL_KEY: 0.7108433734939759
2026-01-19 00:25:49,641:INFO: f1 score of VALUE: 0.8375
2026-01-19 11:34:59,191:INFO: device: cuda:0
2026-01-19 11:34:59,191:INFO: --------Process Done!--------
2026-01-19 11:34:59,198:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:34:59,199:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:34:59,199:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:34:59,199:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:34:59,199:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:34:59,199:INFO: loading file None
2026-01-19 11:34:59,199:INFO: loading file None
2026-01-19 11:34:59,199:INFO: loading file None
2026-01-19 11:34:59,544:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:34:59,544:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:34:59,544:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:34:59,544:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:34:59,544:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:34:59,544:INFO: loading file None
2026-01-19 11:34:59,545:INFO: loading file None
2026-01-19 11:34:59,545:INFO: loading file None
2026-01-19 11:34:59,590:INFO: --------Dataset Build!--------
2026-01-19 11:34:59,590:INFO: --------Get Dataloader!--------
2026-01-19 11:34:59,590:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 11:34:59,591:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:34:59,591:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 11:35:06,656:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 11:35:06,656:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 11:35:09,191:INFO: --------Start Training!--------
2026-01-19 11:35:20,750:INFO: Epoch: 1, train loss: 839.5304478236607
2026-01-19 11:35:20,985:INFO: Epoch: 1, dev loss: 292.38655853271484, f1 score: 0
2026-01-19 11:35:32,533:INFO: Epoch: 2, train loss: 383.51418467930387
2026-01-19 11:35:32,752:INFO: Epoch: 2, dev loss: 145.6642951965332, f1 score: 0.1168831168831169
2026-01-19 11:35:32,753:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:35:37,463:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:35:37,463:INFO: --------Save best model!--------
2026-01-19 11:35:49,357:INFO: Epoch: 3, train loss: 206.0075351170131
2026-01-19 11:35:49,592:INFO: Epoch: 3, dev loss: 100.05524826049805, f1 score: 0.37714285714285706
2026-01-19 11:35:49,593:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:35:54,354:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:35:54,354:INFO: --------Save best model!--------
2026-01-19 11:36:05,796:INFO: Epoch: 4, train loss: 126.99396215166364
2026-01-19 11:36:06,006:INFO: Epoch: 4, dev loss: 84.24208068847656, f1 score: 0.5555555555555556
2026-01-19 11:36:06,007:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:36:10,767:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:36:10,767:INFO: --------Save best model!--------
2026-01-19 11:36:22,444:INFO: Epoch: 5, train loss: 90.92181777954102
2026-01-19 11:36:22,687:INFO: Epoch: 5, dev loss: 74.39923858642578, f1 score: 0.6395939086294415
2026-01-19 11:36:22,688:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:36:27,458:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:36:27,458:INFO: --------Save best model!--------
2026-01-19 11:36:38,958:INFO: Epoch: 6, train loss: 61.78683846337454
2026-01-19 11:36:39,184:INFO: Epoch: 6, dev loss: 90.85737228393555, f1 score: 0.6363636363636364
2026-01-19 11:36:50,563:INFO: Epoch: 7, train loss: 48.60101515906198
2026-01-19 11:36:50,791:INFO: Epoch: 7, dev loss: 95.26211166381836, f1 score: 0.6502463054187192
2026-01-19 11:36:50,792:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:36:55,554:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:36:55,555:INFO: --------Save best model!--------
2026-01-19 11:37:07,093:INFO: Epoch: 8, train loss: 31.54101232119969
2026-01-19 11:37:07,339:INFO: Epoch: 8, dev loss: 106.88099670410156, f1 score: 0.6431718061674009
2026-01-19 11:37:19,036:INFO: Epoch: 9, train loss: 29.453392543963023
2026-01-19 11:37:19,275:INFO: Epoch: 9, dev loss: 118.86791229248047, f1 score: 0.6851851851851851
2026-01-19 11:37:19,277:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:37:23,832:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:37:23,832:INFO: --------Save best model!--------
2026-01-19 11:37:35,563:INFO: Epoch: 10, train loss: 21.567265774522507
2026-01-19 11:37:35,773:INFO: Epoch: 10, dev loss: 110.42945861816406, f1 score: 0.649789029535865
2026-01-19 11:37:47,292:INFO: Epoch: 11, train loss: 21.11537375194686
2026-01-19 11:37:47,524:INFO: Epoch: 11, dev loss: 135.32505798339844, f1 score: 0.6324786324786325
2026-01-19 11:37:58,964:INFO: Epoch: 12, train loss: 20.11054645691599
2026-01-19 11:37:59,201:INFO: Epoch: 12, dev loss: 131.37890243530273, f1 score: 0.6666666666666666
2026-01-19 11:38:10,611:INFO: Epoch: 13, train loss: 13.463894831282753
2026-01-19 11:38:10,848:INFO: Epoch: 13, dev loss: 148.47873306274414, f1 score: 0.5761316872427984
2026-01-19 11:38:22,540:INFO: Epoch: 14, train loss: 9.087233032499041
2026-01-19 11:38:22,769:INFO: Epoch: 14, dev loss: 143.2682228088379, f1 score: 0.6666666666666666
2026-01-19 11:38:34,027:INFO: Epoch: 15, train loss: 7.974128495369639
2026-01-19 11:38:34,263:INFO: Epoch: 15, dev loss: 160.53834915161133, f1 score: 0.6306306306306306
2026-01-19 11:38:46,059:INFO: Epoch: 16, train loss: 8.69391458587987
2026-01-19 11:38:46,295:INFO: Epoch: 16, dev loss: 163.7979621887207, f1 score: 0.6575342465753424
2026-01-19 11:38:57,697:INFO: Epoch: 17, train loss: 5.656721942126751
2026-01-19 11:38:57,927:INFO: Epoch: 17, dev loss: 156.4784698486328, f1 score: 0.6636771300448431
2026-01-19 11:39:09,323:INFO: Epoch: 18, train loss: 6.737841472029686
2026-01-19 11:39:09,543:INFO: Epoch: 18, dev loss: 167.67297744750977, f1 score: 0.6666666666666667
2026-01-19 11:39:21,016:INFO: Epoch: 19, train loss: 4.3835215379617045
2026-01-19 11:39:21,239:INFO: Epoch: 19, dev loss: 168.4078140258789, f1 score: 0.676056338028169
2026-01-19 11:39:21,240:INFO: Best val f1: 0.6851851851851851
2026-01-19 11:39:21,240:INFO: Training Finished!
2026-01-19 11:39:21,248:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:39:21,248:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:39:21,248:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:39:21,248:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:39:21,248:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:39:21,248:INFO: loading file None
2026-01-19 11:39:21,248:INFO: loading file None
2026-01-19 11:39:21,248:INFO: loading file None
2026-01-19 11:39:21,349:INFO: --------Dataset Build!--------
2026-01-19 11:39:21,349:INFO: --------Get Data-loader!--------
2026-01-19 11:39:21,349:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:39:21,349:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:39:21,349:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:39:28,081:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 11:39:28,082:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:39:28,082:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:39:28,083:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:39:28,083:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:39:28,083:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:39:28,083:INFO: loading file None
2026-01-19 11:39:28,083:INFO: loading file None
2026-01-19 11:39:28,083:INFO: loading file None
2026-01-19 11:39:28,781:INFO: --------Bad Cases reserved !--------
2026-01-19 11:39:28,787:INFO: test loss: 214.94598484039307, f1 score: 0.710455764075067
2026-01-19 11:39:28,787:INFO: f1 score of ORG: 0.8571428571428572
2026-01-19 11:39:28,787:INFO: f1 score of ACTION: 0.5657894736842106
2026-01-19 11:39:28,787:INFO: f1 score of OBJ: 0.6947368421052633
2026-01-19 11:39:28,787:INFO: f1 score of LEVEL_KEY: 0.6797385620915032
2026-01-19 11:39:28,787:INFO: f1 score of VALUE: 0.8104575163398693
2026-01-19 11:43:10,995:INFO: device: cuda:0
2026-01-19 11:43:10,995:INFO: --------Process Done!--------
2026-01-19 11:43:11,002:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:43:11,002:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:43:11,002:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:43:11,002:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:43:11,002:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:43:11,003:INFO: loading file None
2026-01-19 11:43:11,003:INFO: loading file None
2026-01-19 11:43:11,003:INFO: loading file None
2026-01-19 11:43:11,343:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:43:11,343:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:43:11,343:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:43:11,343:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:43:11,343:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:43:11,343:INFO: loading file None
2026-01-19 11:43:11,343:INFO: loading file None
2026-01-19 11:43:11,343:INFO: loading file None
2026-01-19 11:43:11,389:INFO: --------Dataset Build!--------
2026-01-19 11:43:11,390:INFO: --------Get Dataloader!--------
2026-01-19 11:43:11,390:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 11:43:11,390:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:43:11,390:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 11:43:18,532:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 11:43:18,533:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 11:43:21,111:INFO: --------Start Training!--------
2026-01-19 11:43:32,859:INFO: Epoch: 1, train loss: 810.7615258353097
2026-01-19 11:43:33,087:INFO: Epoch: 1, dev loss: 277.29747009277344, f1 score: 0
2026-01-19 11:43:44,984:INFO: Epoch: 2, train loss: 359.70096642630443
2026-01-19 11:43:45,223:INFO: Epoch: 2, dev loss: 135.16484832763672, f1 score: 0.1032258064516129
2026-01-19 11:43:45,224:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:43:49,808:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:43:49,808:INFO: --------Save best model!--------
2026-01-19 11:44:01,415:INFO: Epoch: 3, train loss: 201.24734715053015
2026-01-19 11:44:01,668:INFO: Epoch: 3, dev loss: 97.60684967041016, f1 score: 0.44067796610169496
2026-01-19 11:44:01,669:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:44:06,354:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:44:06,354:INFO: --------Save best model!--------
2026-01-19 11:44:18,065:INFO: Epoch: 4, train loss: 133.41014916556222
2026-01-19 11:44:18,304:INFO: Epoch: 4, dev loss: 107.17047882080078, f1 score: 0.5263157894736842
2026-01-19 11:44:18,305:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:44:23,019:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:44:23,019:INFO: --------Save best model!--------
2026-01-19 11:44:34,443:INFO: Epoch: 5, train loss: 101.17125170571464
2026-01-19 11:44:34,686:INFO: Epoch: 5, dev loss: 71.89886474609375, f1 score: 0.6243902439024389
2026-01-19 11:44:34,688:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:44:39,401:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:44:39,402:INFO: --------Save best model!--------
2026-01-19 11:44:50,881:INFO: Epoch: 6, train loss: 72.96614422116961
2026-01-19 11:44:51,126:INFO: Epoch: 6, dev loss: 95.33684539794922, f1 score: 0.5892857142857143
2026-01-19 11:45:02,597:INFO: Epoch: 7, train loss: 51.23709784235273
2026-01-19 11:45:02,839:INFO: Epoch: 7, dev loss: 97.47750854492188, f1 score: 0.6560000000000001
2026-01-19 11:45:02,840:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:45:07,508:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:45:07,509:INFO: --------Save best model!--------
2026-01-19 11:45:18,697:INFO: Epoch: 8, train loss: 37.605305067130494
2026-01-19 11:45:18,932:INFO: Epoch: 8, dev loss: 92.32061767578125, f1 score: 0.6212765957446807
2026-01-19 11:45:30,700:INFO: Epoch: 9, train loss: 32.44101474966322
2026-01-19 11:45:30,940:INFO: Epoch: 9, dev loss: 91.43180465698242, f1 score: 0.6470588235294118
2026-01-19 11:45:42,494:INFO: Epoch: 10, train loss: 25.763786835329874
2026-01-19 11:45:42,723:INFO: Epoch: 10, dev loss: 102.3469009399414, f1 score: 0.6428571428571429
2026-01-19 11:45:54,293:INFO: Epoch: 11, train loss: 20.952382223946707
2026-01-19 11:45:54,528:INFO: Epoch: 11, dev loss: 133.88999557495117, f1 score: 0.6090909090909091
2026-01-19 11:46:05,899:INFO: Epoch: 12, train loss: 14.709124101059777
2026-01-19 11:46:06,136:INFO: Epoch: 12, dev loss: 120.71741104125977, f1 score: 0.6547085201793722
2026-01-19 11:46:17,794:INFO: Epoch: 13, train loss: 12.34819170832634
2026-01-19 11:46:18,024:INFO: Epoch: 13, dev loss: 177.1698760986328, f1 score: 0.6396396396396397
2026-01-19 11:46:29,652:INFO: Epoch: 14, train loss: 12.036668459751777
2026-01-19 11:46:29,876:INFO: Epoch: 14, dev loss: 208.95013427734375, f1 score: 0.5909090909090908
2026-01-19 11:46:41,287:INFO: Epoch: 15, train loss: 13.578551915075098
2026-01-19 11:46:41,527:INFO: Epoch: 15, dev loss: 174.42339324951172, f1 score: 0.6355932203389831
2026-01-19 11:46:52,871:INFO: Epoch: 16, train loss: 6.852034327174936
2026-01-19 11:46:53,092:INFO: Epoch: 16, dev loss: 185.24160766601562, f1 score: 0.632034632034632
2026-01-19 11:47:04,843:INFO: Epoch: 17, train loss: 8.430551040119358
2026-01-19 11:47:05,079:INFO: Epoch: 17, dev loss: 201.12996673583984, f1 score: 0.6329113924050633
2026-01-19 11:47:05,079:INFO: Best val f1: 0.6560000000000001
2026-01-19 11:47:05,080:INFO: Training Finished!
2026-01-19 11:47:05,087:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:47:05,087:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:47:05,087:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:47:05,087:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:47:05,087:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:47:05,087:INFO: loading file None
2026-01-19 11:47:05,087:INFO: loading file None
2026-01-19 11:47:05,087:INFO: loading file None
2026-01-19 11:47:05,188:INFO: --------Dataset Build!--------
2026-01-19 11:47:05,188:INFO: --------Get Data-loader!--------
2026-01-19 11:47:05,188:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:47:05,189:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:47:05,189:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:47:11,925:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 11:47:11,926:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:47:11,927:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:47:11,927:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:47:11,927:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:47:11,927:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:47:11,927:INFO: loading file None
2026-01-19 11:47:11,927:INFO: loading file None
2026-01-19 11:47:11,927:INFO: loading file None
2026-01-19 11:47:12,625:INFO: --------Bad Cases reserved !--------
2026-01-19 11:47:12,631:INFO: test loss: 185.41055297851562, f1 score: 0.6561360874848117
2026-01-19 11:47:12,631:INFO: f1 score of ORG: 0.8043478260869567
2026-01-19 11:47:12,631:INFO: f1 score of ACTION: 0.47368421052631576
2026-01-19 11:47:12,631:INFO: f1 score of OBJ: 0.6106870229007634
2026-01-19 11:47:12,631:INFO: f1 score of LEVEL_KEY: 0.6666666666666666
2026-01-19 11:47:12,632:INFO: f1 score of VALUE: 0.810126582278481
2026-01-19 11:58:07,150:INFO: device: cuda:0
2026-01-19 11:58:07,150:INFO: --------Process Done!--------
2026-01-19 11:58:07,159:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:58:07,159:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:58:07,159:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:58:07,159:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:58:07,159:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:58:07,159:INFO: loading file None
2026-01-19 11:58:07,159:INFO: loading file None
2026-01-19 11:58:07,159:INFO: loading file None
2026-01-19 11:58:07,501:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:58:07,501:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:58:07,501:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:58:07,501:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:58:07,501:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:58:07,501:INFO: loading file None
2026-01-19 11:58:07,501:INFO: loading file None
2026-01-19 11:58:07,501:INFO: loading file None
2026-01-19 11:58:07,546:INFO: --------Dataset Build!--------
2026-01-19 11:58:07,547:INFO: --------Get Dataloader!--------
2026-01-19 11:58:07,547:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 11:58:07,547:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:58:07,547:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 11:58:14,271:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 11:58:14,271:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 11:58:16,778:INFO: --------Start Training!--------
2026-01-19 11:58:29,556:INFO: device: cuda:0
2026-01-19 11:58:29,556:INFO: --------Process Done!--------
2026-01-19 11:58:29,563:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:58:29,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:58:29,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:58:29,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:58:29,563:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:58:29,563:INFO: loading file None
2026-01-19 11:58:29,563:INFO: loading file None
2026-01-19 11:58:29,563:INFO: loading file None
2026-01-19 11:58:29,906:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 11:58:29,906:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 11:58:29,906:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 11:58:29,906:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 11:58:29,906:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 11:58:29,906:INFO: loading file None
2026-01-19 11:58:29,906:INFO: loading file None
2026-01-19 11:58:29,906:INFO: loading file None
2026-01-19 11:58:29,951:INFO: --------Dataset Build!--------
2026-01-19 11:58:29,952:INFO: --------Get Dataloader!--------
2026-01-19 11:58:29,952:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 11:58:29,952:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 11:58:29,952:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 11:58:37,011:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 11:58:37,012:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 11:58:39,680:INFO: --------Start Training!--------
2026-01-19 11:58:48,614:INFO: Epoch: 1, train loss: 1881.4389474051338
2026-01-19 11:58:48,770:INFO: Epoch: 1, dev loss: 826.4057922363281, f1 score: 0
2026-01-19 11:58:57,224:INFO: Epoch: 2, train loss: 1021.701154436384
2026-01-19 11:58:57,378:INFO: Epoch: 2, dev loss: 446.0684814453125, f1 score: 0.034482758620689655
2026-01-19 11:58:57,379:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:59:02,134:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:59:02,134:INFO: --------Save best model!--------
2026-01-19 11:59:10,514:INFO: Epoch: 3, train loss: 611.2534920828683
2026-01-19 11:59:10,688:INFO: Epoch: 3, dev loss: 262.5313720703125, f1 score: 0.09722222222222222
2026-01-19 11:59:10,689:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:59:15,479:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:59:15,479:INFO: --------Save best model!--------
2026-01-19 11:59:23,678:INFO: Epoch: 4, train loss: 354.63179779052734
2026-01-19 11:59:23,851:INFO: Epoch: 4, dev loss: 211.03961944580078, f1 score: 0.46236559139784944
2026-01-19 11:59:23,853:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:59:28,540:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:59:28,540:INFO: --------Save best model!--------
2026-01-19 11:59:36,850:INFO: Epoch: 5, train loss: 226.62207685198103
2026-01-19 11:59:37,025:INFO: Epoch: 5, dev loss: 176.44353485107422, f1 score: 0.5585585585585586
2026-01-19 11:59:37,026:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:59:41,746:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:59:41,746:INFO: --------Save best model!--------
2026-01-19 11:59:50,486:INFO: Epoch: 6, train loss: 136.45607376098633
2026-01-19 11:59:50,654:INFO: Epoch: 6, dev loss: 163.4351577758789, f1 score: 0.5887850467289718
2026-01-19 11:59:50,655:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 11:59:55,383:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 11:59:55,383:INFO: --------Save best model!--------
2026-01-19 12:00:04,048:INFO: Epoch: 7, train loss: 92.93364361354283
2026-01-19 12:00:04,228:INFO: Epoch: 7, dev loss: 181.4797592163086, f1 score: 0.6603773584905661
2026-01-19 12:00:04,229:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:00:08,831:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:00:08,831:INFO: --------Save best model!--------
2026-01-19 12:00:17,264:INFO: Epoch: 8, train loss: 74.33486829485211
2026-01-19 12:00:17,444:INFO: Epoch: 8, dev loss: 225.02196502685547, f1 score: 0.5961538461538461
2026-01-19 12:00:25,762:INFO: Epoch: 9, train loss: 54.86287416730608
2026-01-19 12:00:25,935:INFO: Epoch: 9, dev loss: 202.16773223876953, f1 score: 0.5991561181434599
2026-01-19 12:00:34,250:INFO: Epoch: 10, train loss: 42.16428763525827
2026-01-19 12:00:34,415:INFO: Epoch: 10, dev loss: 198.62555694580078, f1 score: 0.6283185840707964
2026-01-19 12:00:42,764:INFO: Epoch: 11, train loss: 27.951627322605678
2026-01-19 12:00:42,935:INFO: Epoch: 11, dev loss: 239.20599365234375, f1 score: 0.6605504587155963
2026-01-19 12:00:42,936:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:00:47,588:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:00:47,589:INFO: --------Save best model!--------
2026-01-19 12:00:56,085:INFO: Epoch: 12, train loss: 21.74323042801448
2026-01-19 12:00:56,258:INFO: Epoch: 12, dev loss: 240.081298828125, f1 score: 0.6757990867579908
2026-01-19 12:00:56,259:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:01:00,978:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:01:00,979:INFO: --------Save best model!--------
2026-01-19 12:01:09,389:INFO: Epoch: 13, train loss: 18.462272916521346
2026-01-19 12:01:09,562:INFO: Epoch: 13, dev loss: 228.86619567871094, f1 score: 0.6578947368421053
2026-01-19 12:01:18,170:INFO: Epoch: 14, train loss: 13.957782847540718
2026-01-19 12:01:18,340:INFO: Epoch: 14, dev loss: 242.04638671875, f1 score: 0.6812227074235807
2026-01-19 12:01:18,341:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:01:23,065:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:01:23,065:INFO: --------Save best model!--------
2026-01-19 12:01:31,346:INFO: Epoch: 15, train loss: 9.668878759656634
2026-01-19 12:01:31,526:INFO: Epoch: 15, dev loss: 252.54044342041016, f1 score: 0.658008658008658
2026-01-19 12:01:40,194:INFO: Epoch: 16, train loss: 8.895349834646497
2026-01-19 12:01:40,358:INFO: Epoch: 16, dev loss: 268.5804748535156, f1 score: 0.6434782608695652
2026-01-19 12:01:48,755:INFO: Epoch: 17, train loss: 5.86490124464035
2026-01-19 12:01:48,934:INFO: Epoch: 17, dev loss: 261.1780548095703, f1 score: 0.6607142857142857
2026-01-19 12:01:57,225:INFO: Epoch: 18, train loss: 3.5271243836198534
2026-01-19 12:01:57,404:INFO: Epoch: 18, dev loss: 287.10252380371094, f1 score: 0.631578947368421
2026-01-19 12:02:05,720:INFO: Epoch: 19, train loss: 5.2505903840065
2026-01-19 12:02:05,899:INFO: Epoch: 19, dev loss: 301.5738525390625, f1 score: 0.6547085201793722
2026-01-19 12:02:14,498:INFO: Epoch: 20, train loss: 3.5732604627098357
2026-01-19 12:02:14,661:INFO: Epoch: 20, dev loss: 313.29369354248047, f1 score: 0.6465517241379309
2026-01-19 12:02:22,957:INFO: Epoch: 21, train loss: 2.412776260503701
2026-01-19 12:02:23,121:INFO: Epoch: 21, dev loss: 318.2488555908203, f1 score: 0.6607142857142857
2026-01-19 12:02:31,859:INFO: Epoch: 22, train loss: 2.002319769135543
2026-01-19 12:02:32,028:INFO: Epoch: 22, dev loss: 380.3286819458008, f1 score: 0.6266094420600858
2026-01-19 12:02:40,363:INFO: Epoch: 23, train loss: 2.560489215488945
2026-01-19 12:02:40,544:INFO: Epoch: 23, dev loss: 412.3539123535156, f1 score: 0.6293103448275862
2026-01-19 12:02:49,088:INFO: Epoch: 24, train loss: 1.9029712272541863
2026-01-19 12:02:49,268:INFO: Epoch: 24, dev loss: 405.6595458984375, f1 score: 0.6216216216216216
2026-01-19 12:02:49,268:INFO: Best val f1: 0.6812227074235807
2026-01-19 12:02:49,268:INFO: Training Finished!
2026-01-19 12:02:49,275:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:02:49,275:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:02:49,276:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:02:49,276:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:02:49,276:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:02:49,276:INFO: loading file None
2026-01-19 12:02:49,276:INFO: loading file None
2026-01-19 12:02:49,276:INFO: loading file None
2026-01-19 12:02:49,382:INFO: --------Dataset Build!--------
2026-01-19 12:02:49,382:INFO: --------Get Data-loader!--------
2026-01-19 12:02:49,382:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:02:49,382:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 12:02:49,383:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:02:56,122:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 12:02:56,123:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:02:56,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:02:56,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:02:56,124:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:02:56,124:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:02:56,124:INFO: loading file None
2026-01-19 12:02:56,124:INFO: loading file None
2026-01-19 12:02:56,124:INFO: loading file None
2026-01-19 12:02:56,721:INFO: --------Bad Cases reserved !--------
2026-01-19 12:02:56,727:INFO: test loss: 448.40289306640625, f1 score: 0.7103538663171691
2026-01-19 12:02:56,727:INFO: f1 score of ORG: 0.7835051546391752
2026-01-19 12:02:56,727:INFO: f1 score of ACTION: 0.6330935251798561
2026-01-19 12:02:56,727:INFO: f1 score of OBJ: 0.7184466019417477
2026-01-19 12:02:56,727:INFO: f1 score of LEVEL_KEY: 0.6867469879518072
2026-01-19 12:02:56,727:INFO: f1 score of VALUE: 0.7483870967741936
2026-01-19 12:05:17,563:INFO: device: cuda:0
2026-01-19 12:05:17,563:INFO: --------Process Done!--------
2026-01-19 12:05:17,570:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:05:17,570:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:05:17,570:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:05:17,570:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:05:17,570:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:05:17,570:INFO: loading file None
2026-01-19 12:05:17,570:INFO: loading file None
2026-01-19 12:05:17,570:INFO: loading file None
2026-01-19 12:05:17,919:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:05:17,920:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:05:17,920:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:05:17,920:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:05:17,920:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:05:17,920:INFO: loading file None
2026-01-19 12:05:17,920:INFO: loading file None
2026-01-19 12:05:17,920:INFO: loading file None
2026-01-19 12:05:17,966:INFO: --------Dataset Build!--------
2026-01-19 12:05:17,966:INFO: --------Get Dataloader!--------
2026-01-19 12:05:17,966:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 12:05:17,967:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 12:05:17,967:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 12:05:24,710:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 12:05:24,711:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 12:05:27,236:INFO: --------Start Training!--------
2026-01-19 12:06:19,063:INFO: device: cuda:0
2026-01-19 12:06:19,063:INFO: --------Process Done!--------
2026-01-19 12:06:19,070:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:06:19,070:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:06:19,070:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:06:19,070:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:06:19,070:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:06:19,071:INFO: loading file None
2026-01-19 12:06:19,071:INFO: loading file None
2026-01-19 12:06:19,071:INFO: loading file None
2026-01-19 12:06:19,427:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:06:19,428:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:06:19,428:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:06:19,428:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:06:19,428:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:06:19,428:INFO: loading file None
2026-01-19 12:06:19,428:INFO: loading file None
2026-01-19 12:06:19,428:INFO: loading file None
2026-01-19 12:06:19,475:INFO: --------Dataset Build!--------
2026-01-19 12:06:19,475:INFO: --------Get Dataloader!--------
2026-01-19 12:06:19,476:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 12:06:19,476:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 12:06:19,476:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 12:06:26,538:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 12:06:26,539:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 12:06:29,107:INFO: --------Start Training!--------
2026-01-19 12:06:38,026:INFO: Epoch: 1, train loss: 1913.819135393415
2026-01-19 12:06:38,196:INFO: Epoch: 1, dev loss: 911.451171875, f1 score: 0
2026-01-19 12:06:46,153:INFO: Epoch: 2, train loss: 1062.1150556291852
2026-01-19 12:06:46,322:INFO: Epoch: 2, dev loss: 467.1184997558594, f1 score: 0
2026-01-19 12:06:55,086:INFO: Epoch: 3, train loss: 605.6349705287388
2026-01-19 12:06:55,268:INFO: Epoch: 3, dev loss: 242.70022583007812, f1 score: 0.1830065359477124
2026-01-19 12:06:55,269:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:07:00,022:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:07:00,022:INFO: --------Save best model!--------
2026-01-19 12:07:08,758:INFO: Epoch: 4, train loss: 349.61126708984375
2026-01-19 12:07:08,943:INFO: Epoch: 4, dev loss: 175.4120330810547, f1 score: 0.3756906077348066
2026-01-19 12:07:08,944:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:07:13,681:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:07:13,682:INFO: --------Save best model!--------
2026-01-19 12:07:22,396:INFO: Epoch: 5, train loss: 213.82906886509485
2026-01-19 12:07:22,561:INFO: Epoch: 5, dev loss: 173.2167205810547, f1 score: 0.5427135678391961
2026-01-19 12:07:22,562:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:07:27,339:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:07:27,340:INFO: --------Save best model!--------
2026-01-19 12:07:36,156:INFO: Epoch: 6, train loss: 134.58125686645508
2026-01-19 12:07:36,326:INFO: Epoch: 6, dev loss: 169.90331268310547, f1 score: 0.6407766990291263
2026-01-19 12:07:36,328:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:07:41,045:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:07:41,046:INFO: --------Save best model!--------
2026-01-19 12:07:49,852:INFO: Epoch: 7, train loss: 132.5519676208496
2026-01-19 12:07:50,030:INFO: Epoch: 7, dev loss: 183.68061065673828, f1 score: 0.6204081632653061
2026-01-19 12:07:58,392:INFO: Epoch: 8, train loss: 87.8736207144601
2026-01-19 12:07:58,572:INFO: Epoch: 8, dev loss: 184.31214141845703, f1 score: 0.6181818181818182
2026-01-19 12:08:06,592:INFO: Epoch: 9, train loss: 68.24330384390694
2026-01-19 12:08:06,775:INFO: Epoch: 9, dev loss: 160.84893035888672, f1 score: 0.6912442396313364
2026-01-19 12:08:06,776:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:08:11,463:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:08:11,463:INFO: --------Save best model!--------
2026-01-19 12:08:19,791:INFO: Epoch: 10, train loss: 45.197611400059294
2026-01-19 12:08:19,970:INFO: Epoch: 10, dev loss: 204.02606964111328, f1 score: 0.6238532110091742
2026-01-19 12:08:28,821:INFO: Epoch: 11, train loss: 40.51812022072928
2026-01-19 12:08:29,004:INFO: Epoch: 11, dev loss: 150.0867156982422, f1 score: 0.6964285714285715
2026-01-19 12:08:29,005:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:08:33,706:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:08:33,706:INFO: --------Save best model!--------
2026-01-19 12:08:42,128:INFO: Epoch: 12, train loss: 29.582041467939103
2026-01-19 12:08:42,308:INFO: Epoch: 12, dev loss: 204.5364227294922, f1 score: 0.6511627906976745
2026-01-19 12:08:50,900:INFO: Epoch: 13, train loss: 18.113081778798783
2026-01-19 12:08:51,080:INFO: Epoch: 13, dev loss: 207.41943359375, f1 score: 0.6173913043478261
2026-01-19 12:08:59,417:INFO: Epoch: 14, train loss: 15.558937311172485
2026-01-19 12:08:59,599:INFO: Epoch: 14, dev loss: 233.30998992919922, f1 score: 0.6727272727272728
2026-01-19 12:09:08,394:INFO: Epoch: 15, train loss: 10.699941098690033
2026-01-19 12:09:08,577:INFO: Epoch: 15, dev loss: 219.58565521240234, f1 score: 0.6495726495726495
2026-01-19 12:09:17,181:INFO: Epoch: 16, train loss: 7.538937934807369
2026-01-19 12:09:17,358:INFO: Epoch: 16, dev loss: 246.7970962524414, f1 score: 0.6915887850467289
2026-01-19 12:09:25,995:INFO: Epoch: 17, train loss: 6.956690600940159
2026-01-19 12:09:26,163:INFO: Epoch: 17, dev loss: 231.30089569091797, f1 score: 0.6835443037974683
2026-01-19 12:09:34,746:INFO: Epoch: 18, train loss: 4.497567415237427
2026-01-19 12:09:34,930:INFO: Epoch: 18, dev loss: 299.6152648925781, f1 score: 0.6787330316742082
2026-01-19 12:09:43,307:INFO: Epoch: 19, train loss: 6.5418213829398155
2026-01-19 12:09:43,470:INFO: Epoch: 19, dev loss: 283.0609130859375, f1 score: 0.6608695652173913
2026-01-19 12:09:52,082:INFO: Epoch: 20, train loss: 5.304622646953378
2026-01-19 12:09:52,237:INFO: Epoch: 20, dev loss: 288.0315856933594, f1 score: 0.6607142857142857
2026-01-19 12:10:00,661:INFO: Epoch: 21, train loss: 2.8234888315200806
2026-01-19 12:10:00,826:INFO: Epoch: 21, dev loss: 310.4054718017578, f1 score: 0.6575342465753424
2026-01-19 12:10:00,826:INFO: Best val f1: 0.6964285714285715
2026-01-19 12:10:00,826:INFO: Training Finished!
2026-01-19 12:10:00,834:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:10:00,834:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:10:00,834:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:10:00,834:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:10:00,834:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:10:00,834:INFO: loading file None
2026-01-19 12:10:00,834:INFO: loading file None
2026-01-19 12:10:00,834:INFO: loading file None
2026-01-19 12:10:00,934:INFO: --------Dataset Build!--------
2026-01-19 12:10:00,934:INFO: --------Get Data-loader!--------
2026-01-19 12:10:00,934:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 12:10:00,934:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 12:10:00,935:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 12:10:07,689:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 12:10:07,691:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 12:10:07,691:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 12:10:07,691:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 12:10:07,691:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 12:10:07,691:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 12:10:07,691:INFO: loading file None
2026-01-19 12:10:07,691:INFO: loading file None
2026-01-19 12:10:07,691:INFO: loading file None
2026-01-19 12:10:08,296:INFO: --------Bad Cases reserved !--------
2026-01-19 12:10:08,302:INFO: test loss: 342.8876152038574, f1 score: 0.728476821192053
2026-01-19 12:10:08,302:INFO: f1 score of ORG: 0.8085106382978724
2026-01-19 12:10:08,302:INFO: f1 score of ACTION: 0.6153846153846153
2026-01-19 12:10:08,302:INFO: f1 score of OBJ: 0.71356783919598
2026-01-19 12:10:08,302:INFO: f1 score of LEVEL_KEY: 0.7030303030303031
2026-01-19 12:10:08,302:INFO: f1 score of VALUE: 0.8311688311688312
2026-01-19 15:22:27,605:INFO: device: cuda:0
2026-01-19 15:22:27,605:INFO: --------Process Done!--------
2026-01-19 15:22:27,612:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:22:27,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:22:27,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:22:27,612:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:22:27,613:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:22:27,613:INFO: loading file None
2026-01-19 15:22:27,613:INFO: loading file None
2026-01-19 15:22:27,613:INFO: loading file None
2026-01-19 15:22:27,959:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:22:27,959:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:22:27,959:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:22:27,959:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:22:27,959:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:22:27,959:INFO: loading file None
2026-01-19 15:22:27,960:INFO: loading file None
2026-01-19 15:22:27,960:INFO: loading file None
2026-01-19 15:22:28,005:INFO: --------Dataset Build!--------
2026-01-19 15:22:28,006:INFO: --------Get Dataloader!--------
2026-01-19 15:22:28,006:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 15:22:28,006:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 15:22:28,006:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 15:22:35,101:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 15:22:35,102:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 15:22:37,745:INFO: --------Start Training!--------
2026-01-19 15:25:11,342:INFO: device: cuda:0
2026-01-19 15:25:11,342:INFO: --------Process Done!--------
2026-01-19 15:25:11,349:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:25:11,349:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:25:11,349:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:25:11,350:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:25:11,350:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:25:11,350:INFO: loading file None
2026-01-19 15:25:11,350:INFO: loading file None
2026-01-19 15:25:11,350:INFO: loading file None
2026-01-19 15:25:11,699:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:25:11,699:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:25:11,699:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:25:11,699:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:25:11,700:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:25:11,700:INFO: loading file None
2026-01-19 15:25:11,700:INFO: loading file None
2026-01-19 15:25:11,700:INFO: loading file None
2026-01-19 15:25:11,746:INFO: --------Dataset Build!--------
2026-01-19 15:25:11,746:INFO: --------Get Dataloader!--------
2026-01-19 15:25:11,746:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 15:25:11,746:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 15:25:11,747:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 15:25:18,829:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 15:25:18,830:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 15:25:21,421:INFO: --------Start Training!--------
2026-01-19 15:25:40,115:INFO: Epoch: 1, train loss: 507.28536169869557
2026-01-19 15:31:10,556:INFO: device: cuda:0
2026-01-19 15:31:10,556:INFO: --------Process Done!--------
2026-01-19 15:31:10,563:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:31:10,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:31:10,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:31:10,563:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:31:10,563:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:31:10,563:INFO: loading file None
2026-01-19 15:31:10,563:INFO: loading file None
2026-01-19 15:31:10,563:INFO: loading file None
2026-01-19 15:31:10,905:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:31:10,905:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:31:10,905:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:31:10,906:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:31:10,906:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:31:10,906:INFO: loading file None
2026-01-19 15:31:10,906:INFO: loading file None
2026-01-19 15:31:10,906:INFO: loading file None
2026-01-19 15:31:10,952:INFO: --------Dataset Build!--------
2026-01-19 15:31:10,952:INFO: --------Get Dataloader!--------
2026-01-19 15:31:10,952:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 15:31:10,953:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 15:31:10,953:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 15:31:18,011:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 15:31:18,012:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 15:31:20,628:INFO: --------Start Training!--------
2026-01-19 15:31:39,311:INFO: Epoch: 1, train loss: 513.2466893877302
2026-01-19 15:31:39,657:INFO: Epoch: 1, dev loss: 387.89049857003346, f1 score: 0.002296211251435132
2026-01-19 15:31:39,659:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:31:44,366:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:31:44,366:INFO: --------Save best model!--------
2026-01-19 15:32:02,626:INFO: Epoch: 2, train loss: 428.91982133047924
2026-01-19 15:32:02,980:INFO: Epoch: 2, dev loss: 386.72084263392856, f1 score: 0.0022988505747126436
2026-01-19 15:32:20,827:INFO: Epoch: 3, train loss: 297.58590275900707
2026-01-19 15:32:21,177:INFO: Epoch: 3, dev loss: 383.9603609357561, f1 score: 0.002314814814814815
2026-01-19 15:32:21,178:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:32:25,844:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:32:25,844:INFO: --------Save best model!--------
2026-01-19 15:32:44,121:INFO: Epoch: 4, train loss: 217.01088769095284
2026-01-19 15:32:44,473:INFO: Epoch: 4, dev loss: 380.2114966256278, f1 score: 0
2026-01-19 15:33:02,663:INFO: Epoch: 5, train loss: 157.28046185629708
2026-01-19 15:33:03,002:INFO: Epoch: 5, dev loss: 376.0063912527902, f1 score: 0.002577319587628866
2026-01-19 15:33:03,003:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:33:07,596:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:33:07,596:INFO: --------Save best model!--------
2026-01-19 15:33:25,627:INFO: Epoch: 6, train loss: 112.13452584402901
2026-01-19 15:33:25,980:INFO: Epoch: 6, dev loss: 371.63041251046315, f1 score: 0.005738880918220946
2026-01-19 15:33:25,982:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:33:30,649:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:33:30,649:INFO: --------Save best model!--------
2026-01-19 15:33:48,346:INFO: Epoch: 7, train loss: 84.33805331162044
2026-01-19 15:33:48,701:INFO: Epoch: 7, dev loss: 367.14659336635043, f1 score: 0.00966183574879227
2026-01-19 15:33:48,702:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:33:53,414:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:33:53,414:INFO: --------Save best model!--------
2026-01-19 15:34:11,480:INFO: Epoch: 8, train loss: 63.67880909783499
2026-01-19 15:34:11,829:INFO: Epoch: 8, dev loss: 362.57743072509766, f1 score: 0.011131725417439705
2026-01-19 15:34:11,830:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:34:16,522:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:34:16,522:INFO: --------Save best model!--------
2026-01-19 15:34:34,633:INFO: Epoch: 9, train loss: 48.85616431917463
2026-01-19 15:34:34,994:INFO: Epoch: 9, dev loss: 357.9754551478795, f1 score: 0.015037593984962405
2026-01-19 15:34:34,994:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:34:39,729:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:34:39,729:INFO: --------Save best model!--------
2026-01-19 15:34:58,238:INFO: Epoch: 10, train loss: 39.81768461636135
2026-01-19 15:34:58,592:INFO: Epoch: 10, dev loss: 353.3068291800363, f1 score: 0.019292604501607715
2026-01-19 15:34:58,593:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:35:03,284:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:35:03,284:INFO: --------Save best model!--------
2026-01-19 15:35:21,451:INFO: Epoch: 11, train loss: 31.62427215065275
2026-01-19 15:35:21,801:INFO: Epoch: 11, dev loss: 348.5615997314453, f1 score: 0.00796812749003984
2026-01-19 15:35:39,976:INFO: Epoch: 12, train loss: 26.175064521176473
2026-01-19 15:35:40,324:INFO: Epoch: 12, dev loss: 343.7066214425223, f1 score: 0.02
2026-01-19 15:35:40,325:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:35:44,963:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:35:44,963:INFO: --------Save best model!--------
2026-01-19 15:36:03,171:INFO: Epoch: 13, train loss: 21.535531316484725
2026-01-19 15:36:03,518:INFO: Epoch: 13, dev loss: 338.7460675920759, f1 score: 0.02197802197802198
2026-01-19 15:36:03,520:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:36:08,219:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:36:08,219:INFO: --------Save best model!--------
2026-01-19 15:36:26,277:INFO: Epoch: 14, train loss: 19.351411014795303
2026-01-19 15:36:26,615:INFO: Epoch: 14, dev loss: 333.68670654296875, f1 score: 0.03870967741935484
2026-01-19 15:36:26,616:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:36:31,312:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:36:31,312:INFO: --------Save best model!--------
2026-01-19 15:36:49,459:INFO: Epoch: 15, train loss: 14.369769564696721
2026-01-19 15:36:49,805:INFO: Epoch: 15, dev loss: 328.53052466256275, f1 score: 0.04477611940298508
2026-01-19 15:36:49,806:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:36:54,485:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:36:54,486:INFO: --------Save best model!--------
2026-01-19 15:37:12,746:INFO: Epoch: 16, train loss: 11.856441725577627
2026-01-19 15:37:13,102:INFO: Epoch: 16, dev loss: 323.17332186017717, f1 score: 0.04651162790697675
2026-01-19 15:37:13,103:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:37:17,794:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:37:17,795:INFO: --------Save best model!--------
2026-01-19 15:37:35,883:INFO: Epoch: 17, train loss: 9.286848835647106
2026-01-19 15:37:36,225:INFO: Epoch: 17, dev loss: 317.6400037493025, f1 score: 0.06956521739130435
2026-01-19 15:37:36,226:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:37:40,902:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:37:40,902:INFO: --------Save best model!--------
2026-01-19 15:37:59,239:INFO: Epoch: 18, train loss: 8.193773540535144
2026-01-19 15:37:59,594:INFO: Epoch: 18, dev loss: 312.0813527788435, f1 score: 0.0925925925925926
2026-01-19 15:37:59,595:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:38:04,285:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:38:04,285:INFO: --------Save best model!--------
2026-01-19 15:38:22,650:INFO: Epoch: 19, train loss: 6.689416848123074
2026-01-19 15:38:22,989:INFO: Epoch: 19, dev loss: 306.31202915736606, f1 score: 0.0761904761904762
2026-01-19 15:38:41,487:INFO: Epoch: 20, train loss: 5.830308519303799
2026-01-19 15:38:41,825:INFO: Epoch: 20, dev loss: 300.50433131626676, f1 score: 0.07547169811320754
2026-01-19 15:39:00,015:INFO: Epoch: 21, train loss: 5.591859369472202
2026-01-19 15:39:00,362:INFO: Epoch: 21, dev loss: 294.5057967049735, f1 score: 0.07766990291262137
2026-01-19 15:39:18,313:INFO: Epoch: 22, train loss: 3.6377726016300067
2026-01-19 15:39:18,659:INFO: Epoch: 22, dev loss: 288.53595079694475, f1 score: 0.07547169811320754
2026-01-19 15:39:36,589:INFO: Epoch: 23, train loss: 3.2593151602361883
2026-01-19 15:39:36,939:INFO: Epoch: 23, dev loss: 282.4871935163225, f1 score: 0.07547169811320754
2026-01-19 15:39:55,505:INFO: Epoch: 24, train loss: 2.922180528885552
2026-01-19 15:39:55,855:INFO: Epoch: 24, dev loss: 276.2259826660156, f1 score: 0.07476635514018691
2026-01-19 15:40:14,207:INFO: Epoch: 25, train loss: 3.4346086409475123
2026-01-19 15:40:14,561:INFO: Epoch: 25, dev loss: 269.9155306134905, f1 score: 0.07339449541284403
2026-01-19 15:40:32,747:INFO: Epoch: 26, train loss: 3.1840666367539336
2026-01-19 15:40:33,086:INFO: Epoch: 26, dev loss: 263.4622562953404, f1 score: 0.0925925925925926
2026-01-19 15:40:50,851:INFO: Epoch: 27, train loss: 1.9744875339258994
2026-01-19 15:40:51,197:INFO: Epoch: 27, dev loss: 256.8094024658203, f1 score: 0.11009174311926606
2026-01-19 15:40:51,198:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:40:55,911:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:40:55,911:INFO: --------Save best model!--------
2026-01-19 15:41:14,255:INFO: Epoch: 28, train loss: 2.600063517689705
2026-01-19 15:41:14,607:INFO: Epoch: 28, dev loss: 250.02181679861886, f1 score: 0.12727272727272726
2026-01-19 15:41:14,608:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:41:19,296:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:41:19,297:INFO: --------Save best model!--------
2026-01-19 15:41:37,216:INFO: Epoch: 29, train loss: 2.560612349770963
2026-01-19 15:41:37,565:INFO: Epoch: 29, dev loss: 243.1097902570452, f1 score: 0.12612612612612614
2026-01-19 15:41:55,815:INFO: Epoch: 30, train loss: 2.488101222685405
2026-01-19 15:41:56,159:INFO: Epoch: 30, dev loss: 236.3010411943708, f1 score: 0.17543859649122806
2026-01-19 15:41:56,160:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:42:00,852:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:42:00,853:INFO: --------Save best model!--------
2026-01-19 15:42:19,326:INFO: Epoch: 31, train loss: 1.4041916579008102
2026-01-19 15:42:19,683:INFO: Epoch: 31, dev loss: 229.36895969935827, f1 score: 0.20869565217391306
2026-01-19 15:42:19,683:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:42:24,363:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:42:24,363:INFO: --------Save best model!--------
2026-01-19 15:42:42,057:INFO: Epoch: 32, train loss: 1.1307567033384527
2026-01-19 15:42:42,403:INFO: Epoch: 32, dev loss: 222.38804353986467, f1 score: 0.20689655172413793
2026-01-19 15:43:00,236:INFO: Epoch: 33, train loss: 1.3275339026004076
2026-01-19 15:43:00,592:INFO: Epoch: 33, dev loss: 215.33233315604073, f1 score: 0.2033898305084746
2026-01-19 15:43:18,672:INFO: Epoch: 34, train loss: 1.3211329217467989
2026-01-19 15:43:19,016:INFO: Epoch: 34, dev loss: 207.98224585396903, f1 score: 0.22033898305084745
2026-01-19 15:43:19,017:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:43:23,688:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:43:23,688:INFO: --------Save best model!--------
2026-01-19 15:43:42,087:INFO: Epoch: 35, train loss: 1.1181425915897958
2026-01-19 15:43:42,440:INFO: Epoch: 35, dev loss: 200.64854322160994, f1 score: 0.21666666666666665
2026-01-19 15:44:00,857:INFO: Epoch: 36, train loss: 0.9471392809812512
2026-01-19 15:44:01,212:INFO: Epoch: 36, dev loss: 193.35702460152763, f1 score: 0.2644628099173553
2026-01-19 15:44:01,213:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:44:05,862:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:44:05,863:INFO: --------Save best model!--------
2026-01-19 15:44:23,541:INFO: Epoch: 37, train loss: 1.2078309288647557
2026-01-19 15:44:23,891:INFO: Epoch: 37, dev loss: 186.17572893415178, f1 score: 0.29508196721311475
2026-01-19 15:44:23,892:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:44:28,603:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:44:28,603:INFO: --------Save best model!--------
2026-01-19 15:44:46,833:INFO: Epoch: 38, train loss: 0.9303247705767197
2026-01-19 15:44:47,194:INFO: Epoch: 38, dev loss: 178.73032706124442, f1 score: 0.2975206611570248
2026-01-19 15:44:47,195:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:44:51,864:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:44:51,865:INFO: --------Save best model!--------
2026-01-19 15:45:10,354:INFO: Epoch: 39, train loss: 1.0824542048919414
2026-01-19 15:45:10,704:INFO: Epoch: 39, dev loss: 171.42040307181222, f1 score: 0.3140495867768595
2026-01-19 15:45:10,705:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:45:15,404:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:45:15,405:INFO: --------Save best model!--------
2026-01-19 15:45:33,792:INFO: Epoch: 40, train loss: 1.7582140773800867
2026-01-19 15:45:34,132:INFO: Epoch: 40, dev loss: 164.42174421037947, f1 score: 0.30645161290322576
2026-01-19 15:45:52,331:INFO: Epoch: 41, train loss: 1.2262141673958726
2026-01-19 15:45:52,673:INFO: Epoch: 41, dev loss: 157.3449205671038, f1 score: 0.304
2026-01-19 15:46:10,927:INFO: Epoch: 42, train loss: 0.9882794246077538
2026-01-19 15:46:11,278:INFO: Epoch: 42, dev loss: 150.2567481994629, f1 score: 0.30158730158730157
2026-01-19 15:46:29,627:INFO: Epoch: 43, train loss: 0.5905556186168853
2026-01-19 15:46:29,972:INFO: Epoch: 43, dev loss: 143.30187933785575, f1 score: 0.2992125984251969
2026-01-19 15:46:48,307:INFO: Epoch: 44, train loss: 1.161020198537569
2026-01-19 15:46:48,651:INFO: Epoch: 44, dev loss: 136.5055422101702, f1 score: 0.304
2026-01-19 15:47:06,712:INFO: Epoch: 45, train loss: 1.1364443468794758
2026-01-19 15:47:07,061:INFO: Epoch: 45, dev loss: 130.062255859375, f1 score: 0.34146341463414637
2026-01-19 15:47:07,062:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:47:11,744:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:47:11,745:INFO: --------Save best model!--------
2026-01-19 15:47:30,257:INFO: Epoch: 46, train loss: 0.5269562392162958
2026-01-19 15:47:30,601:INFO: Epoch: 46, dev loss: 123.76265171595982, f1 score: 0.3709677419354838
2026-01-19 15:47:30,602:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:47:35,305:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:47:35,306:INFO: --------Save best model!--------
2026-01-19 15:47:53,805:INFO: Epoch: 47, train loss: 0.5363610418114279
2026-01-19 15:47:54,132:INFO: Epoch: 47, dev loss: 117.70301709856305, f1 score: 0.3870967741935484
2026-01-19 15:47:54,133:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:47:58,782:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:47:58,782:INFO: --------Save best model!--------
2026-01-19 15:48:16,847:INFO: Epoch: 48, train loss: 0.3992926653008908
2026-01-19 15:48:17,189:INFO: Epoch: 48, dev loss: 111.96380015781948, f1 score: 0.4032258064516129
2026-01-19 15:48:17,190:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:48:21,878:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:48:21,878:INFO: --------Save best model!--------
2026-01-19 15:48:39,682:INFO: Epoch: 49, train loss: 0.08039773939110871
2026-01-19 15:48:40,033:INFO: Epoch: 49, dev loss: 106.40157100132534, f1 score: 0.4193548387096774
2026-01-19 15:48:40,034:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:48:44,718:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:48:44,719:INFO: --------Save best model!--------
2026-01-19 15:49:02,835:INFO: Epoch: 50, train loss: 0.5583157252487061
2026-01-19 15:49:03,185:INFO: Epoch: 50, dev loss: 101.10782296316964, f1 score: 0.4193548387096774
2026-01-19 15:49:03,186:INFO: Best val f1: 0.4193548387096774
2026-01-19 15:49:03,186:INFO: Training Finished!
2026-01-19 15:49:03,193:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:49:03,194:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:49:03,194:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:49:03,194:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:49:03,194:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:49:03,194:INFO: loading file None
2026-01-19 15:49:03,194:INFO: loading file None
2026-01-19 15:49:03,194:INFO: loading file None
2026-01-19 15:49:03,292:INFO: --------Dataset Build!--------
2026-01-19 15:49:03,293:INFO: --------Get Data-loader!--------
2026-01-19 15:49:03,293:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:49:03,293:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 15:49:03,293:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:49:10,034:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 15:49:10,036:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:49:10,036:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:49:10,036:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:49:10,036:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:49:10,036:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:49:10,036:INFO: loading file None
2026-01-19 15:49:10,036:INFO: loading file None
2026-01-19 15:49:10,036:INFO: loading file None
2026-01-19 15:49:10,994:INFO: --------Bad Cases reserved !--------
2026-01-19 15:49:10,999:INFO: test loss: 198.8089771270752, f1 score: 0.6684210526315789
2026-01-19 15:49:10,999:INFO: f1 score of ORG: 0.6521739130434783
2026-01-19 15:49:10,999:INFO: f1 score of ACTION: 0.588235294117647
2026-01-19 15:49:10,999:INFO: f1 score of OBJ: 0.6226415094339622
2026-01-19 15:49:10,999:INFO: f1 score of LEVEL_KEY: 0.6987951807228916
2026-01-19 15:49:10,999:INFO: f1 score of VALUE: 0.7792207792207793
2026-01-19 15:50:30,037:INFO: device: cuda:0
2026-01-19 15:50:30,037:INFO: --------Process Done!--------
2026-01-19 15:50:30,044:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:50:30,044:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:50:30,044:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:50:30,044:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:50:30,044:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:50:30,044:INFO: loading file None
2026-01-19 15:50:30,044:INFO: loading file None
2026-01-19 15:50:30,044:INFO: loading file None
2026-01-19 15:50:30,380:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 15:50:30,380:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 15:50:30,380:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 15:50:30,380:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 15:50:30,380:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 15:50:30,380:INFO: loading file None
2026-01-19 15:50:30,380:INFO: loading file None
2026-01-19 15:50:30,380:INFO: loading file None
2026-01-19 15:50:30,425:INFO: --------Dataset Build!--------
2026-01-19 15:50:30,425:INFO: --------Get Dataloader!--------
2026-01-19 15:50:30,425:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 15:50:30,425:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 15:50:30,426:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 15:50:37,483:INFO: Weights of BertNER not initialized from pretrained model: ['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 15:50:37,483:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 15:50:40,122:INFO: --------Start Training!--------
2026-01-19 15:50:58,978:INFO: Epoch: 1, train loss: 528.1204196384975
2026-01-19 15:50:59,320:INFO: Epoch: 1, dev loss: 398.9715794154576, f1 score: 0
2026-01-19 15:51:17,956:INFO: Epoch: 2, train loss: 484.7882635934012
2026-01-19 15:51:18,306:INFO: Epoch: 2, dev loss: 398.3630174909319, f1 score: 0
2026-01-19 15:51:36,385:INFO: Epoch: 3, train loss: 403.20513589041576
2026-01-19 15:51:36,742:INFO: Epoch: 3, dev loss: 396.8496660505022, f1 score: 0
2026-01-19 15:51:55,299:INFO: Epoch: 4, train loss: 300.1456460952759
2026-01-19 15:51:55,634:INFO: Epoch: 4, dev loss: 394.29442814418246, f1 score: 0
2026-01-19 15:52:14,023:INFO: Epoch: 5, train loss: 232.22363662719727
2026-01-19 15:52:14,366:INFO: Epoch: 5, dev loss: 391.0471954345703, f1 score: 0
2026-01-19 15:52:32,032:INFO: Epoch: 6, train loss: 183.48332009996687
2026-01-19 15:52:32,378:INFO: Epoch: 6, dev loss: 387.5317927769252, f1 score: 0
2026-01-19 15:52:50,718:INFO: Epoch: 7, train loss: 143.9939501626151
2026-01-19 15:52:51,061:INFO: Epoch: 7, dev loss: 383.88529205322266, f1 score: 0
2026-01-19 15:53:09,495:INFO: Epoch: 8, train loss: 114.8895559651511
2026-01-19 15:53:09,842:INFO: Epoch: 8, dev loss: 380.199462890625, f1 score: 0
2026-01-19 15:53:27,882:INFO: Epoch: 9, train loss: 92.69840448243278
2026-01-19 15:53:28,218:INFO: Epoch: 9, dev loss: 376.50552041190014, f1 score: 0.003344481605351171
2026-01-19 15:53:28,219:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:53:32,878:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:53:32,879:INFO: --------Save best model!--------
2026-01-19 15:53:50,982:INFO: Epoch: 10, train loss: 77.244164620127
2026-01-19 15:53:51,331:INFO: Epoch: 10, dev loss: 372.77208055768693, f1 score: 0
2026-01-19 15:54:09,261:INFO: Epoch: 11, train loss: 63.112255232674734
2026-01-19 15:54:09,624:INFO: Epoch: 11, dev loss: 368.9899935041155, f1 score: 0
2026-01-19 15:54:27,772:INFO: Epoch: 12, train loss: 53.72445140566145
2026-01-19 15:54:28,119:INFO: Epoch: 12, dev loss: 365.1890324183873, f1 score: 0.008456659619450317
2026-01-19 15:54:28,120:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:54:32,820:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:54:32,821:INFO: --------Save best model!--------
2026-01-19 15:54:51,282:INFO: Epoch: 13, train loss: 43.41065256936209
2026-01-19 15:54:51,635:INFO: Epoch: 13, dev loss: 361.3579226902553, f1 score: 0.004830917874396135
2026-01-19 15:55:09,743:INFO: Epoch: 14, train loss: 36.887119378362385
2026-01-19 15:55:10,083:INFO: Epoch: 14, dev loss: 357.45997728620256, f1 score: 0.0171919770773639
2026-01-19 15:55:10,084:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:55:14,756:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:55:14,756:INFO: --------Save best model!--------
2026-01-19 15:55:32,702:INFO: Epoch: 15, train loss: 32.67674643227032
2026-01-19 15:55:33,046:INFO: Epoch: 15, dev loss: 353.5078342982701, f1 score: 0.012698412698412697
2026-01-19 15:55:50,990:INFO: Epoch: 16, train loss: 26.372308049883163
2026-01-19 15:55:51,336:INFO: Epoch: 16, dev loss: 349.49793025425504, f1 score: 0.03496503496503497
2026-01-19 15:55:51,337:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:55:56,006:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:55:56,006:INFO: --------Save best model!--------
2026-01-19 15:56:14,392:INFO: Epoch: 17, train loss: 22.87945710335459
2026-01-19 15:56:14,741:INFO: Epoch: 17, dev loss: 345.4396531241281, f1 score: 0.05622489959839358
2026-01-19 15:56:14,742:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:56:19,425:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:56:19,426:INFO: --------Save best model!--------
2026-01-19 15:56:37,768:INFO: Epoch: 18, train loss: 20.122964590787888
2026-01-19 15:56:38,132:INFO: Epoch: 18, dev loss: 341.31582423618863, f1 score: 0.07792207792207793
2026-01-19 15:56:38,133:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:56:42,791:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:56:42,792:INFO: --------Save best model!--------
2026-01-19 15:57:01,518:INFO: Epoch: 19, train loss: 18.35687536426953
2026-01-19 15:57:01,882:INFO: Epoch: 19, dev loss: 337.115243094308, f1 score: 0.11214953271028039
2026-01-19 15:57:01,883:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:57:06,601:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:57:06,602:INFO: --------Save best model!--------
2026-01-19 15:57:24,471:INFO: Epoch: 20, train loss: 15.905404044049126
2026-01-19 15:57:24,826:INFO: Epoch: 20, dev loss: 332.80694362095426, f1 score: 0.15789473684210525
2026-01-19 15:57:24,827:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:57:29,503:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:57:29,503:INFO: --------Save best model!--------
2026-01-19 15:57:47,694:INFO: Epoch: 21, train loss: 12.669294278536524
2026-01-19 15:57:48,037:INFO: Epoch: 21, dev loss: 328.40352303641185, f1 score: 0.1807909604519774
2026-01-19 15:57:48,038:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:57:52,690:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:57:52,691:INFO: --------Save best model!--------
2026-01-19 15:58:10,729:INFO: Epoch: 22, train loss: 10.615131586790085
2026-01-19 15:58:11,079:INFO: Epoch: 22, dev loss: 323.9162701198033, f1 score: 0.20987654320987653
2026-01-19 15:58:11,080:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:58:15,756:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:58:15,756:INFO: --------Save best model!--------
2026-01-19 15:58:33,837:INFO: Epoch: 23, train loss: 9.008769137518746
2026-01-19 15:58:34,187:INFO: Epoch: 23, dev loss: 319.32521275111606, f1 score: 0.24203821656050958
2026-01-19 15:58:34,188:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:58:38,869:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:58:38,869:INFO: --------Save best model!--------
2026-01-19 15:58:57,249:INFO: Epoch: 24, train loss: 8.184013053774834
2026-01-19 15:58:57,609:INFO: Epoch: 24, dev loss: 314.6822422572545, f1 score: 0.2564102564102564
2026-01-19 15:58:57,609:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:59:02,317:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:59:02,318:INFO: --------Save best model!--------
2026-01-19 15:59:20,393:INFO: Epoch: 25, train loss: 6.785013337220464
2026-01-19 15:59:20,752:INFO: Epoch: 25, dev loss: 309.9190586635045, f1 score: 0.26666666666666666
2026-01-19 15:59:20,753:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:59:25,406:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:59:25,406:INFO: --------Save best model!--------
2026-01-19 15:59:43,621:INFO: Epoch: 26, train loss: 6.79032268588032
2026-01-19 15:59:43,958:INFO: Epoch: 26, dev loss: 305.00063868931363, f1 score: 0.29530201342281875
2026-01-19 15:59:43,959:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 15:59:48,648:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 15:59:48,648:INFO: --------Save best model!--------
2026-01-19 16:00:07,174:INFO: Epoch: 27, train loss: 4.877457942281451
2026-01-19 16:00:07,517:INFO: Epoch: 27, dev loss: 299.8624883379255, f1 score: 0.3013698630136986
2026-01-19 16:00:07,518:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:00:12,213:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:00:12,213:INFO: --------Save best model!--------
2026-01-19 16:00:30,309:INFO: Epoch: 28, train loss: 5.611779875521149
2026-01-19 16:00:30,657:INFO: Epoch: 28, dev loss: 294.68900626046315, f1 score: 0.3076923076923077
2026-01-19 16:00:30,658:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:00:35,324:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:00:35,325:INFO: --------Save best model!--------
2026-01-19 16:00:53,305:INFO: Epoch: 29, train loss: 4.071101217397621
2026-01-19 16:00:53,651:INFO: Epoch: 29, dev loss: 289.404532296317, f1 score: 0.3150684931506849
2026-01-19 16:00:53,652:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:00:58,319:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:00:58,319:INFO: --------Save best model!--------
2026-01-19 16:01:16,759:INFO: Epoch: 30, train loss: 2.96568649155753
2026-01-19 16:01:17,104:INFO: Epoch: 30, dev loss: 284.0854612077986, f1 score: 0.3150684931506849
2026-01-19 16:01:35,439:INFO: Epoch: 31, train loss: 3.1481613256037235
2026-01-19 16:01:35,783:INFO: Epoch: 31, dev loss: 278.6908264160156, f1 score: 0.2987012987012987
2026-01-19 16:01:54,142:INFO: Epoch: 32, train loss: 4.028502081653902
2026-01-19 16:01:54,491:INFO: Epoch: 32, dev loss: 273.1835414341518, f1 score: 0.2987012987012987
2026-01-19 16:02:12,661:INFO: Epoch: 33, train loss: 2.3408888639616117
2026-01-19 16:02:13,003:INFO: Epoch: 33, dev loss: 267.64312526157926, f1 score: 0.2967741935483871
2026-01-19 16:02:31,489:INFO: Epoch: 34, train loss: 2.049142130251442
2026-01-19 16:02:31,831:INFO: Epoch: 34, dev loss: 262.0013743809291, f1 score: 0.29113924050632906
2026-01-19 16:02:50,231:INFO: Epoch: 35, train loss: 3.669613839792354
2026-01-19 16:02:50,582:INFO: Epoch: 35, dev loss: 256.1664777483259, f1 score: 0.2987012987012987
2026-01-19 16:03:08,535:INFO: Epoch: 36, train loss: 2.9408898068858043
2026-01-19 16:03:08,886:INFO: Epoch: 36, dev loss: 250.27134377615792, f1 score: 0.2987012987012987
2026-01-19 16:03:26,977:INFO: Epoch: 37, train loss: 2.3857645012571345
2026-01-19 16:03:27,327:INFO: Epoch: 37, dev loss: 244.17962428501673, f1 score: 0.2948717948717949
2026-01-19 16:03:45,087:INFO: Epoch: 38, train loss: 2.4584190946604525
2026-01-19 16:03:45,433:INFO: Epoch: 38, dev loss: 237.91668156215124, f1 score: 0.2987012987012987
2026-01-19 16:04:03,405:INFO: Epoch: 39, train loss: 2.3231750186532736
2026-01-19 16:04:03,766:INFO: Epoch: 39, dev loss: 231.64524841308594, f1 score: 0.3137254901960784
2026-01-19 16:04:03,766:INFO: Best val f1: 0.3150684931506849
2026-01-19 16:04:03,766:INFO: Training Finished!
2026-01-19 16:04:03,777:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 16:04:03,777:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 16:04:03,777:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 16:04:03,777:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 16:04:03,777:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 16:04:03,778:INFO: loading file None
2026-01-19 16:04:03,778:INFO: loading file None
2026-01-19 16:04:03,778:INFO: loading file None
2026-01-19 16:04:03,878:INFO: --------Dataset Build!--------
2026-01-19 16:04:03,878:INFO: --------Get Data-loader!--------
2026-01-19 16:04:03,878:INFO: loading configuration file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:04:03,878:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 16:04:03,879:INFO: loading weights file /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:04:10,636:INFO: --------Load model from /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/--------
2026-01-19 16:04:10,637:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 16:04:10,637:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 16:04:10,637:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 16:04:10,637:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 16:04:10,638:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 16:04:10,638:INFO: loading file None
2026-01-19 16:04:10,638:INFO: loading file None
2026-01-19 16:04:10,638:INFO: loading file None
2026-01-19 16:04:11,616:INFO: --------Bad Cases reserved !--------
2026-01-19 16:04:11,622:INFO: test loss: 115.20064783096313, f1 score: 0.7002652519893899
2026-01-19 16:04:11,622:INFO: f1 score of ORG: 0.7659574468085107
2026-01-19 16:04:11,622:INFO: f1 score of ACTION: 0.6363636363636364
2026-01-19 16:04:11,622:INFO: f1 score of OBJ: 0.6796116504854369
2026-01-19 16:04:11,622:INFO: f1 score of LEVEL_KEY: 0.6987951807228916
2026-01-19 16:04:11,622:INFO: f1 score of VALUE: 0.7435897435897436
2026-01-19 16:24:15,137:INFO: device: cuda:0
2026-01-19 16:24:15,138:WARNING: Fixed: 水库主流长
2026-01-19 16:24:15,138:WARNING: Fixed: 集雨面积
2026-01-19 16:24:15,138:WARNING: Fixed: 8km
2026-01-19 16:24:15,138:WARNING: Fixed: 20.67km2
2026-01-19 16:24:15,138:WARNING: Fixed: 观测站
2026-01-19 16:24:15,138:WARNING: Fixed: 水情调度中心
2026-01-19 16:24:15,138:WARNING: Fixed: 平江县防汛抗旱指挥部
2026-01-19 16:24:15,138:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,138:WARNING: Fixed: 审批
2026-01-19 16:24:15,138:WARNING: Fixed: 库水位
2026-01-19 16:24:15,138:WARNING: Fixed: 取水闸
2026-01-19 16:24:15,138:WARNING: Fixed: 死水位
2026-01-19 16:24:15,138:WARNING: Fixed: 85.0m
2026-01-19 16:24:15,138:WARNING: Fixed: 开启
2026-01-19 16:24:15,139:WARNING: Fixed: 调度
2026-01-19 16:24:15,139:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-19 16:24:15,139:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,139:WARNING: Fixed: 汨罗江流域
2026-01-19 16:24:15,139:WARNING: Fixed: 水库控制集雨面积
2026-01-19 16:24:15,139:WARNING: Fixed: 水库总库容
2026-01-19 16:24:15,139:WARNING: Fixed: 正常蓄水位
2026-01-19 16:24:15,139:WARNING: Fixed: 20 年一遇设计洪水位
2026-01-19 16:24:15,139:WARNING: Fixed: 200年一遇校核洪水位
2026-01-19 16:24:15,139:WARNING: Fixed: 死水位
2026-01-19 16:24:15,139:WARNING: Fixed: 1.34 km²
2026-01-19 16:24:15,139:WARNING: Fixed: 44.30 万m³
2026-01-19 16:24:15,139:WARNING: Fixed: 97.48 m
2026-01-19 16:24:15,139:WARNING: Fixed: 97.21 m
2026-01-19 16:24:15,139:WARNING: Fixed: 97.51 m
2026-01-19 16:24:15,139:WARNING: Fixed: 89.36 m
2026-01-19 16:24:15,139:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,139:WARNING: Fixed: 库水位
2026-01-19 16:24:15,139:WARNING: Fixed: 64.73～60.00m
2026-01-19 16:24:15,139:WARNING: Fixed:  64.73～62.00m 
2026-01-19 16:24:15,139:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,140:WARNING: Fixed: 浦阳江
2026-01-19 16:24:15,140:WARNING: Fixed: 坝底海拔高程
2026-01-19 16:24:15,140:WARNING: Fixed: 海拔高程
2026-01-19 16:24:15,140:WARNING: Fixed: 自然落差
2026-01-19 16:24:15,140:WARNING: Fixed: 318.12m
2026-01-19 16:24:15,140:WARNING: Fixed: 70.12m
2026-01-19 16:24:15,140:WARNING: Fixed: 248m
2026-01-19 16:24:15,140:WARNING: Fixed: 水位
2026-01-19 16:24:15,140:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,140:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,140:WARNING: Fixed: 防洪特征水位
2026-01-19 16:24:15,140:WARNING: Fixed: 集雨面积
2026-01-19 16:24:15,140:WARNING: Fixed: 集雨面积
2026-01-19 16:24:15,140:WARNING: Fixed: 控制泄量
2026-01-19 16:24:15,140:WARNING: Fixed: 56.5km²
2026-01-19 16:24:15,140:WARNING: Fixed: 47.97km²
2026-01-19 16:24:15,140:WARNING: Fixed: 300m³/s
2026-01-19 16:24:15,140:WARNING: Fixed: 水库库水位
2026-01-19 16:24:15,140:WARNING: Fixed: 库内水位
2026-01-19 16:24:15,140:WARNING: Fixed: 96.39 m
2026-01-19 16:24:15,140:WARNING: Fixed:  50 （mm）
2026-01-19 16:24:15,140:WARNING: Fixed: 95.39 m
2026-01-19 16:24:15,140:WARNING: Fixed: 预报降雨
2026-01-19 16:24:15,141:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,141:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,141:WARNING: Fixed: 62.99m
2026-01-19 16:24:15,141:WARNING: Fixed:  300m³/s 
2026-01-19 16:24:15,141:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,141:WARNING: Fixed: 62.99～64.73m
2026-01-19 16:24:15,141:WARNING: Fixed: 下游下王控制断面安全泄量
2026-01-19 16:24:15,141:WARNING: Fixed: 最大下泄流量
2026-01-19 16:24:15,141:WARNING: Fixed: 关闸蓄洪
2026-01-19 16:24:15,141:WARNING: Fixed: 水库管理及水库调度
2026-01-19 16:24:15,141:WARNING: Fixed: 妥善保管水库调度运行有关资料并归档
2026-01-19 16:24:15,141:WARNING: Fixed: 上级主管部门
2026-01-19 16:24:15,141:WARNING: Fixed: 库水位
2026-01-19 16:24:15,141:WARNING: Fixed: 水库
2026-01-19 16:24:15,141:WARNING: Fixed:  60.48m
2026-01-19 16:24:15,141:WARNING: Fixed: 60.68m
2026-01-19 16:24:15,141:WARNING: Fixed: 关闸蓄洪
2026-01-19 16:24:15,141:WARNING: Fixed: 控制水位
2026-01-19 16:24:15,141:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,141:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,142:WARNING: Fixed: 水位
2026-01-19 16:24:15,142:WARNING: Fixed: 紧急水位
2026-01-19 16:24:15,142:WARNING: Fixed: 指挥所全体人员及物资进入现场
2026-01-19 16:24:15,142:WARNING: Fixed: 并通知下游作好防洪避险准备
2026-01-19 16:24:15,142:WARNING: Fixed: 库水位
2026-01-19 16:24:15,142:WARNING: Fixed: 150m
2026-01-19 16:24:15,142:WARNING: Fixed: 155m
2026-01-19 16:24:15,142:WARNING: Fixed: 采取错峰调度
2026-01-19 16:24:15,142:WARNING: Fixed: 水库
2026-01-19 16:24:15,142:WARNING: Fixed: 防洪调度
2026-01-19 16:24:15,142:WARNING: Fixed: 兴利调度
2026-01-19 16:24:15,142:WARNING: Fixed: 梅汛期限制水位
2026-01-19 16:24:15,142:WARNING: Fixed: 台汛期限制水位
2026-01-19 16:24:15,142:WARNING: Fixed: 批复水位
2026-01-19 16:24:15,142:WARNING: Fixed: 非汛期正常蓄水位
2026-01-19 16:24:15,142:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,142:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,142:WARNING: Fixed: 62.00m
2026-01-19 16:24:15,142:WARNING: Fixed: 防汛抗旱指挥机构
2026-01-19 16:24:15,142:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,143:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,143:WARNING: Fixed: 下泄流量
2026-01-19 16:24:15,143:WARNING: Fixed: 设计洪水位
2026-01-19 16:24:15,143:WARNING: Fixed: 天然洪峰
2026-01-19 16:24:15,143:WARNING: Fixed: 汛限水位
2026-01-19 16:24:15,143:WARNING: Fixed: 泄洪
2026-01-19 16:24:15,143:WARNING: Fixed: 农业灌溉
2026-01-19 16:24:15,143:WARNING: Fixed: 启动抗旱应急调度
2026-01-19 16:24:15,143:WARNING: Fixed: 正常蓄水位
2026-01-19 16:24:15,143:WARNING: Fixed: 105.00m
2026-01-19 16:24:15,143:WARNING: Fixed: 水库流域
2026-01-19 16:24:15,143:WARNING: Fixed: 钱塘江流域
2026-01-19 16:24:15,143:WARNING: Fixed: 壶源江
2026-01-19 16:24:15,143:WARNING: Fixed: 库岭溪
2026-01-19 16:24:15,143:WARNING: Fixed: 富春江
2026-01-19 16:24:15,144:WARNING: Fixed: 海拔
2026-01-19 16:24:15,144:WARNING: Fixed: 流域面积
2026-01-19 16:24:15,144:WARNING: Fixed: 流域面积
2026-01-19 16:24:15,144:WARNING: Fixed: 818m
2026-01-19 16:24:15,144:WARNING: Fixed: 760.9km2
2026-01-19 16:24:15,147:WARNING: Fixed: 383.1km2
2026-01-19 16:24:15,147:WARNING: Fixed: 雨量站
2026-01-19 16:24:15,147:WARNING: Fixed: 鹤山市气象站
2026-01-19 16:24:15,147:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,147:WARNING: Fixed: 金坑岭水库
2026-01-19 16:24:15,147:WARNING: Fixed: 年平均供水总量
2026-01-19 16:24:15,147:WARNING: Fixed:  868 万 m3
2026-01-19 16:24:15,147:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,147:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,147:WARNING: Fixed:  20 年一遇洪水位
2026-01-19 16:24:15,147:WARNING: Fixed: 水库最大下泄流量
2026-01-19 16:24:15,147:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,148:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,148:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,148:WARNING: Fixed: 200年一遇校核洪水位
2026-01-19 16:24:15,148:WARNING: Fixed:  200 年一遇洪水位
2026-01-19 16:24:15,148:WARNING: Fixed: 165.82m
2026-01-19 16:24:15,148:WARNING: Fixed: 165.82m
2026-01-19 16:24:15,148:WARNING: Fixed: 水库及下游抗洪抢险工作
2026-01-19 16:24:15,148:WARNING: Fixed: 址山镇三防指挥
2026-01-19 16:24:15,148:WARNING: Fixed: 址山镇三防指挥所
2026-01-19 16:24:15,148:WARNING: Fixed: 市三防指挥部
2026-01-19 16:24:15,148:WARNING: Fixed: 调度运行计划
2026-01-19 16:24:15,148:WARNING: Fixed: 备案
2026-01-19 16:24:15,148:WARNING: Fixed: 水利局
2026-01-19 16:24:15,148:WARNING: Fixed: 省防指
2026-01-19 16:24:15,148:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-19 16:24:15,148:WARNING: Fixed: 水情中心站
2026-01-19 16:24:15,148:WARNING: Fixed: 水情中心站
2026-01-19 16:24:15,148:WARNING: Fixed: 遥测站
2026-01-19 16:24:15,148:WARNING: Fixed: 水库调度管理
2026-01-19 16:24:15,148:WARNING: Fixed: 水库险情监测与巡视检查
2026-01-19 16:24:15,148:WARNING: Fixed: 抢险
2026-01-19 16:24:15,149:WARNING: Fixed: 应急调度
2026-01-19 16:24:15,149:WARNING: Fixed: 信息报告
2026-01-19 16:24:15,149:WARNING: Fixed: 参与预案的全过程
2026-01-19 16:24:15,149:WARNING: Fixed: 参与应急会商
2026-01-19 16:24:15,149:WARNING: Fixed: 成应急指挥机构交办的任务
2026-01-19 16:24:15,149:WARNING: Fixed: 址山镇三防指挥所
2026-01-19 16:24:15,149:WARNING: Fixed: 奉化区水文站
2026-01-19 16:24:15,149:WARNING: Fixed: 宁波市葛岙水库开发有限公司
2026-01-19 16:24:15,149:WARNING: Fixed: 宁波市水利水电规划设计研究院有限公司
2026-01-19 16:24:15,149:WARNING: Fixed: 浙江省正邦水电建设有限公司
2026-01-19 16:24:15,149:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,149:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,149:WARNING: Fixed: 水库库区
2026-01-19 16:24:15,149:WARNING: Fixed: 水库枢纽
2026-01-19 16:24:15,149:WARNING: Fixed: 水库保护范围面积
2026-01-19 16:24:15,149:WARNING: Fixed: 0.792km2
2026-01-19 16:24:15,149:WARNING: Fixed: 下泄流量
2026-01-19 16:24:15,149:WARNING: Fixed: 下游城镇防洪标准
2026-01-19 16:24:15,149:WARNING: Fixed: 库水位
2026-01-19 16:24:15,149:WARNING: Fixed: 库容
2026-01-19 16:24:15,150:WARNING: Fixed: 60.00～62.00m
2026-01-19 16:24:15,150:WARNING: Fixed: 重叠库容
2026-01-19 16:24:15,150:WARNING: Fixed: 水库
2026-01-19 16:24:15,150:WARNING: Fixed: 水库
2026-01-19 16:24:15,150:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,150:WARNING: Fixed: 正常水位
2026-01-19 16:24:15,150:WARNING: Fixed:  163m
2026-01-19 16:24:15,150:WARNING: Fixed: 设计洪水标准
2026-01-19 16:24:15,150:WARNING: Fixed: 校核洪水标准
2026-01-19 16:24:15,150:WARNING: Fixed: 白云水库
2026-01-19 16:24:15,150:WARNING: Fixed: 大坝
2026-01-19 16:24:15,150:WARNING: Fixed: 放水涵管
2026-01-19 16:24:15,150:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,150:WARNING: Fixed: 防洪调度任务
2026-01-19 16:24:15,150:WARNING: Fixed: 水库大坝
2026-01-19 16:24:15,150:WARNING: Fixed: 20年一遇设计洪水标准
2026-01-19 16:24:15,150:WARNING: Fixed: 200年一遇的校核洪水标准
2026-01-19 16:24:15,151:WARNING: Fixed: 水库防洪控制断面
2026-01-19 16:24:15,151:WARNING: Fixed: 安全泄量
2026-01-19 16:24:15,151:WARNING: Fixed: 10.75*1.7m
2026-01-19 16:24:15,151:WARNING: Fixed: 15.72   m3/s
2026-01-19 16:24:15,151:WARNING: Fixed: 库水位
2026-01-19 16:24:15,151:WARNING: Fixed: 水库
2026-01-19 16:24:15,151:WARNING: Fixed: 61.22m
2026-01-19 16:24:15,151:WARNING: Fixed: 加大下泄流量
2026-01-19 16:24:15,151:WARNING: Fixed: 敞泄
2026-01-19 16:24:15,151:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,151:WARNING: Fixed: 启动
2026-01-19 16:24:15,151:WARNING: Fixed: 气象台
2026-01-19 16:24:15,151:WARNING: Fixed: 水库管理处
2026-01-19 16:24:15,151:WARNING: Fixed: 100mm
2026-01-19 16:24:15,151:WARNING: Fixed: 大坝
2026-01-19 16:24:15,151:WARNING: Fixed: 巡视检查
2026-01-19 16:24:15,151:WARNING: Fixed: 库水位
2026-01-19 16:24:15,151:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,151:WARNING: Fixed: 大坝
2026-01-19 16:24:15,151:WARNING: Fixed: 72.50m
2026-01-19 16:24:15,151:WARNING: Fixed: 自由溢流
2026-01-19 16:24:15,152:WARNING: Fixed: 三市镇
2026-01-19 16:24:15,152:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,152:WARNING: Fixed: 审批
2026-01-19 16:24:15,152:WARNING: Fixed: 协调
2026-01-19 16:24:15,152:WARNING: Fixed: 最高水位
2026-01-19 16:24:15,152:WARNING: Fixed: 1000年一遇校核洪水位
2026-01-19 16:24:15,152:WARNING: Fixed: 66.33m
2026-01-19 16:24:15,152:WARNING: Fixed: 水库
2026-01-19 16:24:15,152:WARNING: Fixed: 水位
2026-01-19 16:24:15,152:WARNING: Fixed: 水底孔
2026-01-19 16:24:15,152:WARNING: Fixed: 165.50m
2026-01-19 16:24:15,152:WARNING: Fixed: 开启
2026-01-19 16:24:15,152:WARNING: Fixed: 排沙
2026-01-19 16:24:15,152:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,152:WARNING: Fixed: 水库
2026-01-19 16:24:15,152:WARNING: Fixed: 库水位
2026-01-19 16:24:15,152:WARNING: Fixed: 校核洪水标
2026-01-19 16:24:15,152:WARNING: Fixed: 66.33m
2026-01-19 16:24:15,152:WARNING: Fixed: 泄洪
2026-01-19 16:24:15,152:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,152:WARNING: Fixed: 防限水位
2026-01-19 16:24:15,152:WARNING: Fixed: 163m
2026-01-19 16:24:15,152:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,152:WARNING: Fixed: 每隔一小时将水库水位、降雨量等情况报告
2026-01-19 16:24:15,153:WARNING: Fixed: 汛期水位
2026-01-19 16:24:15,153:WARNING: Fixed: 355.12m 
2026-01-19 16:24:15,153:WARNING: Fixed: 鹤山市址山镇农业综合服务中心
2026-01-19 16:24:15,153:WARNING: Fixed: 大坝、输水涵管和溢洪道
2026-01-19 16:24:15,153:WARNING: Fixed: 抢险
2026-01-19 16:24:15,153:WARNING: Fixed: 采取切实有效的保坝措施
2026-01-19 16:24:15,153:WARNING: Fixed: 防洪限制水位
2026-01-19 16:24:15,153:WARNING: Fixed: 96.39m
2026-01-19 16:24:15,153:WARNING: Fixed: 万安水库
2026-01-19 16:24:15,153:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,153:WARNING: Fixed: 防洪限制水位
2026-01-19 16:24:15,153:WARNING: Fixed: 145.0m
2026-01-19 16:24:15,153:WARNING: Fixed: 市水务局
2026-01-19 16:24:15,153:WARNING: Fixed: 下达调度令
2026-01-19 16:24:15,153:WARNING: Fixed: 开启
2026-01-19 16:24:15,153:WARNING: Fixed: 长山水库
2026-01-19 16:24:15,153:WARNING: Fixed: 1500
2026-01-19 16:24:15,153:WARNING: Fixed: 2000
2026-01-19 16:24:15,153:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,153:WARNING: Fixed: 金坑岭水库
2026-01-19 16:24:15,153:WARNING: Fixed: 壶源江电站
2026-01-19 16:24:15,154:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,154:WARNING: Fixed: 下游下王控制断面安全泄量
2026-01-19 16:24:15,154:WARNING: Fixed: 水库最大下泄流量
2026-01-19 16:24:15,154:WARNING: Fixed: 300m³/s
2026-01-19 16:24:15,154:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,154:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,154:WARNING: Fixed: 20年一遇设计洪水位
2026-01-19 16:24:15,154:WARNING: Fixed: 200年一遇校核洪水位
2026-01-19 16:24:15,154:WARNING: Fixed: 溢洪水深
2026-01-19 16:24:15,154:WARNING: Fixed: 165.20m
2026-01-19 16:24:15,154:WARNING: Fixed: 165.82m
2026-01-19 16:24:15,154:WARNING: Fixed: 2.82m
2026-01-19 16:24:15,154:WARNING: Fixed: 水库防汛工作
2026-01-19 16:24:15,154:WARNING: Fixed: 址山镇三防指挥
2026-01-19 16:24:15,154:WARNING: Fixed: 金坑岭水库
2026-01-19 16:24:15,154:WARNING: Fixed: 库水位
2026-01-19 16:24:15,154:WARNING: Fixed: 最低运行水位
2026-01-19 16:24:15,154:WARNING: Fixed: 324.92m
2026-01-19 16:24:15,154:WARNING: Fixed:  338.12m
2026-01-19 16:24:15,154:WARNING: Fixed: 停止发电
2026-01-19 16:24:15,154:WARNING: Fixed: 坝址
2026-01-19 16:24:15,154:WARNING: Fixed: 防洪控制点
2026-01-19 16:24:15,154:WARNING: Fixed: 水库下泄流量
2026-01-19 16:24:15,154:WARNING: Fixed: 固定泄量调度
2026-01-19 16:24:15,155:WARNING: Fixed: 累计降雨量
2026-01-19 16:24:15,155:WARNING: Fixed: 1小时降雨量
2026-01-19 16:24:15,155:WARNING: Fixed: 降雨量
2026-01-19 16:24:15,155:WARNING: Fixed: 100mm
2026-01-19 16:24:15,155:WARNING: Fixed: 50mm
2026-01-19 16:24:15,155:WARNING: Fixed: 30mm
2026-01-19 16:24:15,155:WARNING: Fixed: 在30分钟内将降雨情况报告
2026-01-19 16:24:15,155:WARNING: Fixed: 加报一次
2026-01-19 16:24:15,155:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,155:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,155:WARNING: Fixed: 汛限水位
2026-01-19 16:24:15,155:WARNING: Fixed: 65.5m
2026-01-19 16:24:15,155:WARNING: Fixed: 镇农办
2026-01-19 16:24:15,155:WARNING: Fixed: 汇报
2026-01-19 16:24:15,155:WARNING: Fixed: 梅汛期限制水位
2026-01-19 16:24:15,155:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,155:WARNING: Fixed: 60.00m 
2026-01-19 16:24:15,155:WARNING: Fixed: 库水位
2026-01-19 16:24:15,155:WARNING: Fixed: 下泄
2026-01-19 16:24:15,155:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,155:WARNING: Fixed: 东江河道
2026-01-19 16:24:15,155:WARNING: Fixed: 汛期水位
2026-01-19 16:24:15,155:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,156:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,156:WARNING: Fixed: 发电放水
2026-01-19 16:24:15,156:WARNING: Fixed: 自由泄洪
2026-01-19 16:24:15,156:WARNING: Fixed: 水位
2026-01-19 16:24:15,156:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,156:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,156:WARNING: Fixed: 坝址
2026-01-19 16:24:15,156:WARNING: Fixed: 防洪控制点
2026-01-19 16:24:15,156:WARNING: Fixed: 防洪控制点洪水
2026-01-19 16:24:15,156:WARNING: Fixed: 补偿调度
2026-01-19 16:24:15,156:WARNING: Fixed: 坝体
2026-01-19 16:24:15,156:WARNING: Fixed: 坝基
2026-01-19 16:24:15,156:WARNING: Fixed: 坝肩渗漏点
2026-01-19 16:24:15,156:WARNING: Fixed: 输水涵管
2026-01-19 16:24:15,156:WARNING: Fixed: 定期进行重点巡视检查
2026-01-19 16:24:15,156:WARNING: Fixed: 防洪调度
2026-01-19 16:24:15,156:WARNING: Fixed: 奉化区水利局
2026-01-19 16:24:15,156:WARNING: Fixed: 宁波市水利局
2026-01-19 16:24:15,156:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,156:WARNING: Fixed:  338.12m
2026-01-19 16:24:15,156:WARNING: Fixed: 灌溉
2026-01-19 16:24:15,156:WARNING: Fixed: 供水
2026-01-19 16:24:15,156:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,156:WARNING: Fixed:  338.12m
2026-01-19 16:24:15,157:WARNING: Fixed: 供水调度
2026-01-19 16:24:15,157:WARNING: Fixed: 年平均相对湿度
2026-01-19 16:24:15,157:WARNING: Fixed: 年平均陆面蒸发量
2026-01-19 16:24:15,157:WARNING: Fixed: 平均径流
2026-01-19 16:24:15,157:WARNING: Fixed:  80%以上
2026-01-19 16:24:15,157:WARNING: Fixed: 75%以上
2026-01-19 16:24:15,157:WARNING: Fixed: 650mm～700mm
2026-01-19 16:24:15,157:WARNING: Fixed: 750mm～800mm
2026-01-19 16:24:15,157:WARNING: Fixed: 1000mm～400mm
2026-01-19 16:24:15,157:WARNING: Fixed: 720mm
2026-01-19 16:24:15,157:WARNING: Fixed: 控制水位
2026-01-19 16:24:15,157:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,157:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,157:WARNING: Fixed: 水库坝前水位
2026-01-19 16:24:15,157:WARNING: Fixed: 设计水位
2026-01-19 16:24:15,157:WARNING: Fixed: 防汛领导小组
2026-01-19 16:24:15,157:WARNING: Fixed: 向上级有关部门报告
2026-01-19 16:24:15,157:WARNING: Fixed: 启动上下游群众转移预案
2026-01-19 16:24:15,157:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,157:WARNING: Fixed: 61.22m
2026-01-19 16:24:15,157:WARNING: Fixed: 加大下泄流量
2026-01-19 16:24:15,158:WARNING: Fixed: 敞泄
2026-01-19 16:24:15,158:WARNING: Fixed: 汛限水位
2026-01-19 16:24:15,158:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,158:WARNING: Fixed: 奉化江干流
2026-01-19 16:24:15,158:WARNING: Fixed: 奉化江
2026-01-19 16:24:15,158:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,158:WARNING: Fixed: 雨量将
2026-01-19 16:24:15,158:WARNING: Fixed: 150mm
2026-01-19 16:24:15,158:WARNING: Fixed: 水库
2026-01-19 16:24:15,158:WARNING: Fixed: 泄洪
2026-01-19 16:24:15,158:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,158:WARNING: Fixed: 河床
2026-01-19 16:24:15,158:WARNING: Fixed: 高程
2026-01-19 16:24:15,158:WARNING: Fixed: 10～35m
2026-01-19 16:24:15,158:WARNING: Fixed: 65.0～36.0m
2026-01-19 16:24:15,158:WARNING: Fixed: 库水位
2026-01-19 16:24:15,158:WARNING: Fixed: 水库
2026-01-19 16:24:15,158:WARNING: Fixed: 历史最高水位
2026-01-19 16:24:15,158:WARNING: Fixed: 设计洪水位
2026-01-19 16:24:15,158:WARNING: Fixed: 设计死水位
2026-01-19 16:24:15,158:WARNING: Fixed: 356.86 m
2026-01-19 16:24:15,158:WARNING: Fixed: 324.92m
2026-01-19 16:24:15,158:WARNING: Fixed: 库水位
2026-01-19 16:24:15,158:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,159:WARNING: Fixed: 20mm
2026-01-19 16:24:15,159:WARNING: Fixed: 三市镇人民政府
2026-01-19 16:24:15,159:WARNING: Fixed: 死水位
2026-01-19 16:24:15,159:WARNING: Fixed: 130.0m
2026-01-19 16:24:15,159:WARNING: Fixed: 水库
2026-01-19 16:24:15,159:WARNING: Fixed: 停止发电
2026-01-19 16:24:15,159:WARNING: Fixed: 水位
2026-01-19 16:24:15,159:WARNING: Fixed: 水库
2026-01-19 16:24:15,159:WARNING: Fixed: 危急水位
2026-01-19 16:24:15,159:WARNING: Fixed: 总指所
2026-01-19 16:24:15,159:WARNING: Fixed: 决策
2026-01-19 16:24:15,159:WARNING: Fixed: 视情况采取措施
2026-01-19 16:24:15,159:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,159:WARNING: Fixed: 浦江县水电局
2026-01-19 16:24:15,159:WARNING: Fixed: 东江
2026-01-19 16:24:15,159:WARNING: Fixed: 山顶高程
2026-01-19 16:24:15,159:WARNING: Fixed: 岸坡坡度
2026-01-19 16:24:15,159:WARNING: Fixed: 120～600m
2026-01-19 16:24:15,159:WARNING: Fixed: 10～35°
2026-01-19 16:24:15,159:WARNING: Fixed: 鹤山市址山镇三防指挥所
2026-01-19 16:24:15,159:WARNING: Fixed: 组织协调有关职能部门工作
2026-01-19 16:24:15,160:WARNING: Fixed: 水库设计防洪标准
2026-01-19 16:24:15,160:WARNING: Fixed: 水位
2026-01-19 16:24:15,160:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,160:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,160:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,160:WARNING: Fixed: 壶源江
2026-01-19 16:24:15,160:WARNING: Fixed: 主流长
2026-01-19 16:24:15,160:WARNING: Fixed: 集雨面积
2026-01-19 16:24:15,160:WARNING: Fixed: 8km
2026-01-19 16:24:15,160:WARNING: Fixed: 20.67km2
2026-01-19 16:24:15,160:WARNING: Fixed: 起调水位
2026-01-19 16:24:15,160:WARNING: Fixed: 水库最大下泄流量
2026-01-19 16:24:15,160:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,160:WARNING: Fixed: 60.48m
2026-01-19 16:24:15,160:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,160:WARNING: Fixed: 库水位
2026-01-19 16:24:15,160:WARNING: Fixed: 管理单位
2026-01-19 16:24:15,160:WARNING: Fixed: 防汛预案
2026-01-19 16:24:15,160:WARNING: Fixed: 库水位
2026-01-19 16:24:15,160:WARNING: Fixed: 水库
2026-01-19 16:24:15,160:WARNING: Fixed: 下泄
2026-01-19 16:24:15,160:WARNING: Fixed: 汛限水位
2026-01-19 16:24:15,160:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,161:WARNING: Fixed: 横山水库
2026-01-19 16:24:15,161:WARNING: Fixed: 灌溉面积
2026-01-19 16:24:15,161:WARNING: Fixed: 3766亩
2026-01-19 16:24:15,161:WARNING: Fixed: 启动防御超标准洪水预案
2026-01-19 16:24:15,161:WARNING: Fixed: 应急指挥机构
2026-01-19 16:24:15,161:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,161:WARNING: Fixed: 控制蓄水位
2026-01-19 16:24:15,161:WARNING: Fixed: 62.00m
2026-01-19 16:24:15,161:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,161:WARNING: Fixed: 设计洪水位
2026-01-19 16:24:15,161:WARNING: Fixed: 校核洪水位
2026-01-19 16:24:15,161:WARNING: Fixed:  65.31m
2026-01-19 16:24:15,161:WARNING: Fixed: 66.33m
2026-01-19 16:24:15,161:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,161:WARNING: Fixed: 浦江县人民政府防汛防旱指挥部
2026-01-19 16:24:15,161:WARNING: Fixed: 浦江县水务局
2026-01-19 16:24:15,161:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,161:WARNING: Fixed: 水库
2026-01-19 16:24:15,161:WARNING: Fixed: 库水位
2026-01-19 16:24:15,161:WARNING: Fixed: 设计洪水标准
2026-01-19 16:24:15,161:WARNING: Fixed: 65.31m
2026-01-19 16:24:15,161:WARNING: Fixed: 全开泄洪
2026-01-19 16:24:15,162:WARNING: Fixed: 大坝
2026-01-19 16:24:15,162:WARNING: Fixed: 大坝
2026-01-19 16:24:15,162:WARNING: Fixed: 管理所
2026-01-19 16:24:15,162:WARNING: Fixed: 东江
2026-01-19 16:24:15,162:WARNING: Fixed: 流域面积
2026-01-19 16:24:15,162:WARNING: Fixed: 116km²
2026-01-19 16:24:15,162:WARNING: Fixed: 水情监测站
2026-01-19 16:24:15,162:WARNING: Fixed: 入库流量
2026-01-19 16:24:15,162:WARNING: Fixed: 库水位数据
2026-01-19 16:24:15,162:WARNING: Fixed: 实时采集
2026-01-19 16:24:15,162:WARNING: Fixed: 县防汛指挥部
2026-01-19 16:24:15,162:WARNING: Fixed: 应急抢险
2026-01-19 16:24:15,162:WARNING: Fixed: 信息报送
2026-01-19 16:24:15,162:WARNING: Fixed: 水库
2026-01-19 16:24:15,162:WARNING: Fixed: 调度运行管理工作
2026-01-19 16:24:15,162:WARNING: Fixed: 100年一遇校核洪水
2026-01-19 16:24:15,162:WARNING: Fixed: 启动特大洪水应急预案
2026-01-19 16:24:15,162:WARNING: Fixed: 省水利厅
2026-01-19 16:24:15,162:WARNING: Fixed: 大坝
2026-01-19 16:24:15,162:WARNING: Fixed: 市三防办
2026-01-19 16:24:15,162:WARNING: Fixed: 报告
2026-01-19 16:24:15,163:WARNING: Fixed: 水位
2026-01-19 16:24:15,163:WARNING: Fixed: 112.3m
2026-01-19 16:24:15,163:WARNING: Fixed: 防灾预警线
2026-01-19 16:24:15,163:WARNING: Fixed: 正常蓄水位
2026-01-19 16:24:15,163:WARNING: Fixed: 控制水位
2026-01-19 16:24:15,163:WARNING: Fixed:  355.12m
2026-01-19 16:24:15,163:WARNING: Fixed: 355.12m
2026-01-19 16:24:15,163:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,163:WARNING: Fixed: 浦江县西水东调管理处
2026-01-19 16:24:15,163:WARNING: Fixed: 浦江县水务局
2026-01-19 16:24:15,163:WARNING: Fixed: 启闭闸门
2026-01-19 16:24:15,163:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,163:WARNING: Fixed: 东江干流
2026-01-19 16:24:15,163:WARNING: Fixed: 枢纽工程
2026-01-19 16:24:15,163:WARNING: Fixed: 非溢流坝
2026-01-19 16:24:15,163:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,163:WARNING: Fixed: 灌溉电站
2026-01-19 16:24:15,163:WARNING: Fixed: 防洪调度指令
2026-01-19 16:24:15,163:WARNING: Fixed: 操作闸门
2026-01-19 16:24:15,163:WARNING: Fixed: 省水利厅
2026-01-19 16:24:15,163:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,163:WARNING: Fixed: 163m
2026-01-19 16:24:15,163:WARNING: Fixed: 15分钟内，要将水库的水位、降雨量等情况报告
2026-01-19 16:24:15,164:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,164:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,164:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,164:WARNING: Fixed: 水库库水位
2026-01-19 16:24:15,164:WARNING: Fixed:  96.39 m
2026-01-19 16:24:15,164:WARNING: Fixed: 100 （mm
2026-01-19 16:24:15,164:WARNING: Fixed: 94.39 m
2026-01-19 16:24:15,164:WARNING: Fixed: 预报降雨
2026-01-19 16:24:15,164:WARNING: Fixed: 库内水位
2026-01-19 16:24:15,164:WARNING: Fixed: 上级主管部门
2026-01-19 16:24:15,164:WARNING: Fixed: 调蓄
2026-01-19 16:24:15,164:WARNING: Fixed: 大坝
2026-01-19 16:24:15,164:WARNING: Fixed: 奉化气象站
2026-01-19 16:24:15,164:WARNING: Fixed: 平均气温
2026-01-19 16:24:15,164:WARNING: Fixed: 极端最高气温
2026-01-19 16:24:15,164:WARNING: Fixed: 极端最低气温
2026-01-19 16:24:15,164:WARNING: Fixed: 16.3℃
2026-01-19 16:24:15,164:WARNING: Fixed: 39.0℃
2026-01-19 16:24:15,164:WARNING: Fixed: -11.1℃
2026-01-19 16:24:15,164:WARNING: Fixed: 库水位
2026-01-19 16:24:15,164:WARNING: Fixed: 62.00m
2026-01-19 16:24:15,164:WARNING: Fixed:  20mm
2026-01-19 16:24:15,165:WARNING: Fixed: 库水位
2026-01-19 16:24:15,165:WARNING: Fixed: 60.68m
2026-01-19 16:24:15,165:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,165:WARNING: Fixed: 水库最大下泄流量
2026-01-19 16:24:15,165:WARNING: Fixed: 水库坝前水位
2026-01-19 16:24:15,165:WARNING: Fixed: 超标准洪水
2026-01-19 16:24:15,165:WARNING: Fixed: 应急指挥机构
2026-01-19 16:24:15,165:WARNING: Fixed: 启动防御超标准洪水预案
2026-01-19 16:24:15,165:WARNING: Fixed: 下泄流量
2026-01-19 16:24:15,165:WARNING: Fixed: 下游城镇防洪标准
2026-01-19 16:24:15,165:WARNING: Fixed: 启动
2026-01-19 16:24:15,165:WARNING: Fixed: 县江
2026-01-19 16:24:15,165:WARNING: Fixed: 横山水库
2026-01-19 16:24:15,165:WARNING: Fixed: 编制相应的调度方案
2026-01-19 16:24:15,165:WARNING: Fixed: 大坝
2026-01-19 16:24:15,165:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,165:WARNING: Fixed: 输水设备
2026-01-19 16:24:15,165:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,165:WARNING: Fixed: 最大泄量
2026-01-19 16:24:15,165:WARNING: Fixed: 1200m³/s
2026-01-19 16:24:15,165:WARNING: Fixed: 敞泄
2026-01-19 16:24:15,165:WARNING: Fixed: 防洪限制水位
2026-01-19 16:24:15,166:WARNING: Fixed: 163m
2026-01-19 16:24:15,166:WARNING: Fixed: 三市镇渡头村
2026-01-19 16:24:15,166:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,166:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,166:WARNING: Fixed: 杭坪镇外胡村
2026-01-19 16:24:15,166:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,166:WARNING: Fixed:  64.73～65.31m
2026-01-19 16:24:15,166:WARNING: Fixed:  50m³/s
2026-01-19 16:24:15,166:WARNING: Fixed: 20～50年一遇洪水位
2026-01-19 16:24:15,166:WARNING: Fixed: 最大下泄流量
2026-01-19 16:24:15,166:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,166:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,166:WARNING: Fixed: 多年平均供水
2026-01-19 16:24:15,166:WARNING: Fixed: 优质水供水量
2026-01-19 16:24:15,166:WARNING: Fixed: 供水保证率
2026-01-19 16:24:15,166:WARNING: Fixed: 2800万m³
2026-01-19 16:24:15,166:WARNING: Fixed: 2272 万 m³/年
2026-01-19 16:24:15,166:WARNING: Fixed: 95%
2026-01-19 16:24:15,166:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,166:WARNING: Fixed: 54.09m
2026-01-19 16:24:15,166:WARNING: Fixed: 2022年底水位
2026-01-19 16:24:15,166:WARNING: Fixed: 防洪高水位
2026-01-19 16:24:15,166:WARNING: Fixed: 校核洪水位
2026-01-19 16:24:15,167:WARNING: Fixed: 正常蓄水位
2026-01-19 16:24:15,167:WARNING: Fixed: 158.20m
2026-01-19 16:24:15,167:WARNING: Fixed: 160.50m
2026-01-19 16:24:15,167:WARNING: Fixed: 155.00m
2026-01-19 16:24:15,167:WARNING: Fixed: 青山水库
2026-01-19 16:24:15,167:WARNING: Fixed: 总库容
2026-01-19 16:24:15,167:WARNING: Fixed: 1200 万m³
2026-01-19 16:24:15,167:WARNING: Fixed: 坝高
2026-01-19 16:24:15,167:WARNING: Fixed: 总库容
2026-01-19 16:24:15,167:WARNING: Fixed: 40m
2026-01-19 16:24:15,167:WARNING: Fixed: 655万m³
2026-01-19 16:24:15,167:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,167:WARNING: Fixed: 科学调蓄
2026-01-19 16:24:15,167:WARNING: Fixed: 补水
2026-01-19 16:24:15,167:WARNING: Fixed: 水库
2026-01-19 16:24:15,167:WARNING: Fixed: 下游河道
2026-01-19 16:24:15,167:WARNING: Fixed: 水库溢洪道
2026-01-19 16:24:15,167:WARNING: Fixed: 水库排洪调度工作
2026-01-19 16:24:15,167:WARNING: Fixed: 库区
2026-01-19 16:24:15,167:WARNING: Fixed: 大坝
2026-01-19 16:24:15,167:WARNING: Fixed: 水政监察大队
2026-01-19 16:24:15,167:WARNING: Fixed: 巡查
2026-01-19 16:24:15,168:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,168:WARNING: Fixed: 报告水库情况并做好记录
2026-01-19 16:24:15,179:INFO: --------admin_train data process DONE!--------
2026-01-19 16:24:15,180:WARNING: Fixed: 宁波市葛岙水库开发有限公司
2026-01-19 16:24:15,180:WARNING: Fixed: 宁波市水利局
2026-01-19 16:24:15,180:WARNING: Fixed: 奉化区水利局
2026-01-19 16:24:15,180:WARNING: Fixed: 实施调度
2026-01-19 16:24:15,180:WARNING: Fixed: 调度
2026-01-19 16:24:15,180:WARNING: Fixed: 洪水调度
2026-01-19 16:24:15,180:WARNING: Fixed: 水位
2026-01-19 16:24:15,180:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,180:WARNING: Fixed: 溢洪道
2026-01-19 16:24:15,180:WARNING: Fixed: 正常水位
2026-01-19 16:24:15,180:WARNING: Fixed: 预计水位
2026-01-19 16:24:15,180:WARNING: Fixed: 163m
2026-01-19 16:24:15,180:WARNING: Fixed: 下游
2026-01-19 16:24:15,180:WARNING: Fixed: 保护耕地
2026-01-19 16:24:15,180:WARNING: Fixed: 5.2 万人
2026-01-19 16:24:15,180:WARNING: Fixed:  3.5 万亩
2026-01-19 16:24:15,180:WARNING: Fixed: 启闭机房
2026-01-19 16:24:15,181:WARNING: Fixed: 发电机组
2026-01-19 16:24:15,181:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,181:WARNING: Fixed: 平均灌溉供水总量
2026-01-19 16:24:15,181:WARNING: Fixed: 年平均生态供水总量
2026-01-19 16:24:15,181:WARNING: Fixed: 鄞江
2026-01-19 16:24:15,181:WARNING: Fixed: 周公宅水库
2026-01-19 16:24:15,181:WARNING: Fixed: 皎口水库
2026-01-19 16:24:15,181:WARNING: Fixed: 河长
2026-01-19 16:24:15,181:WARNING: Fixed: 69km
2026-01-19 16:24:15,181:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,181:WARNING: Fixed: 正常水位
2026-01-19 16:24:15,181:WARNING: Fixed: 20年一遇设计洪水位
2026-01-19 16:24:15,181:WARNING: Fixed: 溢洪水深
2026-01-19 16:24:15,181:WARNING: Fixed: 163m
2026-01-19 16:24:15,181:WARNING: Fixed: 165.20m
2026-01-19 16:24:15,181:WARNING: Fixed: 2.20m
2026-01-19 16:24:15,181:WARNING: Fixed: 水库防汛工作
2026-01-19 16:24:15,181:WARNING: Fixed: 址山镇三防指挥所
2026-01-19 16:24:15,181:WARNING: Fixed: 库水位
2026-01-19 16:24:15,181:WARNING: Fixed:  62.99m 
2026-01-19 16:24:15,181:WARNING: Fixed: 64.73m
2026-01-19 16:24:15,181:WARNING: Fixed: 水库关闸蓄洪
2026-01-19 16:24:15,181:WARNING: Fixed: 库水位
2026-01-19 16:24:15,182:WARNING: Fixed: 64.73m
2026-01-19 16:24:15,182:WARNING: Fixed: 65.31m
2026-01-19 16:24:15,182:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,182:WARNING: Fixed: 水库控制最大下泄流量
2026-01-19 16:24:15,182:WARNING: Fixed: 水库
2026-01-19 16:24:15,182:WARNING: Fixed: 敞泄
2026-01-19 16:24:15,182:WARNING: Fixed: 最大下泄流量
2026-01-19 16:24:15,182:WARNING: Fixed: 设计洪水的洪峰流量
2026-01-19 16:24:15,182:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,182:WARNING: Fixed: 关闸蓄洪
2026-01-19 16:24:15,182:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,182:WARNING: Fixed: 163m
2026-01-19 16:24:15,182:WARNING: Fixed: 流域累计面雨量
2026-01-19 16:24:15,182:WARNING: Fixed: 瞬时流量
2026-01-19 16:24:15,182:WARNING: Fixed: 100mm
2026-01-19 16:24:15,182:WARNING: Fixed: 100m³/s
2026-01-19 16:24:15,182:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,182:WARNING: Fixed: 60.48m
2026-01-19 16:24:15,182:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,182:WARNING: Fixed: 水库最大下泄流量
2026-01-19 16:24:15,182:WARNING: Fixed: 防洪高水位线
2026-01-19 16:24:15,182:WARNING: Fixed: 防洪高水位线
2026-01-19 16:24:15,182:WARNING: Fixed: 校核洪水位线
2026-01-19 16:24:15,183:WARNING: Fixed: 防洪限制水位线
2026-01-19 16:24:15,183:WARNING: Fixed: 水库防洪调度区
2026-01-19 16:24:15,183:WARNING: Fixed: 下游防洪调度区
2026-01-19 16:24:15,183:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,183:WARNING: Fixed: 校核洪水位
2026-01-19 16:24:15,183:WARNING: Fixed: 设计洪水位
2026-01-19 16:24:15,183:WARNING: Fixed:  357.52m
2026-01-19 16:24:15,183:WARNING: Fixed: 356.86m
2026-01-19 16:24:15,183:WARNING: Fixed: 排洪闸
2026-01-19 16:24:15,183:WARNING: Fixed: 开度及下泄流量
2026-01-19 16:24:15,183:WARNING: Fixed: 上报
2026-01-19 16:24:15,183:WARNING: Fixed: 镇三防办
2026-01-19 16:24:15,183:WARNING: Fixed: 编制
2026-01-19 16:24:15,183:WARNING: Fixed: 奉化区水利局
2026-01-19 16:24:15,183:WARNING: Fixed: 宁波市水利局
2026-01-19 16:24:15,183:WARNING: Fixed: 奉化江
2026-01-19 16:24:15,183:WARNING: Fixed: 海拔
2026-01-19 16:24:15,183:WARNING: Fixed: 976m
2026-01-19 16:24:15,183:WARNING: Fixed: 限制水位
2026-01-19 16:24:15,183:WARNING: Fixed: 限制水位
2026-01-19 16:24:15,183:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,183:WARNING: Fixed: 60.00m
2026-01-19 16:24:15,183:WARNING: Fixed: 库水位
2026-01-19 16:24:15,184:WARNING: Fixed: 下泄
2026-01-19 16:24:15,184:WARNING: Fixed: 剡江
2026-01-19 16:24:15,184:WARNING: Fixed: 亭下水库
2026-01-19 16:24:15,184:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,184:WARNING: Fixed: 163m
2026-01-19 16:24:15,184:WARNING: Fixed: 防洪调度
2026-01-19 16:24:15,184:WARNING: Fixed: 大坝
2026-01-19 16:24:15,184:WARNING: Fixed: 汛前水位
2026-01-19 16:24:15,184:WARNING: Fixed: 正常水位
2026-01-19 16:24:15,184:WARNING: Fixed: 163m
2026-01-19 16:24:15,184:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,184:WARNING: Fixed: 外胡水库
2026-01-19 16:24:15,184:WARNING: Fixed: 金坑岭水库
2026-01-19 16:24:15,184:WARNING: Fixed: 浦江县电网
2026-01-19 16:24:15,184:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,184:WARNING: Fixed: 62.99m
2026-01-19 16:24:15,184:WARNING: Fixed: 300m³/s 
2026-01-19 16:24:15,184:WARNING: Fixed: 50m³/s
2026-01-19 16:24:15,184:WARNING: Fixed: 下游河道下王控制断面安全泄量
2026-01-19 16:24:15,184:WARNING: Fixed: 最大下泄流量
2026-01-19 16:24:15,184:WARNING: Fixed: 渡头村
2026-01-19 16:24:15,184:WARNING: Fixed: 斗岭水库
2026-01-19 16:24:15,185:WARNING: Fixed: 执行
2026-01-19 16:24:15,185:WARNING: Fixed: 葛岙水库
2026-01-19 16:24:15,185:WARNING: Fixed: 水库
2026-01-19 16:24:15,185:WARNING: Fixed: 控制蓄水位
2026-01-19 16:24:15,185:WARNING: Fixed:  60.00m
2026-01-19 16:24:15,185:WARNING: Fixed: 水库水位
2026-01-19 16:24:15,185:WARNING: Fixed: 88.0m
2026-01-19 16:24:15,185:WARNING: Fixed: 警戒水位
2026-01-19 16:24:15,185:WARNING: Fixed: 防汛抢险队
2026-01-19 16:24:15,185:WARNING: Fixed: 坝高
2026-01-19 16:24:15,185:WARNING: Fixed: 坝顶高程
2026-01-19 16:24:15,185:WARNING: Fixed: 坝顶轴线长
2026-01-19 16:24:15,185:WARNING: Fixed: 坝顶宽
2026-01-19 16:24:15,185:WARNING: Fixed: 8.6 m
2026-01-19 16:24:15,185:WARNING: Fixed: 99.08 m
2026-01-19 16:24:15,185:WARNING: Fixed: 139 m
2026-01-19 16:24:15,185:WARNING: Fixed: 6.00m
2026-01-19 16:24:15,185:WARNING: Fixed: 东江
2026-01-19 16:24:15,185:WARNING: Fixed: 东江
2026-01-19 16:24:15,185:WARNING: Fixed: 东江干流
2026-01-19 16:24:15,185:WARNING: Fixed: 镇三防指挥所
2026-01-19 16:24:15,185:WARNING: Fixed: 防汛机构
2026-01-19 16:24:15,185:WARNING: Fixed: 输水涵管
2026-01-19 16:24:15,185:WARNING: Fixed: 启闭设备
2026-01-19 16:24:15,186:WARNING: Fixed: 维修养护
2026-01-19 16:24:15,186:WARNING: Fixed: 大坝
2026-01-19 16:24:15,186:WARNING: Fixed: 撤离预警
2026-01-19 16:24:15,186:WARNING: Fixed: 市三防指挥所
2026-01-19 16:24:15,186:WARNING: Fixed: 水库
2026-01-19 16:24:15,186:WARNING: Fixed: 监测
2026-01-19 16:24:15,186:WARNING: Fixed: 入库流量
2026-01-19 16:24:15,186:WARNING: Fixed: 500m³/s
2026-01-19 16:24:15,186:WARNING: Fixed: 市应急管理局
2026-01-19 16:24:15,189:INFO: --------admin_test data process DONE!--------
2026-01-19 16:24:15,189:INFO: --------Process Done!--------
2026-01-19 16:24:15,195:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 16:24:15,195:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 16:24:15,195:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 16:24:15,195:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 16:24:15,195:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 16:24:15,195:INFO: loading file None
2026-01-19 16:24:15,195:INFO: loading file None
2026-01-19 16:24:15,195:INFO: loading file None
2026-01-19 16:24:15,534:INFO: Model name 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'pretrained_bert_models/chinese_roberta_wwm_large_ext/' is a path or url to a directory containing tokenizer files.
2026-01-19 16:24:15,534:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/added_tokens.json. We won't load it.
2026-01-19 16:24:15,534:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/special_tokens_map.json. We won't load it.
2026-01-19 16:24:15,534:INFO: Didn't find file pretrained_bert_models/chinese_roberta_wwm_large_ext/tokenizer_config.json. We won't load it.
2026-01-19 16:24:15,534:INFO: loading file pretrained_bert_models/chinese_roberta_wwm_large_ext/vocab.txt
2026-01-19 16:24:15,534:INFO: loading file None
2026-01-19 16:24:15,534:INFO: loading file None
2026-01-19 16:24:15,534:INFO: loading file None
2026-01-19 16:24:15,580:INFO: --------Dataset Build!--------
2026-01-19 16:24:15,580:INFO: --------Get Dataloader!--------
2026-01-19 16:24:15,581:INFO: loading configuration file pretrained_bert_models/chinese_roberta_wwm_large_ext/config.json
2026-01-19 16:24:15,581:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 16,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

2026-01-19 16:24:15,582:INFO: loading weights file pretrained_bert_models/chinese_roberta_wwm_large_ext/pytorch_model.bin
2026-01-19 16:24:22,743:INFO: Weights of BertNER not initialized from pretrained model: ['fusion_projection.weight', 'fusion_projection.bias', 'bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']
2026-01-19 16:24:22,743:INFO: Weights from pretrained model not used in BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2026-01-19 16:24:25,693:INFO: --------Start Training!--------
2026-01-19 16:24:44,566:INFO: Epoch: 1, train loss: 526.9188673836844
2026-01-19 16:24:44,915:INFO: Epoch: 1, dev loss: 402.0371431623186, f1 score: 0
2026-01-19 16:25:03,450:INFO: Epoch: 2, train loss: 404.48793601989746
2026-01-19 16:25:03,815:INFO: Epoch: 2, dev loss: 385.62220982142856, f1 score: 0
2026-01-19 16:25:22,260:INFO: Epoch: 3, train loss: 268.36843545096264
2026-01-19 16:25:22,609:INFO: Epoch: 3, dev loss: 349.53907775878906, f1 score: 0
2026-01-19 16:25:41,256:INFO: Epoch: 4, train loss: 216.1375322341919
2026-01-19 16:25:41,623:INFO: Epoch: 4, dev loss: 306.18775830950057, f1 score: 0
2026-01-19 16:26:00,027:INFO: Epoch: 5, train loss: 169.78597784042358
2026-01-19 16:26:00,386:INFO: Epoch: 5, dev loss: 262.0138626098633, f1 score: 0
2026-01-19 16:26:18,863:INFO: Epoch: 6, train loss: 130.27107776914323
2026-01-19 16:26:19,223:INFO: Epoch: 6, dev loss: 219.07812826974052, f1 score: 0
2026-01-19 16:26:37,425:INFO: Epoch: 7, train loss: 103.61536400658744
2026-01-19 16:26:37,779:INFO: Epoch: 7, dev loss: 181.403925214495, f1 score: 0
2026-01-19 16:26:55,919:INFO: Epoch: 8, train loss: 80.26764808382306
2026-01-19 16:26:56,277:INFO: Epoch: 8, dev loss: 150.3722381591797, f1 score: 0
2026-01-19 16:27:14,593:INFO: Epoch: 9, train loss: 59.851913928985596
2026-01-19 16:27:14,946:INFO: Epoch: 9, dev loss: 125.57132284981864, f1 score: 0
2026-01-19 16:27:33,643:INFO: Epoch: 10, train loss: 45.84956065246037
2026-01-19 16:27:34,004:INFO: Epoch: 10, dev loss: 105.79218183244977, f1 score: 0.04651162790697675
2026-01-19 16:27:34,005:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:27:38,763:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:27:38,763:INFO: --------Save best model!--------
2026-01-19 16:27:56,915:INFO: Epoch: 11, train loss: 34.23719359295709
2026-01-19 16:27:57,271:INFO: Epoch: 11, dev loss: 90.27023097446987, f1 score: 0.10526315789473685
2026-01-19 16:27:57,272:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:28:01,982:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:28:01,982:INFO: --------Save best model!--------
2026-01-19 16:28:20,670:INFO: Epoch: 12, train loss: 28.222946780068533
2026-01-19 16:28:21,026:INFO: Epoch: 12, dev loss: 78.41405160086495, f1 score: 0.18181818181818182
2026-01-19 16:28:21,027:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:28:25,755:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:28:25,756:INFO: --------Save best model!--------
2026-01-19 16:28:44,273:INFO: Epoch: 13, train loss: 22.48370917354311
2026-01-19 16:28:44,633:INFO: Epoch: 13, dev loss: 68.95465850830078, f1 score: 0.3137254901960784
2026-01-19 16:28:44,634:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:28:49,384:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:28:49,385:INFO: --------Save best model!--------
2026-01-19 16:29:07,438:INFO: Epoch: 14, train loss: 19.244973193321908
2026-01-19 16:29:07,801:INFO: Epoch: 14, dev loss: 61.87994820731027, f1 score: 0.33027522935779813
2026-01-19 16:29:07,802:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:29:12,513:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:29:12,513:INFO: --------Save best model!--------
2026-01-19 16:29:30,902:INFO: Epoch: 15, train loss: 15.539316986288343
2026-01-19 16:29:31,263:INFO: Epoch: 15, dev loss: 56.519098554338726, f1 score: 0.4587155963302752
2026-01-19 16:29:31,264:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:29:36,013:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:29:36,014:INFO: --------Save best model!--------
2026-01-19 16:29:54,435:INFO: Epoch: 16, train loss: 13.063151643212352
2026-01-19 16:29:54,788:INFO: Epoch: 16, dev loss: 52.551788330078125, f1 score: 0.48214285714285715
2026-01-19 16:29:54,789:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:29:59,537:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:29:59,538:INFO: --------Save best model!--------
2026-01-19 16:30:17,876:INFO: Epoch: 17, train loss: 9.83479434464659
2026-01-19 16:30:18,221:INFO: Epoch: 17, dev loss: 49.74982343401228, f1 score: 0.5172413793103449
2026-01-19 16:30:18,222:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:30:22,946:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:30:22,946:INFO: --------Save best model!--------
2026-01-19 16:30:41,300:INFO: Epoch: 18, train loss: 9.663203252213341
2026-01-19 16:30:41,657:INFO: Epoch: 18, dev loss: 47.854038783482146, f1 score: 0.5423728813559322
2026-01-19 16:30:41,658:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:30:46,401:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:30:46,401:INFO: --------Save best model!--------
2026-01-19 16:31:04,855:INFO: Epoch: 19, train loss: 8.772936352661677
2026-01-19 16:31:05,211:INFO: Epoch: 19, dev loss: 46.83465576171875, f1 score: 0.5517241379310345
2026-01-19 16:31:05,212:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:31:09,936:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:31:09,936:INFO: --------Save best model!--------
2026-01-19 16:31:28,245:INFO: Epoch: 20, train loss: 7.452154125486102
2026-01-19 16:31:28,605:INFO: Epoch: 20, dev loss: 46.27393668038504, f1 score: 0.5913043478260869
2026-01-19 16:31:28,606:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:31:33,351:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:31:33,352:INFO: --------Save best model!--------
2026-01-19 16:31:51,583:INFO: Epoch: 21, train loss: 6.893628932535648
2026-01-19 16:31:51,946:INFO: Epoch: 21, dev loss: 46.12154497419085, f1 score: 0.5982905982905983
2026-01-19 16:31:51,947:INFO: Configuration saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/config.json
2026-01-19 16:31:56,689:INFO: Model weights saved in /home/c403/msy/CLUENER2020/BERT-CRF/experiments/admin_split/pytorch_model.bin
2026-01-19 16:31:56,690:INFO: --------Save best model!--------
2026-01-19 16:32:15,267:INFO: Epoch: 22, train loss: 5.551403021706002
2026-01-19 16:32:15,615:INFO: Epoch: 22, dev loss: 46.914808000837056, f1 score: 0.5982905982905983
2026-01-19 16:32:33,748:INFO: Epoch: 23, train loss: 5.113231264054775
2026-01-19 16:32:34,109:INFO: Epoch: 23, dev loss: 48.02182224818638, f1 score: 0.576271186440678
2026-01-19 16:32:52,369:INFO: Epoch: 24, train loss: 4.5195835780884535
2026-01-19 16:32:52,740:INFO: Epoch: 24, dev loss: 50.24992152622768, f1 score: 0.5714285714285714
2026-01-19 16:33:10,826:INFO: Epoch: 25, train loss: 4.78265087492764
2026-01-19 16:33:11,179:INFO: Epoch: 25, dev loss: 51.66487121582031, f1 score: 0.5714285714285714
2026-01-19 16:33:29,320:INFO: Epoch: 26, train loss: 3.6769616095615283
2026-01-19 16:33:29,692:INFO: Epoch: 26, dev loss: 53.39703151157924, f1 score: 0.5932203389830509
2026-01-19 16:33:47,767:INFO: Epoch: 27, train loss: 2.6171803902834654
2026-01-19 16:33:48,103:INFO: Epoch: 27, dev loss: 56.187207903180806, f1 score: 0.576271186440678
